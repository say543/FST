


==================
atis test dataset 
==================
problematic queries 
with number as token
query_with_unkonwn slot_issue   what meals are served on american flight 665 673 from milwaukee to seattle      O B-meal O O O B-airline_name O B-flight_number I-flight_number O B-fromloc.city_name O B-toloc.city_name
query_with_unkonwn slot_issue   show me the delta flights which serve a snack to coach passengers       O O O B-airline_name O O O O B-meal_description O B-compartment O


query_with_unkonwn slot_issue   list airports in arizona nevada and california please   O O O B-state_name I-state_name O B-state_name O
query_with_unkonwn slot_issue   what class is fare code q       O O O O O B-booking_class
query_with_unkonwn slot_issue   a flight from baltimore to san francisco arriving between 5 and 8 pm    O B-flight O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O O B-arrive_time.start_time O B-arrive_time.end_time I-arrive


has i-flight_number unknow slot

solution:
ignore if slot is not inside the list



=================
atis how to load file 
=================
archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)
'atis_model'
'pytorch_model.bin'
? not sure where the location is

cached_path
resolved_archive_file = cached_path(

TRANSFORMERS_CACHE
cache_dir : 'C:\\Users\\chiecha.REDMOND/.cache\\huggingface\\transformers'
in only stores weights, not other thing


=================
bert config  && huper paarmeter
atis experiment
=================

//joint bert 
        # Prepare optimizer and schedule (linear warmup and decay)
        no_decay = ['bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
             'weight_decay': self.args.weight_decay},
            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
        ]
		//fixed in code
		i update to this in atis


optimizer
	no hvd compressor

schedular traing steps 
t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs
		(140 // 1) * 5
		// floor operation 
		// fixed in code 
		// my case no floor operation but should be the same 
		
		
if (step + 1) % self.args.gradient_accumulation_steps == 0:
	whatever step it is ,  it will alawys be zero


batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU
	? it seems only one example will be evaluate, batch_size is not being used
		
epoch_iterator = tqdm(train_dataloader, desc="Iteration")
	4487 / 32 = 140
		會補足
	step will be 32


if 0 < self.args.max_steps < global_step:
	max_steps = -1 and global_step keeps +1
	so it will nevert happen


epsilon
1e-08
	pass as parameter, the same

drop rate = 0.1
evavulate batchsize = 64
trainf batch size = 32
use crf = false
warmup_steps = 0
	//fixed in code, the same 
weight_decay = 0.0
	//fixed in code
	i update to this in atis
gradient_accumulation_step = 1

igonre_index = 0 

learning rate 5e-05
	pass as parameter
	i updare to this in atis 
logging steps 200

max_grad_borm 1.0

max_seq_len = 50
max_steps = -1

num_train_epochs = 5.0
	pass as parameter, the same
save_steps = 200
	it will trigger to save model
seed = 1234
	? not sure if the same as seed_val fixed code , random seed for torch, cuda
	https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/utils.py
	refer to here 
	fixed in code 
	i update to this in atis

slot_loss_coef = 1.0
	fixed in code
	
slot_pad_label : PAD

pad_token_label_id
	assocaited with slot_pad_label
	

epoch = 5

bert config
	attention_probs_dropout_prob = 0.1
		fixed in code, the same 
	chunk_size_feed_forward : 0 
		fixed in code, the same 
	diviertiy penalty -0.0
		fixed in code, the same 
	do _ sample : False 
		fixed in code, the same 
	early_stopping = false 
		fixed in code, the same 
	gradient_checkpoint : false
		fixed in code, the same 
	hodden_act : gelu
		fixed in code, the same 
	hidden_dropout_prob = 0.1
		fixed in code, the same 
	hidden_size - 768
	idslabel 
		0 : label_0
		1: labe l1
		
	initialzie_range = 0.02
		fixed in code, the same 
	intermediat_size = 3072
		fixed in code, the same 
	
	layer_nor,_eps = 1e-12
		fixed in code, the same 
	length_penalty : 1.0
		fixed in code, the same 
	
	max_length 20
		fixed in code, the same 
	max_grad_norm 1.0
		fixed int code, the same 
		

	max_position_embedding:512
		fixed in code, the same 
	model_type : bert 
		fixed in code, the same 
	name_or_path : bert-case-uncased
		? not sure if it affects or not 
		need to provide by myself
	
	ni repated_ngram_size : 0
		fixed in code, the same 
	num attention ahead = 12
		fixed in code, the same 
	num beam grups 1
		fixed in code, the same 
	num beams 1
		fixed in code, the same 
	num_hidden layers -12
		my case i go witj 3 
	num_labels = 2
		fixed in code, the same 
	num_return_sequences 1
		fixed in code, the same 
	output attention false 
		fixed in code, the same 
	pad token _id = 0
		fixed in code, the same 
	psoitoon_emveddding absolue
		fixed in code, the same 
	repetititon penalty  1.0
		fixed in code, the same 
	return ditc : true 
		fixed in code, the same 
	temperatire 1.0
		fixed in code, the same 
	top k 50
		fixed in code, the same 
	top p 1.0
		fixed in code, the same 
	torchscript fasle
		fixed in code, the same 
	type vocan size = 2
		fixed in code, the same 
	vocan size = 30522
		fixed in code, the same 
	



// my config

run 7
	改了parameter
	跟run 4 比起來 slot 確實有比較好  雖然還是低
	atis iteration 中最後snapshot 沒有return failure 的case (還不知道原因是啥)
	intent 每個turn 都依樣 ? 這個真的不make sense
	from 
	Validation metric after iteration 5 : {'total_intent_precision': 0.7321428571428571, 'total_intent_recall': 0.7321428571428571, 'total_slot_precision': 0, 'total_slot_recall': 0}
	to
	Validation metric after iteration 5 : {'total_intent_precision': 0.7321428571428571, 'total_intent_recall': 0.7321428571428571, 'total_slot_precision': 0.5263466042154566, 'total_slot_recall': 0.2630962832894352}

run 10
	to 12 layers 
	all zero 
	
	? not sure why 

run 13 
	same as rum 7, max token = 50
	no big difference
	
run 37
	ouput_randomsampler_v1_02152021v1
	add random sampler
	status : fail but can output model
	Validation metric after iteration 5 : {'total_intent_precision': 0.9631449631449631, 'total_intent_recall': 0.9631449631449631, 'total_slot_precision': 0.6362861138588533, 'total_slot_recall': 0.6925818667854756}
	using cpu locally to load gpu trained model(*.pt) and do metric verification
			yes it can be verified, the same

run 42
	ouput_randomsampler_layer12_v1_02162021v1
	add random sampler
	status : fail but can output model
	try 12 layers to see whether performance is better than run 37
	Validation metric after iteration 5 : {'total_intent_precision': 0.9325441143622962, 'total_intent_recall': 0.9325441143622962, 'total_slot_precision': 0.570620239390642, 'total_slot_recall': 0.5840944531075963}
	Validation metric Iob after iteration 5 : {'intent_acc': 0.9325441143622962, 'slot_precision': 0.6037959381044488, 'slot_recall': 0.6852431127208869, 'slot_f1': 0.6419464294894864}			
	// no big difference
	using cpu locally to load gpu trained model(*.pt) and do metric verification
		yes it can be verified, the same
		
run 43/45
	add random sampler
	extend epoch from  5 to 15 to see performance
	layer = 3
	Validation metric after iteration 14 : {'total_intent_precision': 0.9955327228054501, 'total_intent_recall': 0.9955327228054501, 'total_slot_precision': 0.2737879045217596, 'total_slot_recall': 0.778050048266132}
	Validation metric Iob after iteration 14 : {'intent_acc': 0.9955327228054501, 'slot_precision': 0.27106448912714387, 'slot_recall': 0.8352540884644934, 'slot_f1': 0.4092993236611045}
	intent improves but slots does not update much

run 49/51
	add random sampler
	status : fail but can output model
	alawyas this error it can be different log files  eg : 70_drive_log_0.txt,  70_drive_log_1.txt
	Traceback (most recent call last):
	File "atis_train_horovod_joint_intent_slot_TNLR.py", line 2031, in <module>
		model_to_save.save_pretrained(out_dir)
	File "/azureml-envs/azureml_1807a796a9d0358a0054a3b50c9aa95a/lib/python3.6/site-packages/transformers/modeling_utils.py", line 812, in save_pretrained
		torch.save(state_dict, output_model_file)
	File "/azureml-envs/azureml_1807a796a9d0358a0054a3b50c9aa95a/lib/python3.6/site-packages/torch/serialization.py", line 328, in save
		_legacy_save(obj, opened_file, pickle_module, pickle_protocol)
	File "/azureml-envs/azureml_1807a796a9d0358a0054a3b50c9aa95a/lib/python3.6/site-packages/torch/serialization.py", line 196, in __exit__
		self.file_like.close()
	OSError: [Errno 5] Input/output error
		1>sometimes error happens to model_to_save.save_pretrained(out_dir) 
		2> sometimes error happens to torch.save(model, os.path.join(out_dir, 'model.pt'))
		看起來像是race condition
		trying in run 58/60 by comment the second one but it still error
		not sure why....

	extend epoch from  5 to 15 to see performance
	layer = 3
	igonre PAD token for evaluation to check performance - folliwng atis logic
	
	最好加個tab 分割
	wo pad metric 真的可以提升
	雖然iob 跟my logic 仍然有落差
	my logic 
		w pad
			slot_precision 非常低 
			Validation metric after iteration 15 : {'total_intent_precision': 0.9959794505249051, 'total_intent_recall': 0.9959794505249051, 'total_slot_precision': 0.3740392826643894, 'total_slot_recall': 0.7805747382490532}
			pad	: total_tp: 4912, total_fp: 27029, total_fn: 4042
			
			很多slot 的false positive 都增加了
			可能就來自於pad 的location 我想
		wo pad
			iteration 5 可以到80
			Validation metric wo pad after iteration 15 : {'total_intent_precision': 0.9959794505249051, 'total_intent_recall': 0.9959794505249051, 'total_slot_precision': 0.9048287478944413, 'total_slot_recall': 0.8960742882562278}
			pad	: total_tp: 0, total_fp: 111, total_fn: 0
	
	打算用這個version 在local repo 一下 
		but pytorch_model.bin size = 0 , 輸出有問題 我想... 只有model.pt
		mode.pt 沒辦法load 不知道為啥.... 無法verify
	training set 看看是不是都依樣
			
			
	
run 67/69
	remove ranmdom sampler 
	extend to 25 epochs to check metrics
	to see if 'status : fail but can output model' can be removed
	the status is correct by metric is low....
	Validation metric wo pad after iteration 25 : {'total_intent_precision': 0.9714285714285714, 'total_intent_recall': 0.9714285714285714, 'total_slot_precision': 0.7425512104283054, 'total_slot_recall': 0.6940818102697999}


run 70/74
	add random sampler
	status : fail but can output model
	alawyas this error it can be different log files  eg : 70_drive_log_5.txt,  70_drive_log_7.txt
	overall intent precision: 0.9975429975429976, overall intent recall: 0.9975429975429976
	Validation metric wo pad after iteration 15 : {'total_intent_precision': 0.9975429975429976, 'total_intent_recall': 0.9975429975429976, 'total_slot_precision': 0.8887891873703101, 'total_slot_recall': 0.8812277580071174}
	
	ouput_randomsampler_layer12_v1_02172021v1
	this onw pytorch_model.bin is not empty
	trying to replicate metrics and validating test dataset (run 49/51) but might be litle bit different
	
	atis_train
		local 
			Validation metric wo pad: {'total_intent_precision': 0.9975429975429976, 'total_intent_recall': 0.9975429975429976, 'total_slot_precision': 0.8887891873703101, 'total_slot_recall': 0.8812277580071174
	
	atis_test
		some problematic queris
		slot is lower compared to train set
			 Validation metric wo pad: {'total_intent_precision': 0.9761904761904762, 'total_intent_recall': 0.9761904761904762, 'total_slot_precision': 0.8285714285714286, 'total_slot_recall': 0.8255052661542841}
	
=================
evaluation logic
i shou;d ignore o and pad for calculation 
i thikn 
================
	
=================
code flow 
=================
joint bert 
	in the first place 
	model zero_grad but i do not have , i have optimizer.zero_gra[
	
	in some case model will .zerograd  but i do have 
	
	

=================
original aml behavaior
=================
run 1
 Validation metric after iteration 5 : {'total_intent_precision': 0.7321428571428571, 'total_intent_recall': 0.7321428571428571, 'total_slot_precision': 0, 'total_slot_recall': 0}



=================
original atis behavior
=================

max_len =50

i want to fly from baltimore to dallas round trip

attention (same as my att_masks)
len(12)
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...]

input_id (same as my text_id)
[101, 1045, 2215, 2000, 4875, 2013, 6222, 2000, 5759, 2461, 4440, 102, 0, 0, ...]


len = 12


token_type1_id (i do not provide this )
all zero to max_len = 50
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]

slot label id (same as my labels_for_text_id)
len = 12
[0, 2, 2, 2, 2, 2, 73, 2, 114, 98, 99, 0, 0, 0, ...]
[0, 2, 2, 2, 2, 2, 73, 2, 114, 98, 99, 0, 0, 0, ...]


0 2 2 2 100, 111   
0 for pad token
2 for untag toekn
CLS SEP for zero
otherthan that using 2



i'm looking for a flight from charlotte to las vegas that stops in st. louis hopefully a dinner flight how can i find that out
golden query to check preprocessing

before adding CLS and sep
[2, 2, 2, 2, 2, 2, 2, 2, 73, 2, 114, 115, 2, 2, ...]
len() = 29=8
2
2
2
2
2
2
2
2
73
2
114
115
2
2
2
// here st.louis extend
103
103
104
2
2
81
2
2
2
2
2
2






slot precision 0.94



=================
TNLR  using atis repo
train = dev = eva
epoch = 5
layer = 3
=================

C:\Users\chiecha.REDMOND\.cache\huggingface_TNLR_02162021v1
// eboch_atis_five_extend_b_slot_and_with_CLS_SEP_PAD_with_TNLR_02162021
//including PAD for evaluation
evaluate TNLR {'intent_acc': 0.9669495310406432, 'slot_precision': 0.5178640144451351, 'slot_recall': 0.7128305423134906, 'slot_f1': 0.5999039879299113, 'sementic_frame_acc': 0.13867798124162573} :


// remove pad for evaluation
// slot is better
 evaluate TNLR {'intent_acc': 0.9669495310406432, 'slot_precision': 0.7281873056520943, 'slot_recall': 0.7662154359402066, 'slot_f1': 0.7467175190696511, 'sementic_frame_acc': 0.4582402858418937} :



=================
TNLR  using atis repo
train = dev = eva
epoch = 5
layer = 12
=================
layer12 is not better than layer3

// eboch_atis_five_extend_b_slot_and_with_CLS_SEP_PAD_with_TNLR_layer12_02162021
//including PAD for evaluation
 evaluate TNLR {'intent_acc': 0.93144260830728, 'slot_precision': 0.5129070455891059, 'slot_recall': 0.3529723342704641, 'slot_f1': 0.41816909226944704, 'sementic_frame_acc': 0.0031263957123715946} :


// remove pad for evaluation
// not being better.... not sure why
 evaluate TNLR {'intent_acc': 0.93144260830728, 'slot_precision': 0.49209596429235636, 'slot_recall': 0.5092705459677936, 'slot_f1': 0.5005359732643924, 'sementic_frame_acc': 0.08240285841893702} :




=================
v1
CLS , SEP label = 0, as pad
B-label extend 
train = dev = eva
this case 
https://github.com/chakki-works/seqeval/blob/master/seqeval/metrics/sequence_labeling.py
 warnings.warn('{} seems not to be NE tag.'.format(chunk))
         >>> from seqeval.metrics.sequence_labeling import get_entities
        >>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']
        >>> get_entities(seq)
        [('PER', 0, 1), ('LOC', 3, 3)]
		
since v1 will break this chunk


this case evaluation will be different
my logic: B-code I-code are different span 
IOB logic: B-code I-code are the same slot 

=================

// applied in atis repo
// cpu local
// slot 9,1,2
{'intent_acc': 0.9973202322465387}
{'slot_f1': 0.982715575187248, 'slot_precision': 0.9805812839348451, 'slot_recall': 0.984859177519728}
{'sementic_frame_acc': 0.9522108083966057}


//applied in my local
//3 layers
//cpu
// same setting but different runs might be differentve81
//first outputs_temp_load_v1_02142021v1
//outputs_local_load_v1_02142021v1
  Validation metric after iteration 5 : {'total_intent_precision': 0.9220460129551039, 'total_intent_recall': 0.9220460129551039, 'total_slot_precision': 0.6902472527472527, 'total_slot_recall': 0.7089552238805971}
//second time 
outputs_temp_load_v1_02152021v1 >outputs_local_load_v1_02152021v1
   Validation metric after iteration 5 : {'total_intent_precision': 0.8981460799642618, 'total_intent_recall': 0.8981460799642618, 'total_slot_precision': 0.5670757511637748, 'total_slot_recall': 0.5970149253731343}
  
Time: 32m 1s
// test local to load model and do metric calculation again, to further verify
// load onnx bin still cannot work -error
Unable to load from type '<class 'onnx.onnx_ml_pb2.ModelProto'>'
// store
E:\azure_ml_notebook\outputs_temp_load_v1_02142021v1
// using pytorch_model.bin to local test
// third time, my evauation replicate success in atis_model_saved
// thrid time  IOB logic is higher
Validation metric Iob: {'intent_acc': 0.8981460799642618, 'slot_precision': 0.6036986236579482, 'slot_recall': 0.700471956975085, 'slot_f1': 0.6484948558364029}  
// trying to reproduce for all merics - wo pad and wo pad
 Validation metric : {'total_intent_precision': 0.8956890775072593, 'total_intent_recall': 0.8956890775072593, 'total_slot_precision': 0.4078921277017648, 'total_slot_recall': 0.5346031038835672}
 Validation metric wo pad: {'total_intent_precision': 0.8956890775072593, 'total_intent_recall': 0.8956890775072593, 'total_slot_precision': 0.5564067712838255, 'total_slot_recall': 0.5007784697508897}
 Validation metric Iob wo pad: {'intent_acc': 0.8956890775072593, 'slot_precision': 0.4143335281304787, 'slot_recall': 0.6419986829107672, 'slot_f1': 0.5036324303380583}
 Validation metric Iob: {'intent_acc': 0.8956890775072593, 'slot_precision': 0.6458380757265346, 'slot_recall': 0.6290137826552858, 'slot_f1': 0.6373149136107523}




//applied in my local
//12 layers
//cpu
// not big improvement
outputs_local_load_layer12_v1_02162021v1
 Validation metric : {'total_intent_precision': 0.9309805673442038, 'total_intent_recall': 0.9309805673442038, 'total_slot_precision': 0.537526347485697, 'total_slot_recall': 0.5302220242073216}


=================
MDM experiment 
compared to hvd initilization
CLS , SEP label = 0, as pad
=================

run ?
	using distributed sampler
	original hyper parameters
	epoch = 25 to check
	so far 
	即使用distributedSampler  結果也還可以  看起來atis 就是data 量不夠多
	等跑完25 可能要加入code
		check semantic frame (不知道yue 有沒有這個code)
	Validation metric wo pad after iteration 17 : {'total_intent_precision': 0.974581939799331, 'total_intent_recall': 0.974581939799331, 'total_slot_precision': 0.8828073246900728, 'total_slot_recall': 0.9165029228275287}
	
	