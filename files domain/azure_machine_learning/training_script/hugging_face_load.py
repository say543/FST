

# huggingface model page
#https://huggingface.co/transformers/main_classes/model.html



# ? not sure why having this
#https://discuss.pytorch.org/t/assertionerror-torch-not-compiled-with-cuda-enabled-unable-to-predict-on-local-machine-cpu/88183



# tutorial simlar to  train_horovod.py
# <done> verified in model_saved_test.py
# explian this code is to save 'If we have a distributed model, save only the encapsulated model'
# model_to_save = model.module if hasattr(model, 'module') else model
# ? not sure how state_dict() worrks and compare to (torch.save(model, os.path.join(out_dir, 'model.pt')))
# test in the futrue
# torch.save(model_to_save.state_dict(), output_model_file)
# not sure how it is different from  originla pretrain output, test it in the future ?
# <done> verified in model_saved_test.py, two versions of saving those files
# (model_to_save.save_pretrained(out_dir)
#    tokenizer.save_pretrained(out_dir))
# model_to_save.config.to_json_file(output_config_file)
# tokenizer.save_vocabulary(output_vocab_file)

# for reload model
# ? not sure if BertConfig can be used for distillation bert. need to test
# version 1: using old transformer model, load each file speficially 
#https://huggingface.co/transformers/v1.2.0/serialization.html
# version 2: using a output folder to load
#https://zhuanlan.zhihu.com/p/143209797
#https://mccormickml.com/2019/07/22/BERT-fine-tuning/



# output to onnx
# god example to preovide text / tokenized text as input for bert or distillation bert
# torch save and load model  for Ber class
# also good for inference
#https://huggingface.co/transformers/serialization.html



# fine tune  procedure using adam
# using adam
# also Token Classfication (slot tagging example for discuss) 
# can use this as reference
# DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))
# easy code
# hugging face dataset
# https://huggingface.co/transformers/custom_datasets.html



# torch officalto onnx offical github link
#https://github.com/pytorch/pytorch/blob/master/docs/source/onnx.rst


#microsoft pytorach model using ONNX
# similar to officla onnx github but has runtime performance profiling result
#https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21368-operationalizing-pytorch-models-using-onnx-and-onnx-runtime.pdf



#distill bert 
# loss function discusssion
#https://blog.csdn.net/fengzhou_/article/details/107211090?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control

#BertforSequenceClassification
## code explanation
# logits is liear output(each label's probability) but not with softmax (https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)
## BertforSequenceClassification = bert + dropout + linear + classifier
## bert-base-cased: for english
# (? in my code, i use distilbert-base-uncased for distilbert)
## tokens_tensor: input queriy token index
## segments_tensor: for sentence boundary , for example : sentence 1 (all 0) + [SEP](0) + sentence 2(all 0)
## masks_tensor: attention range , 1: attention should be done 0: padding , no need 
# ? similar to att_masks i use in my code
# pad token : for each batch we need to do zro padding to make sure all patchs having the same length 
# also masks_tensor/segments_tensor  should be zero for postion where you have 'pad token'
## has defined FakeNewsDataset class to define dataset
# __getitem__ : return a single traning data(tokens_tensor, segments_tensor, label_tensor) compatible with Bert format
# __len__ : return number of training data
# in my code, i use class tensorDataset / class DataLoader
# ? but i only provide tokens_tensor, how doesit work
## has defined DataLoader  class to assign mini batch
# use input from FakeNewsDataset traning data(tokens_tensor, segments_tensor, label_tensor)
# add zero paddong to (tokens_tensor, segments_tensor) by function pad_sequence(from torch.nn.utils.rnn import pad_sequence)
# generate masks_tensors, 1: attention should be done 0: padding , no need 
## has code to extract a small batch size
# in my code, i use class DataLoader 
# no segment_tenors snice only single sentence 
# masks_tensors is provided as part of training_data
# it is generated by att_masks based on various length of each query
# no padding 
# it is done by max_length and 'padding='
# text_ids = [tokenizer.encode(text, max_length=300, padding='max_length', truncation=True) for text in texts]
# has batch size argument
## has example code to show up model architecture by model.named_children()
## has example code to calculate accuracy
# def get_predictions(model, dataloader, compute_acc=False):
# tokens_tensors, segments_tensors, masks_tensors = data[:3]
# in my code
# for k, (mb_x, mb_m, mb_y) in enumerate(train_dataloader):

## has example to output parameters
# def get_learnable_params(module):
#整個分類模型的參數量：102269955
#線性分類器的參數量：2307
# in my code
# def count_parameters(model):
# number of parameters 66955010 
# ? why my experiment having less parameters but takes longer time to train
# ? might be related to padding i add as 300, test in the future


## has example code to train
# 將參數梯度歸零
# optimizer.zero_grad()
# # forward pass
# backword
# ? in my code, function has little bit different. need to study in the future
# for k, (mb_x, mb_m, mb_y) in enumerate(train_dataloader):


# PySnooper : open source to output trace information
# can test it in the future
# https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html
# https://blog.csdn.net/SZU_Hadooper/article/details/102490443
# https://mccormickml.com/2019/07/22/BERT-fine-tuning/


# basicsuage for different models 
#basic：
bertModel
bertTokenizer
#pretrain
bertForMaskedLM
bertForNextSentencePrediction
bertForPreTraining
#Fine-tuning 
bertForSequenceClassification
bertForTokenClassification
bertForQuestionAnswering
bertForMultipleChoice
https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html


#bert architecture discussion
# onlt read through section 'Model Outputs'
# bert + single-layer neural network as the classifier
# ? so inmy case case will also need to d othis
#https://jalammar.github.io/illustrated-bert/

# here Distill bert outputs emneedding as input to logistic regression
# using the the first vector(same as embedding side, eg : 768) (the one associated with the [CLS] token) to feed logistic regression
# (similar to feature extraction)
# here using tensorflow code so ignore it at first


# for Distill bert sequence claasfication
# we add the special tokens needed for sentence classifications 
# (these are [CLS] at the first position, and [SEP] at the end of the sentence).
# so input verctor does have [SEP] 
# ? in my dummy input, i ignore [SEP] not sure it wlll affect or not
#https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/



# pytorch dataloader class
# how to run it iteratively
# https://www.itread01.com/content/1545234667.html

#  this link has similar process as train_horovod.py trainig / valid procedure
# if training too long (trainnig loss down , validation loss up), it means overfitting
# https://zhuanlan.zhihu.com/p/143209797
# this link has similar process as the folloiwngf code for each function




# initial BertForMaskedLM and do inference
# ? can leverage this runtine to load any distill bert fine-tune version 
# and do inference to verify probability ouput before doing onnx conversion
#https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html
#customized distill bert by config
# 'Under the hood: pretrained models'
# using AutoTokenizer, AutoModelForSequenceClassification
# ? can use distill based to create a test, run model and do inference
#>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
#>>> tokenizer = AutoTokenizer.from_pretrained(model_name) 
# https://huggingface.co/transformers/quicktour.html


# AML official another lin
#anothe link good for tutorial, not using huggingface though
# also use hvd
# ? decide it want to train it the future
#https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/ml-frameworks/pytorch
# simple training script
# hyper parameter tuning (section : Start a hyperparameter sweep)
# ? trying to define range for hyper parameter tunning in the ftureu
https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb
#https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/pytorch_train.py




#another AML tutorial might not be so food
# leave it not very useful
#https://github.com/Azure/AzureML-BERT/blob/master/pretrain/PyTorch/notebooks/BERT_Pretrain.ipynb


# using onnx.load() to load onnx model
# ort settion to test onnx runtime model
# ? trying ot do it in the futrue
#https://github.com/onnx/tutorials/blob/master/tutorials/PytorchOnnxExport.ipynb
# https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html


# pips list ot check package verison being installed
torch                                1.7.1
# command: python --version
# 3.6.19
#https://stackoverflow.com/questions/10214827/find-which-version-of-package-is-installed-with-pip

# warring message
# consulted yue already so leave it
#Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
https://github.com/onnx/onnx/issues/2836#:~:text=Jump%20to%20bottom-,TracerWarning%3A%20Converting%20a%20tensor%20to%20a%20Python%20index%20might%20cause,not%20generalize%20to%20other%20inputs!






# huggung face ert sorce code
# BertForTokenClassification used in yue
#https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertModel
# huggung face distill bert sorce code
#https://huggingface.co/transformers/_modules/transformers/models/distilbert/modeling_distilbert.html




# another tutorial for bert
# similar to yue's bert turotial sharead in internet
## BERT has two constraints:
# All sentences must be padded or truncated to a single, fixed length.
# The maximum sentence length is 512 tokens.
## for Dataoader class, 
# RandomSamploer for traninig data
# SeqeuntialSampler for validation data
# in my code, i use DistributedSampler for both inorder to work with hvd


#output_attentions = False, # Whether the model returns attentions weights.
#output_hidden_states = False, # Whether the model returns all hidden-states.
# set it false then you can fine tune
# using adamW() to optimize 
# ? (AdamW is a class from the huggingface library (as opposed to pytorch) )
# ? might be wrong since it is from transformer package
# use get_linear_schedule_with_warmup
# Total number of training steps is [number of batches] x [number of epochs]. 
# (Note that this is not the same as the number of training samples).
# num_warmup_steps = 0, # Default value in run_glue.py

# has calculating accuracy function for sequenceClassfication
# 241, batch = 40
# ? can try to use it in the futrue
# has comment for each training procedure
# in my code, i have added comment respectively.
# question related
# ? model.train() can move to outside epoch loop, can try in the futrue
# ? training time and valication time can be calculated seperately
# ? torch.no_grad(): can move inside to each batch to check performance
# ? having test procedure (inlcuding data loader etc). can be leveraged in the future

# weight decay rate - from hugging gace
# in my code i do have 
# ? not sure why needs weight decay rate, need to study in the future
#https://mccormickml.com/2019/07/22/BERT-fine-tuning/
#https://zhuanlan.zhihu.com/p/143209797
#https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1




#another turtorial - using DistilBertForSequenceClassification  for recommendation system
# has sigmoid function to get output
# version 1 
# y_pred = sigmoid(outputs[0].detach().cpu().numpy())
# for logit it is the ouput of https://pytorch.org/docs/stable/generated/torch.nn.Linear.html
# and it is a readl value
# detach() : speed tuning up
# http://www.bnikolic.co.uk/blog/pytorch-detach.html
# .cpu()
# move logic to cpu even though using gpu to train
# ? add this in my code to see probability and compare it with QAS output
#https://github.com/sarang0909/Explore-PyTorch/blob/master/Part2_Pytorch_Sentiment_Analysis.ipynb
# version 2
# for logits
# outputs = model(inputs)
# outputs = torch.sigmoid(outputs)
#           predictions.append(outputs.cpu().detach().numpy().tolist())
# https://pytorch.org/docs/stable/generated/torch.sigmoid.html
# it seems torch sigmoid can do the output param already
# ? ? add this in my code to see probability and compare it with QAS output
#https://www.kaggle.com/samson22/distilbert-in-pytorch

# convert numpy array to torch.sensor
# in my code , i do have it
#http://www.bnikolic.co.uk/blog/pytorch-detach.html



#distill bert experiment, define a new class with simplifeid forward function
# a good example to define your own forward function
#Customized distillbert for sequence classfication
# DistilBertForSequenceClassification
#DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
https://www.kaggle.com/kashnitsky/distillbert-catalyst-amazon-product-reviews
'''
class DistilBertForSequenceClassification(nn.Module):
    """
    Simplified version of the same class by HuggingFace.
    See transformers/modeling_distilbert.py in the transformers repository.
    """

    def __init__(self, pretrained_model_name: str, num_classes: int = None):
        """
        Args:
            pretrained_model_name (str): HuggingFace model name.
                See transformers/modeling_auto.py
            num_classes (int): the number of class labels
                in the classification task
        """
        super().__init__()

        config = AutoConfig.from_pretrained(
            pretrained_model_name, num_labels=num_classes)

        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,
                                                    config=config)
        self.pre_classifier = nn.Linear(config.dim, config.dim)
        self.classifier = nn.Linear(config.dim, num_classes)
        self.dropout = nn.Dropout(config.seq_classif_dropout)

    def forward(self, features, attention_mask=None, head_mask=None):
        """Compute class probabilities for the input sequence.

        Args:
            features (torch.Tensor): ids of each token,
                size ([bs, seq_length]
            attention_mask (torch.Tensor): binary tensor, used to select
                tokens which are used to compute attention scores
                in the self-attention heads, size [bs, seq_length]
            head_mask (torch.Tensor): 1.0 in head_mask indicates that
                we keep the head, size: [num_heads]
                or [num_hidden_layers x num_heads]
        Returns:
            PyTorch Tensor with predicted class probabilities
        """
        assert attention_mask is not None, "attention mask is none"
        distilbert_output = self.distilbert(input_ids=features,
                                            attention_mask=attention_mask,
                                            head_mask=head_mask)
        # we only need the hidden state here and don't need
        # transformer output, so index 0
        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)
        # we take embeddings from the [CLS] token, so again index 0
        pooled_output = hidden_state[:, 0]  # (bs, dim)
        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)
        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)
        pooled_output = self.dropout(pooled_output)  # (bs, dim)
        logits = self.classifier(pooled_output)  # (bs, dim)

        return logits
'''



# tensor flow
# distillataion
# leave it in the future
# https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/



# saving and reloading issue
# but this is not to load pytorch model
# leave it in the futrue
#https://github.com/huggingface/transformers/issues/8272

#fast bert
# class BertDataBunch
# another wrapper to train
# leave it in the future
#https://github.com/kaushaltrivedi/fast-bert
# https://github.com/kaushaltrivedi/fast-bert/blob/81a6a594b81947f6a3a42dd4315403a9b27ff7c9/fast_bert/data_cls.py
#BertLearner.from_pretrained_model(
# static method
# with logits.softmax as example
# https://github.com/kaushaltrivedi/fast-bert/blob/b41ee05af18fbbff3e5fa209476041ce345e4c6a/fast_bert/learner_cls.py#L167




#softmax temperature for distilbert
# ? put into machine_learning .txt
#distill bert experiment, define a new class with simplifeid forward function
#https://zhuanlan.zhihu.com/p/91288247


# tinybert
# check in the futrue
# https://zhuanlan.zhihu.com/p/94359189


# fastai
# along with hugging face
# check in the future
# https://www.kaggle.com/melissarajaram/distilbert-fastai-huggingface-transformers



#github cide example
# leave it in the future
#https://github.com/sontung/hci-intermodal-reasoning/blob/4e792fc3daa5f1d572d0286dcb589d1357ccfd0c/text_network.py




# python help 
# add code base to support
#output trace when there is warning
#https://stackoverflow.com/questions/22373927/get-traceback-of-warnings

# python /onnx help
# How to debug torch.nn.export()  using forward function
# add print along your own forward function to test
https://discuss.pytorch.org/t/how-to-debug-torch-nn-export/46906


# vscode help
# how to disalbe just my code in vscode
https://code.visualstudio.com/docs/python/debugging
https://github.com/microsoft/vscode-python/issues/7347


# vscode help
# issue
# if running in debug model ( or run with debugger by vscode), torch.onnx.export  will not generate anything
# ? not sure why


# onnx warnning message debug
# inherited from nn.module
# using DistilBertForSequenceClassification inside
https://github.com/zhaogaofeng611/TextMatch/blob/master/DistilBert/model.py

#trasformer ? ? ?  error message container proved
#
#/azureml-envs/azureml_59b829dd316a9a6c2291bc4de6df663c/lib/python3.6/site-packages/transformers/modeling_utils.py:1645: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
#  input_tensor.shape == tensor_shape for input_tensor in input_tensors
# trying to upgrade to 4.1.1
# code become correct line but still wrong message
# refer to chunk_to_forward_debug_inside, its seq_len_dim =1
# and tensor value is different from input tensor... not sure why

# input_tensors[0].shape = torch.Size([1, 14, 768])

#azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/transformers/modeling_utils.py:1757: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
#  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors

#default chunk_dim = seq_len_dim = 1 
# related bug fix in pytorch but i still fails
# https://discuss.pytorch.org/t/best-way-to-convert-a-list-to-a-tensor/59949/2
#https://github.com/huggingface/transformers/issues/8349
# based on this , it looks ok. assertion is nothing wrong but just warning message
#This is what the warning is about. I think the tracer warning is pointing to a legit concern here. See the "Limitations" section in this PyTorch page.
#The recommended way to capture this dynamic behavior is to use TorchScript in the export. Here's an example, that shows slicing,
# https://github.com/Microsoft/onnxruntime/issues/679

# source code discussion
https://huggingface.co/transformers/glossary.html#feed-forward-chunking
https://huggingface.co/transformers/main_classes/configuration.html
https://huggingface.co/transformers/_modules/transformers/modeling_utils.html
# source code 
# apply_chunking_to_forward
# according trace log it is from class 
class FFN(nn.Module):
https://huggingface.co/transformers/_modules/transformers/modeling_distilbert.html



# BertForTokenClassification
# git
# how to do git search code
# https://docs.github.com/cn/free-pro-team@latest/github/searching-for-information-on-github/searching-code


# yue's bert script folder
# sequence should be bert's output 1
# token should be bert's output 0

#https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fbert_slot_model.py&version=GBmaster&_a=contents


#TNLR
# unilm github
# https://github.com/microsoft/unilm/tree/master/unilm
#haoda
# yue ma
# 01072021
#only checkpoint
#need to find vocab.txt from bert (not distilled bert)
# it might have model_config.json
#     
#\\FSU\Shares\TuringShare\NLR_Models\Monolingual\NLRv1-Base-Uncased\model_config.json
#​[1:05 PM] Yue Ma
    
#self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]
#​[1:17 PM] Yue Ma
    
#https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fbert_intent_model.py&version=GBmaster&line=31&lineEnd=48&lineStartColumn=1&lineEndColumn=31&lineStyle=plain&_a=contents
#​[1:32 PM] Yue Ma

#https://github.com/onnx/tutorials/blob/master/tutorials/VisualizingAModel.md
#onnx/tutorialsTutorials for creating and using ONNX models. Contribute to onnx/tutorials development by creating an account on GitHub.github.com

# 01132021
# load pt
# \\FSU\Shares\TuringShare\NLR_Models\Monolingual\NLRv3-Base-Uncased
# use default bert_config snice not providing confi.json
# and you can change layer 
# ? nneed to figure out how ot do that
# here is to provide labels
# dimention you cannot change
#from transformers import BertConfig;
#bert_config = BertConfig();
#bert_config.num_labels = 12;
#bert_config.output_attentions = False;
#bert_config.output_hidden_states = False;





#onnix graph it repository
#https://github.com/onnx/tutorials/blob/master/tutorials/VisualizingAModel.md



# preprocessing
# preprocess_bert.py v.s train_horovod_slot.py
# different
# 1>  my training data is non consistent
#0	hey cortana look for my password document	file_search	files	look for <contact_name> my </contact_name> <file_keyword> password </file_keyword> document
# former:
# look for my password document	O O contact_name file_keyword O
# later:
# hey co ##rta ##na look for my password document	O file_keyword file_keyword file_keyword O O contact_name O O
# resoan 1 : query and traningi data inconsistent

# 2> Where is my music?	file_search	files	Where is <contact_name> my </contact_name> <file_type> music </file_type> ?
# former:
# where is my music ?	O O contact_name file_type O
# later:
# where is my music ?	file_type file_type contact_name O O
# reason:
# logic wrong while extracting slots


#3>
#0	shows me files I was composing.	file_navigate	files	shows me files <contact_name> I </contact_name> was <file_action> composing </file_action>.
# former:
# shows me files i was composing .	O O O contact_name O file_action O
# later:
# shows me files i was composing .	O O contact_name O O file_action O
# reason:
# logic wrong while extracting slots
# for 'i' when it searches index it will wrong
# so need to refer to the function 'splitWithBert()' 
# it also preprocess toekn inside xml pair <> <> then seperate it
# this is more ideal to do






# preprocessing script from yue
# [for tokenization]
# logic
# query : search spreadsheets saved after 3/12
#annotation_result_arrry:
#-1 : mean <> start and end 
#between -1 -1 it is the slot type
#-1 : will not a
#['O', -1, 'file_type', 'file_type', 'file_type', -1, -1, 'file_action', -1, -1, 'date', 'date', 'date', 'date', ...]

#word array : based on annnotated array / annotation_result_array to generate 
#get
#['search', 'spreads', '##hee', '##ts', 'saved', 'after', '3', '/', '12']


#annotation_filtered_array
#label array with string
#['O', 'file_type', 'file_type', 'file_type', 'file_action', 'date', 'date', 'date', 'date']


# 'adding cutoff length for query' section
#		do not nudetstand very well


# 'if not self.checkQueryValid(word_string)':
#		if exisitng 
#		not sure why needs this


# [for generating traninig data]
# i leave it at first right know
# generateTrainTestData

https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fpreprocess_bert.py&version=GBmaster&_a=contents



# if using yue's class and output loss , slot_output as multiple outputs
# it will be this error
# run 73 (child 75)
[2021-01-09T03:34:31.385345] The experiment failed. Finalizing run...
Starting the daemon thread to refresh tokens in background for process with pid = 153
Cleaning up all outstanding Run operations, waiting 900.0 seconds
2 items cleaning up...
Cleanup took 0.4081854820251465 seconds
Traceback (most recent call last):
  File "train_horovod_slot.py", line 1019, in <module>
    dynamic_axes = {'input_ids': {1: '?'}, 'loss': {1: '?'}, 'slot_output': {1: '?'}}
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/onnx/__init__.py", line 148, in export
    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/onnx/utils.py", line 66, in export
    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/onnx/utils.py", line 416, in _export
    fixed_batch_size=fixed_batch_size)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/onnx/utils.py", line 279, in _model_to_graph
    graph, torch_out = _trace_and_get_graph_from_model(model, args, training)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/onnx/utils.py", line 236, in _trace_and_get_graph_from_model
    trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(model, args, _force_outplace=True, _return_inputs_states=True)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/jit/__init__.py", line 277, in _get_trace_graph
    outs = ONNXTracedModule(f, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/jit/__init__.py", line 360, in forward
    self._force_outplace,
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/jit/__init__.py", line 350, in wrapper
    out_vars, _ = _flatten(outs)
RuntimeError: Only tuples, lists and Variables supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. But got unsupported type NoneType

[2021-01-09T03:34:32.048969] Finished context manager injector with Exception.
#RuntimeError: Only tuples, lists and Variables supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. But got unsupported type NoneType
https://blog.csdn.net/qq_33120609/article/details/105857725


#BertForTokenClassification
https://github.com/DavidNemeskey/emBERT/blob/62825a1ef6b7d1e1eee8b8bf4644281c17860670/embert/model.py
#BertForTokenClassification
#https://github.com/DavidNemeskey/emBERT/blob/26faed05408ba8651020fef1ae8d07f331b5cd86/scripts/train_embert.py
#model_dir
#config = BertConfig.from_pretrained(
#                model_dir,
#                # os.path.join(args.bert_model, 'bert_config.json'),
#                num_labels=num_labels, finetuning_task=args.task_name
#            )

# class definition
# here return needs to be tuple
# in class 
# class TokenClassifier(BertForTokenClassification):
#return (loss, logits)




# yue's cpmmen


#RuntimeError: Only tuples, lists and Variables supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. But got unsupported type NoneType
# related ticket and issues
#https://github.com/pytorch/pytorch/issues/42391

#torch._C._nn.nll_loss?
#https://discuss.pytorch.org/t/where-is-torch-c-nn-nll-loss/9769





# hyper paramter tunning 
#https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b



#BertConfig
# specify layer
# how to verify,
# using output config.json to verify
#"num_hidden_layers": 12,
# Number of trainable parameters: 108907797 
#"num_hidden_layers": 3,
#Number of trainable parameters:    45116949 
#https://rdrr.io/github/jonathanbratt/RBERT/man/BertConfig.html
#https://www.deepspeed.ai/tutorials/bert-pretraining/




#joint intent and slot filling
# yues code as reference but following internet at first
# https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fbert_intent_slot.py&version=GBmaster&_a=contents
# option 1 and paper
# has good table for comparison
#https://arxiv.org/pdf/1902.10909.pdf
https://github.com/monologg/JointBERT
# intent label
#https://github.com/monologg/JointBERT/blob/master/data/snips/dev/label
# query 
# https://github.com/monologg/JointBERT/blob/master/data/snips/dev/seq.in
# slot output
# https://github.com/monologg/JointBERT/blob/master/data/snips/dev/seq.out
# for intent
# ? i do not need unk snice i have files_others or might be i need to have another default
# check MDM
# i can use current prod model to output intent for each slot data 
# (current slot data has intent  but might not e correct)
# if domain score >=0.35 , then intent should be valid, otherwise, setup it
# as default OTHER intent if needed
# convert_examples_to_features()
# this will generate features including intent. need to see how data being feed in
# class TensorDataset to store
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/main.py#L13
# here stores tensor dataset
# and feed to trainer class
# class trainer, train() function
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L15
# also using linear warmp and decay
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L87
# here output class and input has all arguments needed
# self.model_class.from_pretrained() pass classs
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L29
# here define possible class can be loaded
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/utils.py#L14
# here using **inputs to pass all necessary algorithm
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L87
# ? not sure about whether batch size will affect or not
#https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L87
# for self.bert, providng token_type_ids
# ? for my setting and yue's setting we do not do that might be uncessary
# it is necessary when calling bertModel
#  0 for the first part, 1 for the second part
# ? but question answer might need it ?
# https://huggingface.co/transformers/glossary.html
# https://huggingface.co/transformers/model_doc/bert.html
#https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/modeling_jointbert.py#L24
# [0] for sequence output (token classfication)
# [1] for intent classficaition output (sequence classfication)
# loss calculation
#total_loss = intent_loss + coef * slot_loss (Change coef with --slot_loss_coef option)
# (in yue's code, it defines two losses, one for intent and othe other for slot)
# total_loss = intent_loss + slot_loss;
# total_loss.backward() <= and sum them up to do backward propagation
# in debugging mode, tensor(2.4028) + tensor(3.0916) = tensor(5.4944)
# ? not sure how this backprog works  but internet also use single lost to backward propagate
# https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fbert_intent_slot.py&version=GBmaster&_a=contents
# define intent_logits class
# drop out rate the same bert
# hidden size dimention the same bert
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/module.py#L4
# define slot_logits class
# hidden size dimention the same bert
#https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/module.py#L15
# ouput is a tuple 
# ? but it looks like this will fail onnx, can try to have two outputs or two class
#https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/modeling_jointbert.py#L59
# no care about inference for onnx
# need to follow what yue does
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/modeling_jointbert.py#L59
# ignore_index is ignored in function
# not sure wy need this
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/modeling_jointbert.py#L48 
# word_tokens = [unk_token]  # For handling the bad-encoded word
# ? not sure whether we need to hanlde this or not, or check yue's code
# ? can add test ihe futurue
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/data_loader.py#L148

# evaluation
# intent_logits.detach().cpu().numpy() generates array()
#array([[ 2.39753183e-02, -1.00037996e-02,  2.51673833e-02,
        -4.94064158e-03, -1.02473386e-02,  1.26954960e-02,
        -1.67937279e-02,  6.12567812e-02,  2.03567557e-04,
         2.56004222e-02,  6.15039468e-02],
       [ 2.23806202e-02, -1.10518197e-02,  2.86248215e-02,
        -3.70978750e-03, -1.03999898e-02,  1.11508286e-02,
        -1.12427333e-02,  6.14888370e-02, -9.67073254e-04,
         2.52279546e-02,  6.27490729e-02]])
# each is np.array
>>> np.append(a, b, axis=0)
array([[1, 2],
       [3, 4],
       [2, 1],
       [4, 3]])
#np.argmax(c, axis=1)
>>> np.argmax(c, axis=1)
array([1, 1, 0, 0], dtype=int64)
# intent has the first one
# 
#https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L161


# also has function to calculate metrics (itent, slot)



# tensor.backward()
#
# will use the chain rule to compute the gradient for every parameter in the network. For a give param, w of size d, it will perform: gradient * dy/dw where dy/dw will will be computed by the chain rule.
# https://discuss.pytorch.org/t/what-does-tensor-backward-do-mathematically/27953


# QAS output 
# # for domain, it needs to have probabilty
# files_enus_mv7_domain_svm_score (tag: 1, string: 0)
# 0[-1,-1]=0.1968164
# for intent
# files_enus_mv7_intent_svm_score (tag: 11, string: 0)
 #                       0[-1,-1]=2.2269628
 #                       1[-1,-1]=-1.5521804
 #                       2[-1,-1]=-1.6113045
 #                       3[-1,-1]=-1.0233102
 #                       4[-1,-1]=-1.3408698
 #                       5[-1,-1]=-1.8470569
 #                       6[-1,-1]=-1.0570247
 #                       7[-1,-1]=-1.167596
 #                       8[-1,-1]=-1.8137677
 #                       9[-1,-1]=-0.99997
 #                       10[-1,-1]=-1.389961





# how to get tensor's value
 # Consider using variable.item() instead for 0-dim tensors. For higher-dimensional variables, use variable.tolist().
# https://discuss.pytorch.org/t/get-value-out-of-torch-cuda-float-tensor/2539/11




# nn.linear
#https://pytorch.org/docs/stable/generated/torch.nn.Linear.html



# good paper
#https://zhuanlan.zhihu.com/p/143123368



#yue's joint intent and slot
# not sure how this function works
# ? get_lexicon_vector might be can be checked
# https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fevaluate_bert.py&version=GBmaster&_a=contents
# in another file, it is truly being used here
# (by search function in repo)
# https://msasg.visualstudio.com/LanguageUnderstanding/_search?action=contents&text=get_lexicon_vector&type=code&lp=code-Project&filters=ProjectFilters%7BLanguageUnderstanding%7DRepositoryFilters%7BTimex_Deep_Model%7D&pageSize=25&result=DefaultCollection%2FLanguageUnderstanding%2FTimex_Deep_Model%2FGBmaster%2F%2FTimexModelScripts%2FTimexModelScripts%2FTensorflow%2Fevaluate_model_hypertuning.py


# evaluate_model()
# do not go with batch size
# [for slot]
# each query then evaluate model to get output
# predicted_labels_slot = list(map((lambda x: self.slot_dict_rev[x]), slot_result));
# this also 1-d list
# predicted_slot_arrays = self.create_slot_arrays_iob(predicted_labels_slot)
# predicted_slot_arrays will output
# key : contact_name , list : [[1,2], [3,4]]
# 1-2 is the span to have contact name
# golden_set=set(map(tuple, golden_slot_arrays[label]))
# [[1,2], [3,4]] =>((1,2) , (3,4))
# set is for type, map just tuple for each label's sublist
# since set , order does not mater
# python set operation
#https://www.geeksforgeeks.org/python-set-operations-union-intersection-difference-symmetric-difference/
# [for intent]



# pandas read empty string , not NAN, by setting an option
# https://stackoverflow.com/questions/10867028/get-pandas-read-csv-to-read-empty-values-as-empty-string-instead-of-nan/43832955




# bert output
# paper 
# https://arxiv.org/abs/1810.04805
# the authors describe two ways to work with BERT, 
# 1> one as with “feature extraction” mechanism. 
# That is, we use the final output of BERT as an input to another model. 
# This way we’re “extracting” features from text using BERT and then use it in a separate model for the actual task in hand. 
# 2> The other way is by “fine-tuning” BERT. 
# That is, we add additional layer/s on top of BERT and then train the whole thing together. 
# This way, we train our additional layer/s and also change (fine-tune) the BERTs weights.
# below link using the seocnd 
# [1] : pool output , for sequence claissfcaiotn
# [0]: sequence output (miusleading name) for token claasfication
# https://www.kaggle.com/questions-and-answers/86510
#Let's take an example of "You are a kaggle kernels master".
#Before passing it to bert model you need to add [CLS] token in the begining and [SEP] token at the end of the sentence.
#
#Now the sentence is "[CLS] You are a kaggle kernels master [SEP]".#
#
#Now if you give above sentence to BertModel you will get 768 dimension embedding for each token in the given sentence. So 'sequence output' will give output of dimension [1, 8, 768] since there are 8 tokens including [CLS] and [SEP] and 'pooled output' will give output of dimension [1, 1, 768] which is the embedding of [CLS] token.

# https://towardsdatascience.com/bert-to-the-rescue-17671379687f
# https://github.com/huggingface/transformers/issues/1827
# https://github.com/shudima/notebooks/blob/master/BERT_to_the_rescue.ipynb

# comment until here
 

#https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1

#https://github.com/DavidNemeskey/emBERT/blob/62825a1ef6b7d1e1eee8b8bf4644281c17860670/embert/model.py




# torch.max
#https://www.journaldev.com/39463/pytorch-torch-max

# torch sensor iterative
# https://stackoverflow.com/questions/62791942/how-do-i-iterate-over-pytorch-2d-tensors


# add_special_tokens=True
# <done>
# can add special toekn
# default is true
# text = "[CLS] a visually stunning rumination on love [SEP]"
# version 1
#fast_tokenized_text_old = fast_tokenizer.tokenize(text)
# version 2
#fast_tokenized_batch : BatchEncoding = fast_tokenizer(text)
#fast_tokenized_text :Encoding  =fast_tokenized_batch[0]
# version1 will not double
# version 2 will add duplicated

fast token ouput version 1: ['[CLS]', 'a', 'visually', 'stunning', 'rum', '##ination', 'on', 'love', '[SEP]']
fast token ouput version 2: ['[CLS]', '[CLS]', 'a', 'visually', 'stunning', 'rum', '##ination', 'on', 'love', '[SEP]', '[SEP]']

# with only a single
# text = "[CLS] a visually stunning rumination on love"
# version 1
#fast_tokenized_text_old = fast_tokenizer.tokenize(text)
# version 2
#fast_tokenized_batch : BatchEncoding = fast_tokenizer(text)
#fast_tokenized_text :Encoding  =fast_tokenized_batch[0]
# version1 will compelte it but not double
# version2 will compelte it and double
#fast token ouput version 1: ['[CLS]', 'a', 'visually', 'stunning', 'rum', '##ination', 'on', 'love', '[SEP]']
#fast token ouput version 2: ['[CLS]', '[CLS]', 'a', 'visually', 'stunning', 'rum', '##ination', 'on', 'love', '[SEP]', '[SEP]']


# ? can verify in the future to see 
# ? for domain traing(version) and slot traninig(version 2) , what should be the correct way
# ? if add to onnx or traning data does it affect or not

#https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/
#https://www.cnblogs.com/dogecheng/p/11907036.html



# torch.tensor(box)[None, :]
#RuntimeError: Could not infer dtype of NoneType
#abnormal value in column
#https://juejin.cn/post/6844904158039015431



# check isna in pandas
#https://datatofish.com/rows-with-nan-pandas-dataframe/



# save to onnx more examples
# https://www.programcreek.com/python/example/126284/transformers.BertTokenizer.from_pretrained









# inherited DistilBertPreTrainedModel
https://github.com/acsyl/transquest_vis/blob/42ece1a6f760681cf8498bb11d0a1627412aa244/algo/transformers/models/distilbert_model.py
https://github.com/SeniorDev009/simpletransformers/blob/b094a5b14adbbd5bf5f88c018bb9e09f9161a9c3/simpletransformers/classification/transformer_models/distilbert_model.py
https://github.com/TharinduDR/STS-Transformers/blob/f6e505d4b780486117bed9108d534b13010c9c8d/ststransformers/models/distilbert_model.py
https://github.com/lucmichalski/dmoz-utils/blob/4d15397ca7205d6b5ea243ae2f19d75db03e04c1/classifier/simpletransformers/simpletransformers/experimental/classification/transformer_models/distilbert_model.py
https://github.com/mfomicheva/TransQuest/blob/47445cf90690f57dcab1c4b416f83aa903b7f2e2/algo/transformers/models/distilbert_model.py
https://github.com/ym001/DAIA/blob/f58fed316e1599926f5457312ea0806e6a5da4e3/models/distilbert_model.py
#https://github.com/TharinduDR/STS-Transformers/blob/f6e505d4b780486117bed9108d534b13010c9c8d/ststransformers/models/distilbert_model.py
https://github.com/eduros93/my_simpletransformers/blob/ccc5c715bd8d682d5a8f68fbddaa8a25f4da6351/simpletransformers/experimental/classification/transformer_models/distilbert_model.py#L29



#inherited DistilBertForSequenceClassification
# this is the example i am looking for to define forward function
# oonly return logits
https://github.com/johannesmelsbach/fast-transformers/blob/63fb37515385b6dc6737c657ebd6eb48026c1d7b/src/fastformer/fastdistilbert.py
# reutrn loss + logits












#DistilBertForTokenClassification
# official tutorial
#https://huggingface.co/transformers/model_doc/distilbert.html
# joint slot and intent
https://github.com/monologg/JointBERT
# token level to sub token level token_labels_to_subtoken_labels()
# but using tensor flow...
# format : rumination -> rum ##ination 
# ? might be ## is a key 
# https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/

# http://docs.deeppavlov.ai/en/master/features/models/bert.html
#http://docs.deeppavlov.ai/en/master/_modules/deeppavlov/models/torch_bert/torch_bert_sequence_tagger.html
#https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/models/bert/bert_sequence_tagger.py#L2
#https://github.com/deepmipt/DeepPavlov/blob/b66179e584d3eb6da73c5731ba7b732dab7e94bd/examples/Pseudo-labeling%20for%20classification.ipynb



# using bert
# https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification/blob/master/run_sequence_labeling.py
# good repo to do that
# BertFastTokenizer
# format 1 : does not tell spanns start and end
# format 2: do tell span start and end, BIO schema (https://www.lighttag.io/blog/sequence-labeling-with-transformers/)
# https://github.com/LightTag/sequence-labeling-with-transformers

# another DistilBertTokenizerFast
# https://huggingface.co/transformers/custom_datasets.html


# CRFon top of BERT
#  does it wokr ot not
# https://www.reddit.com/r/LanguageTechnology/comments/g45nyv/is_putting_a_crf_on_top_of_bert_for_sequence/
# https://blog.csdn.net/qq_16949707/article/details/105742383


# pytorch CRF
#https://www.shuzhiduo.com/A/o75N0m8XzW/


###########################
# yue email slot model below
# add my model setup to test domain
# 
###########################
# file : cortana_email_enus_mv2_dev.bert-uncased-vocab.txt
# =>should be vocab.txt generated by training

# file L cortana_email_enus_mv2_dev.model.onnx.bin
# => should be traced_distill_bert.onnx.bin

# file : cortana_email_enus_mv2_dev.model.onnx.config.txt 
# i rename it to files_enus_mv5.domain.model.onnx.config.txt

#[model]
## onnx bin
#name = cortana_email_enus_mv2_dev.model.onnx.bin
# => name = traced_distill_bert.onnx.bin
#[inputs]
# featureset = input_ids : TAG      name = input_ids      type = INT64      shape = 1,-1      padding = NONE
# input_ids, should be the same as  DNNProcessor's input_ids
# ? what does TAG mean here
# ? how to decide type
# ? what does shape mean here
# ? what odes padding= NONE here mean
# => in my case i am using the same thing

# attention_maskfeatureset = attention_mask : TAG      name = attention_mask      type = INT64      shape = 1,-1      padding = NONE
# ? why needs attention_mask,i do not provide it at first
# same questions as input_ids
# refer to this 
# https://github.com/vilcek/fine-tuning-BERT-for-text-classification/blob/master/02-data-classification.ipynb
# it has the same length of input_ids
# ? Studying padding in the future since it is related
# => in my case i do not provide it

#[outputs]
# featureset = slot_output : WEIGHT      name = slot_output 
# ? not sure if INt 64 type is needed since it is configured durying training but i follow up at first
# it needs to be aigned with output port torch.onnx.export
# ? but name seems unreated to output format of class DistilBertPreTrainedModel
# in class DistilBertForSequenceClassification
# when return_dict is none in argument (config set it as true)
# will return
#         return SequenceClassifierOutput(
#            loss=loss,
#            logits=logits,
#            hidden_states=distilbert_output.hidden_states,
#            attentions=distilbert_output.attentions,
#        )
# => in my code i renmae it to 
# featureset = domain_output : WEIGHT      name = domain_output


# file : cortana_email_cortana_email_enus_mv2_dev.slot.deep.model.pipeline.txt
# => files_enus_mv5.domain.deep.model.pipeline.txt
# ? content not yet verified but need to follow slot



###########################
# yue email slot model above
# add my model setup to test domain
# 
###########################


###########################
# yue email slot model below
# add my model setup to test domain
# <this is obsolete, please refer to a new section>
###########################


# compare to tutorial
#https://msasg.visualstudio.com/Cortana/_git/CoreScienceDataStaging/pullrequest/1997902?_a=files&path=%2Fmodels%2Femail_pme%2Fdeveloper%2Fcortana_email_enus_mv2_dev.slot.deep.model.pipeline.txt
# file : cortana_email_enus_mv2_dev.bert-uncased-vocab.txt
# =>should be vocab.txt generated by training
# cortana_email_enus_mv2_dev.model.onnx.bin
# should be traced_distill_bert.onnx.bin
# file : cortana_email_enus_mv2_dev.model.onnx.config.txt (files_enus_mv5.slot.model.onnx.config.txt)
# https://msasg.visualstudio.com/DefaultCollection/QAS/_wiki/wikis/QAS.wiki/48023/Deploying-ONNX-Models-in-QAS
# follow this link to write your own
#[model]
## onnx bin
#name = cortana_email_enus_mv2_dev.model.onnx.bin
#[inputs]
# input_ids, should be the same as  DNNProcessor's input_ids
# ? what does TAG mean here
# ? how to decide type
# ? what does shape mean here
# ? what odes padding= NONE here mean
# attentopn_mask
# ? why needs attention_mask,i do not provide it at first
# same questions as input_ids
# refer to this 
# https://github.com/vilcek/fine-tuning-BERT-for-text-classification/blob/master/02-data-classification.ipynb
# it has the same length of input_ids
# ? Studying padding in the future since it is related

#[outputs]
# slot output, should be the same as  DNNProcessor's input_ids
# ? not sure if INt 64 type is needed since it is configured durying training but i follow up at first

featureset = input_ids : TAG      name = input_ids      type = INT64      shape = 1,-1      padding = NONE
featureset = attention_mask : TAG      name = attention_mask      type = INT64      shape = 1,-1      padding = NONE
[outputs]
featureset = slot_output : WEIGHT      name = slot_output



# cortana_email_enus_mv2_dev.model.onnx.warmup.txt
# ? not sure why needs this file
# by looking at the content of PR
#  it specific input_ids and attention_mask
#  in my first case , i only specific input_ids
# input_ids	6	0	101	1	0	0	3191	1	1	1	2026	1	2	2	22028	1	3	3	2055	1	4	4	102	1	5	5
# format likes this 
# 6 0 specicy 6 features 
# kth feature
# (feature id from bert, 1, k,k)
# ? not sure the correct format but can go with this setting at first


# for DNNPrcessor
# https://msasg.visualstudio.com/DefaultCollection/QAS/_wiki/wikis/MLG%20Processors/48552/DNNProcessor
# different -runtime will be provided regarding CPU or GPU models and it uses CPU
# in email, no externa lsegmenIDs to provide
# ? not sure if gpu-trained model can be used cpu to run or not
# ? how to filter based on offset
# since tags are the same 
    domain_output (tag: 2, string: 0)
                        0[0,0]=6.409485
                        0[1,1]=-7.704887
# method1 : but this does not apply to intent
#1>
#usinf feature shift to shift one and only one(label  =1) left
#2>
#usinf value aggregator to change it from x[0,0] to  x[-1,-1] 
#3>
#then normalzied (might be normalized does not need )

# result
# 1> not promising result
    run 109 (111 as child ) setup temporary
        # EG: show my file# wrong 
        # normal domain output
            Output:
                files_enus_mv5_domain_svm_score (tag: 1, string: 0)
                    0[-1,-1]=0.8508758
        #DNN output 
        # in my traninig ,positive example label is 1 , negative example label is 0
            Output:
                domain_output (tag: 2, string: 0)
                        0[0,0]=0.115999304
                        0[1,1]=-0.025065321
        # eg: send message to tom
        # wrong 
            Output:
                domain_output (tag: 2, string: 0)
                        0[0,0]=-0.044760797
                        0[1,1]=0.03344066


    run 115 (114 as child ) setup temporary
        # EG: show my file
            # normal domain output
            Output:
                domain_default_linear (tag: 1, string:  0)
                        0[-1,-1]=1.4813591
9: FeatureNormalizer --in=domain_default_linear --out=f0)         v5_domain_svm_score --files_enus_mv5_domain_svm_score --norm=sigmoid        
            Inputs:                                        iles_enus_m
                domain_default_linear (tag: 1, string:  0)
                        0[-1,-1]=1.4813591             0)
            Output:                                                   )
                files_enus_mv5_domain_svm_score (tag: 1
1, string: 0)                                          , string: 0
                        0[-1,-1]=0.8147778
            #DNN output 
            # in my traninig ,positive example label is 1 , negative example label is 0
            Output:
                domain_output (tag: 2, string: 0)
                        0[0,0]=-3.5034482
                        0[1,1]=4.7909913

        # eg: send message to tom
            # normal domain output
                    domain_default_linear (tag: 1, string: 0)
                        0[-1,-1]=-2.8886678
9: FeatureNormalizer --in=domain_default_linear --out=files_enus_mv5_domain_svm_score --norm=sigmoid
        Inputs:
                domain_default_linear (tag: 1, string: 0)
                        0[-1,-1]=-2.8886678
        Output:
                files_enus_mv5_domain_svm_score (tag: 
1, string: 0)
                        0[-1,-1]=0.0527166

            #DNN output 
            # in my traninig ,positive example label is 1 , negative example label is 0
            Output:
                domain_output (tag: 2, string: 0)
                        0[0,0]=6.409485
                        0[1,1]=-7.704887
# 2>
# using pytorch_model.bin will show up error. it cannot be loaded by QAS
# verifyed by 'run 109 (111 as child ) setup temporary'




###########################
# yue email slot model above
# <this is obsolete, please refer to a new section>
###########################


###########################
# yue email slot model below
# add my model setup to test slot
# 
###########################
# file : cortana_email_enus_mv2_dev.bert-uncased-vocab.txt
# =>should be vocab.txt generated by training

# file L cortana_email_enus_mv2_dev.model.onnx.bin
# => should be traced_distill_bert.onnx.bin

# file : cortana_email_enus_mv2_dev.model.onnx.config.txt 
# i rename it to files_enus_mv5.slot.model.onnx.config.txt

#[model]
## onnx bin
#name = cortana_email_enus_mv2_dev.model.onnx.bin
# => name = traced_distill_bert.onnx.bin
#[inputs]
# featureset = input_ids : TAG      name = input_ids      type = INT64      shape = 1,-1      padding = NONE
# input_ids, should be the same as  DNNProcessor's input_ids
# ? what does TAG mean here
# ? how to decide type
# ? what does shape mean here
# ? what odes padding= NONE here mean
# => in my case i am using the same thing

# attention_maskfeatureset = attention_mask : TAG      name = attention_mask      type = INT64      shape = 1,-1      padding = NONE
# ? why needs attention_mask,i do not provide it at first
# same questions as input_ids
# refer to this 
# https://github.com/vilcek/fine-tuning-BERT-for-text-classification/blob/master/02-data-classification.ipynb
# it has the same length of input_ids
# ? Studying padding in the future since it is related
# => in my case i do not provide it

#[outputs]
# featureset = slot_output : WEIGHT      name = slot_output 
# ? not sure if INt 64 type is needed since it is configured durying training but i follow up at first
# it needs to be aigned with output port torch.onnx.export
# ? but name seems unreated to output format of class DistilBertPreTrainedModel
# in class DistilBertForSequenceClassification
# when return_dict is none in argument (config set it as true)
# will return
#        return TokenClassifierOutput(
#            loss=loss,
#            my new change
#            logits=torch.argmax(logits.view(-1, self.num_labels), dim=1),
#            hidden_states=outputs.hidden_states,
#            attentions=outputs.attentions,
#        ) 
# => in my code i renmae it to 
# featureset = slot_output : WEIGHT      name = slot_output


# file : cortana_email_cortana_email_enus_mv2_dev.slot.deep.model.pipeline.txt
# => files_enus_mv5.slot.deep.model.pipeline.txt
# max token cnt change sfrom 1024 to 512
#WordPieceTokenizer --in=ExternalInput1 --out=tokens,input_ids,attention_mask,segment_ids --maxTokenCount=512 --unknownToken=[UNK] --vocabFile=vocab.txt --startToken=[CLS] --endToken=[SEP]
#StringFeatureSetInfo --in=tokens --out=SequenceLength --maxTokenLength=999

# dnn output changes to slot_output
# DNNProcessor --in=input_ids --out=slot_output --config=files_enus_mv5.slot.model.onnx.config.txt --runtime=Onnx-Latest --warmup=files_enus_mv5.slot.model.onnx.warmup.txt

# same as email domain
#SubwordToWord --in=tokens,slot_output --out=words,words_output --ignoreTokens=[CLS],[SEP]

# update related file name
#TagTranslator --in=words_output --out=tag_output --function=WeightToTag
#FeatureNormalizer --in=tag_output --out=files_enus_mv5_slot_dnn_tag --norm=sign --cutoff=-0.5


###########################
# yue email slot model below
# add my model setup to test slot
# 
###########################




###########################
# yue email slot model below real query outproof - in search gold folder
###########################

query : send email to tom

0: WordPieceTokenizer --in=ExternalInput1 --out=tokens,input_ids,attention_mask,segment_ids --maxTokenCount=1024 --unknownToken=[UNK] --vocabFile=cortana_email_enus_mv2_dev.bert-uncased-vocab.txt --startToken=[CLS] --endToken=[SEP]
        Inputs:
                ExternalInput1 (tag: 0, string: 4)
                        send[0,0]=1.0
                        email[1,1]=1.0
                        to[2,2]=1.0
                        tom[3,3]=1.0
        Output:
                tokens (tag: 0, string: 6)
                        [CLS][0,0]=1.0
                        send[0,0]=1.0
                        email[0,0]=1.0
                        to[0,0]=1.0
                        tom[0,0]=1.0
                        [SEP][0,0]=1.0
                input_ids (tag: 6, string: 0)
                        101[0,0]=1.0
                        4604[1,1]=1.0
                        10373[2,2]=1.0
                        2000[3,3]=1.0
                        3419[4,4]=1.0
                        102[5,5]=1.0
                attention_mask (tag: 6, string: 0)
                        1[0,0]=1.0
                        1[1,1]=1.0
                        1[2,2]=1.0
                        1[3,3]=1.0
                        1[4,4]=1.0
                        1[5,5]=1.0
                segment_ids (tag: 6, string: 0)
                        0[0,0]=1.0
                        0[1,1]=1.0
                        0[2,2]=1.0
                        0[3,3]=1.0
                        0[4,4]=1.0
                        0[5,5]=1.0
1: StringFeatureSetInfo --in=tokens --out=SequenceLength --maxTokenLength=999
        Inputs:
                tokens (tag: 0, string: 6)
                        [CLS][0,0]=1.0
                        send[0,0]=1.0
                        email[0,0]=1.0
                        to[0,0]=1.0
                        tom[0,0]=1.0
                        [SEP][0,0]=1.0
        Output:
                SequenceLength (tag: 1, string: 0)
                        0[-1,-1]=6.0
2: DNNProcessor --in=input_ids,attention_mask --out=slot_output --config=cortana_email_enus_mv2_dev.model.onnx.config.txt --runtime=Onnx-Latest --warmup=cortana_email_enus_mv2_dev.model.onnx.warmup.txt
        Inputs:
                input_ids (tag: 6, string: 0)
                        101[0,0]=1.0
                        4604[1,1]=1.0
                        10373[2,2]=1.0
                        2000[3,3]=1.0
                        3419[4,4]=1.0
                        102[5,5]=1.0
                attention_mask (tag: 6, string: 0)
                        1[0,0]=1.0
                        1[1,1]=1.0
                        1[2,2]=1.0
                        1[3,3]=1.0
                        1[4,4]=1.0
                        1[5,5]=1.0
        Output:
                # look like preprocessing output is done already
                # 21 ? might be the slot id ?
                slot_output (tag: 6, string: 0)
                        0[0,0]=0.0
                        0[1,1]=0.0
                        0[2,2]=21.0
                        0[3,3]=0.0
                        0[4,4]=1.0
                        0[5,5]=0.0
3: SubwordToWord --in=tokens,slot_output --out=words,words_output --ignoreTokens=[CLS],[SEP]
        Inputs:
                tokens (tag: 0, string: 6)
                        [CLS][0,0]=1.0
                        send[0,0]=1.0
                        email[0,0]=1.0
                        to[0,0]=1.0
                        [SEP][0,0]=1.0
                        [SEP][0,0]=1.0
                slot_output (tag: 6, string: 0)
                        0[0,0]=0.0
                        0[1,1]=0.0
                        0[2,2]=21.0
                        0[3,3]=0.0
                        0[4,4]=1.0
                        0[5,5]=0.0
        Output:
                words (tag: 0, string: 4)
                        send[0,0]=1.0
                        email[1,1]=1.0
                        to[2,2]=1.0
                        tom[3,3]=1.0
                words_output (tag: 4, string: 0)
                        0[0,0]=0.0
                        0[1,1]=21.0
                        0[2,2]=0.0
                        0[3,3]=1.0
4: TagTranslator --in=words_output --out=tag_output --function=WeightToTag       
        Inputs:
                words_output (tag: 4, string: 0)
                        0[0,0]=0.0
                        0[1,1]=21.0
                        0[2,2]=0.0
                        0[3,3]=1.0
        Output:
                tag_output (tag: 4, string: 0)
                        0[0,0]=0.0
                        21[1,1]=21.0
                        0[2,2]=0.0
                        1[3,3]=1.0
5: FeatureNormalizer --in=tag_output --out=cortana_email_enus_slot_dnn_tag --norm=sign --cutoff=-0.5      =sign --cutoff=-0.5
        Inputs:
                tag_output (tag: 4, string: 0)
                        0[0,0]=0.0
                        21[1,1]=21.0
                        0[2,2]=0.0
                        1[3,3]=1.0
        Output:
                # so this is just to remove weight
                cortana_email_enus_slot_dnn_tag (tag: 4, string: 0)
                        0[0,0]=1.0
                        21[1,1]=1.0
                        0[2,2]=1.0
                        1[3,3]=1.0







###########################
# yue email slot model below real query outproof - in search gold  folder
###########################



###########################
# git branch to test below
# branch for testing
###########################

# DNN output domain_output
# add softmax to test
#users/chiecha/files_bert_domain_qas_setup_12212020




# DNN output slot_output
# add softmax to test
#users/chiecha/files_bert_slot_qas_setup_12282020


###########################
# git branch to test above
# branch for testing
###########################

