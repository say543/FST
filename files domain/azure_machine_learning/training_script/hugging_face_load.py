

# huggingface model page
#https://huggingface.co/transformers/main_classes/model.html



# ? not sure why having this
#https://discuss.pytorch.org/t/assertionerror-torch-not-compiled-with-cuda-enabled-unable-to-predict-on-local-machine-cpu/88183



# tutorial simlar to  train_horovod.py
# <done> verified in model_saved_test.py
# explian this code is to save 'If we have a distributed model, save only the encapsulated model'
# model_to_save = model.module if hasattr(model, 'module') else model
# ? not sure how state_dict() worrks and compare to (torch.save(model, os.path.join(out_dir, 'model.pt')))
# test in the futrue
# torch.save(model_to_save.state_dict(), output_model_file)
# not sure how it is different from  originla pretrain output, test it in the future ?
# <done> verified in model_saved_test.py, two versions of saving those files
# (model_to_save.save_pretrained(out_dir)
#    tokenizer.save_pretrained(out_dir))
# model_to_save.config.to_json_file(output_config_file)
# tokenizer.save_vocabulary(output_vocab_file)

# for reload model
# ? not sure if BertConfig can be used for distillation bert. need to test
# version 1: using old transformer model, load each file speficially 
#https://huggingface.co/transformers/v1.2.0/serialization.html
# version 2: using a output folder to load
#https://zhuanlan.zhihu.com/p/143209797
#https://mccormickml.com/2019/07/22/BERT-fine-tuning/



# output to onnx
# god example to preovide text / tokenized text as input for bert or distillation bert
# torch save and load model  for Ber class
# also good for inference
#https://huggingface.co/transformers/serialization.html



# fine tune  procedure using adam
# using adam
# also Token Classfication (slot tagging example for discuss) 
# can use this as reference
# DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))
# easy code
# hugging face dataset
# https://huggingface.co/transformers/custom_datasets.html



# torch officalto onnx offical github link
#https://github.com/pytorch/pytorch/blob/master/docs/source/onnx.rst


#microsoft pytorach model using ONNX
# similar to officla onnx github but has runtime performance profiling result
#https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21368-operationalizing-pytorch-models-using-onnx-and-onnx-runtime.pdf



#distill bert 
# loss function discusssion
#https://blog.csdn.net/fengzhou_/article/details/107211090?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control

#BertforSequenceClassification
## code explanation
# logits is liear output(each label's probability) but not with softmax (https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)
## BertforSequenceClassification = bert + dropout + linear + classifier
## bert-base-cased: for english
# (? in my code, i use distilbert-base-uncased for distilbert)
## tokens_tensor: input queriy token index
## segments_tensor: for sentence boundary , for example : sentence 1 (all 0) + [SEP](0) + sentence 2(all 0)
## masks_tensor: attention range , 1: attention should be done 0: padding , no need 
# ? similar to att_masks i use in my code
# pad token : for each batch we need to do zro padding to make sure all patchs having the same length 
# also masks_tensor/segments_tensor  should be zero for postion where you have 'pad token'
## has defined FakeNewsDataset class to define dataset
# __getitem__ : return a single traning data(tokens_tensor, segments_tensor, label_tensor) compatible with Bert format
# __len__ : return number of training data
# in my code, i use class tensorDataset / class DataLoader
# ? but i only provide tokens_tensor, how doesit work
## has defined DataLoader  class to assign mini batch
# use input from FakeNewsDataset traning data(tokens_tensor, segments_tensor, label_tensor)
# add zero paddong to (tokens_tensor, segments_tensor) by function pad_sequence(from torch.nn.utils.rnn import pad_sequence)
# generate masks_tensors, 1: attention should be done 0: padding , no need 
## has code to extract a small batch size
# in my code, i use class DataLoader 
# no segment_tenors snice only single sentence 
# masks_tensors is provided as part of training_data
# it is generated by att_masks based on various length of each query
# no padding 
# it is done by max_length and 'padding='
# text_ids = [tokenizer.encode(text, max_length=300, padding='max_length', truncation=True) for text in texts]
# has batch size argument
## has example code to show up model architecture by model.named_children()
## has example code to calculate accuracy
# def get_predictions(model, dataloader, compute_acc=False):
# tokens_tensors, segments_tensors, masks_tensors = data[:3]
# in my code
# for k, (mb_x, mb_m, mb_y) in enumerate(train_dataloader):

## has example to output parameters
# def get_learnable_params(module):
#整個分類模型的參數量：102269955
#線性分類器的參數量：2307
# in my code
# def count_parameters(model):
# number of parameters 66955010 
# ? why my experiment having less parameters but takes longer time to train
# ? might be related to padding i add as 300, test in the future


## has example code to train
# 將參數梯度歸零
# optimizer.zero_grad()
# # forward pass
# backword
# ? in my code, function has little bit different. need to study in the future
# for k, (mb_x, mb_m, mb_y) in enumerate(train_dataloader):


# PySnooper : open source to output trace information
# can test it in the future
# https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html
# https://blog.csdn.net/SZU_Hadooper/article/details/102490443
# https://mccormickml.com/2019/07/22/BERT-fine-tuning/


# basicsuage for different models 
#basic：
bertModel
bertTokenizer
#pretrain
bertForMaskedLM
bertForNextSentencePrediction
bertForPreTraining
#Fine-tuning 
bertForSequenceClassification
bertForTokenClassification
bertForQuestionAnswering
bertForMultipleChoice
https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html


#bert architecture discussion
# onlt read through section 'Model Outputs'
# bert + single-layer neural network as the classifier
# ? so inmy case case will also need to d othis
#https://jalammar.github.io/illustrated-bert/

# here Distill bert outputs emneedding as input to logistic regression
# using the the first vector(same as embedding side, eg : 768) (the one associated with the [CLS] token) to feed logistic regression
# (similar to feature extraction)
# here using tensorflow code so ignore it at first


# for Distill bert sequence claasfication
# we add the special tokens needed for sentence classifications 
# (these are [CLS] at the first position, and [SEP] at the end of the sentence).
# so input verctor does have [SEP] 
# ? in my dummy input, i ignore [SEP] not sure it wlll affect or not
#https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/



# pytorch dataloader class
# how to run it iteratively
# https://www.itread01.com/content/1545234667.html

#  this link has similar process as train_horovod.py trainig / valid procedure
# if training too long (trainnig loss down , validation loss up), it means overfitting
# https://zhuanlan.zhihu.com/p/143209797
# this link has similar process as the folloiwngf code for each function




# initial BertForMaskedLM and do inference
# ? can leverage this runtine to load any distill bert fine-tune version 
# and do inference to verify probability ouput before doing onnx conversion
#https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html
#customized distill bert by config
# 'Under the hood: pretrained models'
# using AutoTokenizer, AutoModelForSequenceClassification
# ? can use distill based to create a test, run model and do inference
#>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
#>>> tokenizer = AutoTokenizer.from_pretrained(model_name) 
# https://huggingface.co/transformers/quicktour.html


# AML official another lin
#anothe link good for tutorial, not using huggingface though
# also use hvd
# ? decide it want to train it the future
#https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/ml-frameworks/pytorch
# simple training script
# hyper parameter tuning (section : Start a hyperparameter sweep)
# ? trying to define range for hyper parameter tunning in the ftureu
https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb
#https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/ml-frameworks/pytorch/train-hyperparameter-tune-deploy-with-pytorch/pytorch_train.py




#another AML tutorial might not be so food
# leave it not very useful
#https://github.com/Azure/AzureML-BERT/blob/master/pretrain/PyTorch/notebooks/BERT_Pretrain.ipynb


# using onnx.load() to load onnx model
# ort settion to test onnx runtime model
# ? trying ot do it in the futrue
#https://github.com/onnx/tutorials/blob/master/tutorials/PytorchOnnxExport.ipynb
# https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html


# pips list ot check package verison being installed
torch                                1.7.1
# command: python --version
# 3.6.19
#https://stackoverflow.com/questions/10214827/find-which-version-of-package-is-installed-with-pip

# warring message
# consulted yue already so leave it
#Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
https://github.com/onnx/onnx/issues/2836#:~:text=Jump%20to%20bottom-,TracerWarning%3A%20Converting%20a%20tensor%20to%20a%20Python%20index%20might%20cause,not%20generalize%20to%20other%20inputs!






# huggung face ert sorce code
# BertForTokenClassification used in yue
#https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertModel
# huggung face distill bert sorce code
#https://huggingface.co/transformers/_modules/transformers/models/distilbert/modeling_distilbert.html




# another tutorial for bert
# similar to yue's bert turotial sharead in internet
## BERT has two constraints:
# All sentences must be padded or truncated to a single, fixed length.
# The maximum sentence length is 512 tokens.
## for Dataoader class, 
# RandomSamploer for traninig data
# SeqeuntialSampler for validation data
# in my code, i use DistributedSampler for both inorder to work with hvd


#output_attentions = False, # Whether the model returns attentions weights.
#output_hidden_states = False, # Whether the model returns all hidden-states.
# set it false then you can fine tune
# using adamW() to optimize 
# ? (AdamW is a class from the huggingface library (as opposed to pytorch) )
# ? might be wrong since it is from transformer package
# use get_linear_schedule_with_warmup
# Total number of training steps is [number of batches] x [number of epochs]. 
# (Note that this is not the same as the number of training samples).
# num_warmup_steps = 0, # Default value in run_glue.py

# has calculating accuracy function for sequenceClassfication
# 241, batch = 40
# ? can try to use it in the futrue
# has comment for each training procedure
# in my code, i have added comment respectively.
# question related
# ? model.train() can move to outside epoch loop, can try in the futrue
# ? training time and valication time can be calculated seperately
# ? torch.no_grad(): can move inside to each batch to check performance
# ? having test procedure (inlcuding data loader etc). can be leveraged in the future

# weight decay rate - from hugging gace
# in my code i do have 
# ? not sure why needs weight decay rate, need to study in the future
#https://mccormickml.com/2019/07/22/BERT-fine-tuning/
#https://zhuanlan.zhihu.com/p/143209797
#https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1




#another turtorial - using DistilBertForSequenceClassification  for recommendation system
# has sigmoid function to get output
# version 1 
# y_pred = sigmoid(outputs[0].detach().cpu().numpy())
# for logit it is the ouput of https://pytorch.org/docs/stable/generated/torch.nn.Linear.html
# and it is a readl value
# detach() : speed tuning up
# http://www.bnikolic.co.uk/blog/pytorch-detach.html
# .cpu()
# move logic to cpu even though using gpu to train
# ? add this in my code to see probability and compare it with QAS output
#https://github.com/sarang0909/Explore-PyTorch/blob/master/Part2_Pytorch_Sentiment_Analysis.ipynb
# version 2
# for logits
# outputs = model(inputs)
# outputs = torch.sigmoid(outputs)
#           predictions.append(outputs.cpu().detach().numpy().tolist())
# https://pytorch.org/docs/stable/generated/torch.sigmoid.html
# it seems torch sigmoid can do the output param already
# ? ? add this in my code to see probability and compare it with QAS output
#https://www.kaggle.com/samson22/distilbert-in-pytorch

# convert numpy array to torch.sensor
# in my code , i do have it
#http://www.bnikolic.co.uk/blog/pytorch-detach.html



#distill bert experiment, define a new class with simplifeid forward function
# a good example to define your own forward function
#Customized distillbert for sequence classfication
# DistilBertForSequenceClassification
#DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
https://www.kaggle.com/kashnitsky/distillbert-catalyst-amazon-product-reviews
'''
class DistilBertForSequenceClassification(nn.Module):
    """
    Simplified version of the same class by HuggingFace.
    See transformers/modeling_distilbert.py in the transformers repository.
    """

    def __init__(self, pretrained_model_name: str, num_classes: int = None):
        """
        Args:
            pretrained_model_name (str): HuggingFace model name.
                See transformers/modeling_auto.py
            num_classes (int): the number of class labels
                in the classification task
        """
        super().__init__()

        config = AutoConfig.from_pretrained(
            pretrained_model_name, num_labels=num_classes)

        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,
                                                    config=config)
        self.pre_classifier = nn.Linear(config.dim, config.dim)
        self.classifier = nn.Linear(config.dim, num_classes)
        self.dropout = nn.Dropout(config.seq_classif_dropout)

    def forward(self, features, attention_mask=None, head_mask=None):
        """Compute class probabilities for the input sequence.

        Args:
            features (torch.Tensor): ids of each token,
                size ([bs, seq_length]
            attention_mask (torch.Tensor): binary tensor, used to select
                tokens which are used to compute attention scores
                in the self-attention heads, size [bs, seq_length]
            head_mask (torch.Tensor): 1.0 in head_mask indicates that
                we keep the head, size: [num_heads]
                or [num_hidden_layers x num_heads]
        Returns:
            PyTorch Tensor with predicted class probabilities
        """
        assert attention_mask is not None, "attention mask is none"
        distilbert_output = self.distilbert(input_ids=features,
                                            attention_mask=attention_mask,
                                            head_mask=head_mask)
        # we only need the hidden state here and don't need
        # transformer output, so index 0
        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)
        # we take embeddings from the [CLS] token, so again index 0
        pooled_output = hidden_state[:, 0]  # (bs, dim)
        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)
        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)
        pooled_output = self.dropout(pooled_output)  # (bs, dim)
        logits = self.classifier(pooled_output)  # (bs, dim)

        return logits
'''



# tensor flow
# distillataion
# leave it in the future
# https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/



# saving and reloading issue
# but this is not to load pytorch model
# leave it in the futrue
#https://github.com/huggingface/transformers/issues/8272

#fast bert
# class BertDataBunch
# another wrapper to train
# leave it in the future
#https://github.com/kaushaltrivedi/fast-bert
# https://github.com/kaushaltrivedi/fast-bert/blob/81a6a594b81947f6a3a42dd4315403a9b27ff7c9/fast_bert/data_cls.py
#BertLearner.from_pretrained_model(
# static method
# with logits.softmax as example
# https://github.com/kaushaltrivedi/fast-bert/blob/b41ee05af18fbbff3e5fa209476041ce345e4c6a/fast_bert/learner_cls.py#L167




#softmax temperature for distilbert
# ? put into machine_learning .txt
#distill bert experiment, define a new class with simplifeid forward function
#https://zhuanlan.zhihu.com/p/91288247


# tinybert
# check in the futrue
# https://zhuanlan.zhihu.com/p/94359189


# fastai
# along with hugging face
# check in the future
# https://www.kaggle.com/melissarajaram/distilbert-fastai-huggingface-transformers



#github cide example
# leave it in the future
#https://github.com/sontung/hci-intermodal-reasoning/blob/4e792fc3daa5f1d572d0286dcb589d1357ccfd0c/text_network.py




# python help 
# add code base to support
#output trace when there is warning
#https://stackoverflow.com/questions/22373927/get-traceback-of-warnings

# python /onnx help
# How to debug torch.nn.export()  using forward function
# add print along your own forward function to test
https://discuss.pytorch.org/t/how-to-debug-torch-nn-export/46906


# vscode help
# how to disalbe just my code in vscode
https://code.visualstudio.com/docs/python/debugging
https://github.com/microsoft/vscode-python/issues/7347


# vscode help
# issue
# if running in debug model ( or run with debugger by vscode), torch.onnx.export  will not generate anything
# ? not sure why


# onnx warnning message debug
# inherited from nn.module
# using DistilBertForSequenceClassification inside
https://github.com/zhaogaofeng611/TextMatch/blob/master/DistilBert/model.py

#trasformer ? ? ?  error message container proved
#
#/azureml-envs/azureml_59b829dd316a9a6c2291bc4de6df663c/lib/python3.6/site-packages/transformers/modeling_utils.py:1645: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
#  input_tensor.shape == tensor_shape for input_tensor in input_tensors
# trying to upgrade to 4.1.1
# code become correct line but still wrong message
# refer to chunk_to_forward_debug_inside, its seq_len_dim =1
# and tensor value is different from input tensor... not sure why

# input_tensors[0].shape = torch.Size([1, 14, 768])

#azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/transformers/modeling_utils.py:1757: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
#  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors

#default chunk_dim = seq_len_dim = 1 
# related bug fix in pytorch but i still fails
# https://discuss.pytorch.org/t/best-way-to-convert-a-list-to-a-tensor/59949/2
#https://github.com/huggingface/transformers/issues/8349
# based on this , it looks ok. assertion is nothing wrong but just warning message
#This is what the warning is about. I think the tracer warning is pointing to a legit concern here. See the "Limitations" section in this PyTorch page.
#The recommended way to capture this dynamic behavior is to use TorchScript in the export. Here's an example, that shows slicing,
# https://github.com/Microsoft/onnxruntime/issues/679

# source code discussion
https://huggingface.co/transformers/glossary.html#feed-forward-chunking
https://huggingface.co/transformers/main_classes/configuration.html
https://huggingface.co/transformers/_modules/transformers/modeling_utils.html
# source code 
# apply_chunking_to_forward
# according trace log it is from class 
class FFN(nn.Module):
https://huggingface.co/transformers/_modules/transformers/modeling_distilbert.html



# BertForTokenClassification
# git
# how to do git search code
# https://docs.github.com/cn/free-pro-team@latest/github/searching-for-information-on-github/searching-code


# yue's bert script folder
# sequence should be bert's output 1
# token should be bert's output 0

#https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fbert_slot_model.py&version=GBmaster&_a=contents


#TNLR
# unilm github
# https://github.com/microsoft/unilm/tree/master/unilm
#haoda
# yue ma
# 01072021
#only checkpoint
#need to find vocab.txt from bert (not distilled bert)
# it might have model_config.json
#     
#\\FSU\Shares\TuringShare\NLR_Models\Monolingual\NLRv1-Base-Uncased\model_config.json
#​[1:05 PM] Yue Ma
    
#self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]
#​[1:17 PM] Yue Ma
    
#https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fbert_intent_model.py&version=GBmaster&line=31&lineEnd=48&lineStartColumn=1&lineEndColumn=31&lineStyle=plain&_a=contents
#​[1:32 PM] Yue Ma

#https://github.com/onnx/tutorials/blob/master/tutorials/VisualizingAModel.md
#onnx/tutorialsTutorials for creating and using ONNX models. Contribute to onnx/tutorials development by creating an account on GitHub.github.com

# 01132021
# load pt
# \\FSU\Shares\TuringShare\NLR_Models\Monolingual\NLRv3-Base-Uncased
# use default bert_config snice not providing confi.json
# and you can change layer 
# ? nneed to figure out how ot do that
# here is to provide labels
# dimention you cannot change
#from transformers import BertConfig;
#bert_config = BertConfig();
#bert_config.num_labels = 12;
#bert_config.output_attentions = False;
#bert_config.output_hidden_states = False;





#onnix graph it repository
#https://github.com/onnx/tutorials/blob/master/tutorials/VisualizingAModel.md



# preprocessing
# preprocess_bert.py v.s train_horovod_slot.py
# different
# 1>  my training data is non consistent
#0	hey cortana look for my password document	file_search	files	look for <contact_name> my </contact_name> <file_keyword> password </file_keyword> document
# former:
# look for my password document	O O contact_name file_keyword O
# later:
# hey co ##rta ##na look for my password document	O file_keyword file_keyword file_keyword O O contact_name O O
# resoan 1 : query and traningi data inconsistent

# 2> Where is my music?	file_search	files	Where is <contact_name> my </contact_name> <file_type> music </file_type> ?
# former:
# where is my music ?	O O contact_name file_type O
# later:
# where is my music ?	file_type file_type contact_name O O
# reason:
# logic wrong while extracting slots


#3>
#0	shows me files I was composing.	file_navigate	files	shows me files <contact_name> I </contact_name> was <file_action> composing </file_action>.
# former:
# shows me files i was composing .	O O O contact_name O file_action O
# later:
# shows me files i was composing .	O O contact_name O O file_action O
# reason:
# logic wrong while extracting slots
# for 'i' when it searches index it will wrong
# so need to refer to the function 'splitWithBert()' 
# it also preprocess toekn inside xml pair <> <> then seperate it
# this is more ideal to do






# preprocessing script from yue
# [for tokenization]
# logic
# query : search spreadsheets saved after 3/12
#annotation_result_arrry:
#-1 : mean <> start and end 
#between -1 -1 it is the slot type
#-1 : will not a
#['O', -1, 'file_type', 'file_type', 'file_type', -1, -1, 'file_action', -1, -1, 'date', 'date', 'date', 'date', ...]

#word array : based on annnotated array / annotation_result_array to generate 
#get
#['search', 'spreads', '##hee', '##ts', 'saved', 'after', '3', '/', '12']


#annotation_filtered_array
#label array with string
#['O', 'file_type', 'file_type', 'file_type', 'file_action', 'date', 'date', 'date', 'date']


# 'adding cutoff length for query' section
#		do not nudetstand very well


# 'if not self.checkQueryValid(word_string)':
#		if exisitng 
#		not sure why needs this


# [for generating traninig data]
# i leave it at first right know
# generateTrainTestData

https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fpreprocess_bert.py&version=GBmaster&_a=contents



# if using yue's class and output loss , slot_output as multiple outputs
# it will be this error
# run 73 (child 75)
[2021-01-09T03:34:31.385345] The experiment failed. Finalizing run...
Starting the daemon thread to refresh tokens in background for process with pid = 153
Cleaning up all outstanding Run operations, waiting 900.0 seconds
2 items cleaning up...
Cleanup took 0.4081854820251465 seconds
Traceback (most recent call last):
  File "train_horovod_slot.py", line 1019, in <module>
    dynamic_axes = {'input_ids': {1: '?'}, 'loss': {1: '?'}, 'slot_output': {1: '?'}}
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/onnx/__init__.py", line 148, in export
    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/onnx/utils.py", line 66, in export
    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/onnx/utils.py", line 416, in _export
    fixed_batch_size=fixed_batch_size)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/onnx/utils.py", line 279, in _model_to_graph
    graph, torch_out = _trace_and_get_graph_from_model(model, args, training)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/onnx/utils.py", line 236, in _trace_and_get_graph_from_model
    trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(model, args, _force_outplace=True, _return_inputs_states=True)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/jit/__init__.py", line 277, in _get_trace_graph
    outs = ONNXTracedModule(f, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/jit/__init__.py", line 360, in forward
    self._force_outplace,
  File "/azureml-envs/azureml_0189aad80ec2226ecb7db7088cba855e/lib/python3.6/site-packages/torch/jit/__init__.py", line 350, in wrapper
    out_vars, _ = _flatten(outs)
RuntimeError: Only tuples, lists and Variables supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. But got unsupported type NoneType

[2021-01-09T03:34:32.048969] Finished context manager injector with Exception.
#RuntimeError: Only tuples, lists and Variables supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. But got unsupported type NoneType
https://blog.csdn.net/qq_33120609/article/details/105857725


#BertForTokenClassification
https://github.com/DavidNemeskey/emBERT/blob/62825a1ef6b7d1e1eee8b8bf4644281c17860670/embert/model.py
#BertForTokenClassification
#https://github.com/DavidNemeskey/emBERT/blob/26faed05408ba8651020fef1ae8d07f331b5cd86/scripts/train_embert.py
#model_dir
#config = BertConfig.from_pretrained(
#                model_dir,
#                # os.path.join(args.bert_model, 'bert_config.json'),
#                num_labels=num_labels, finetuning_task=args.task_name
#            )

# class definition
# here return needs to be tuple
# in class 
# class TokenClassifier(BertForTokenClassification):
#return (loss, logits)




# yue's cpmmen


#RuntimeError: Only tuples, lists and Variables supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. But got unsupported type NoneType
# related ticket and issues
#https://github.com/pytorch/pytorch/issues/42391

#torch._C._nn.nll_loss?
#https://discuss.pytorch.org/t/where-is-torch-c-nn-nll-loss/9769





# hyper paramter tunning 
#https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b



#BertConfig
# specify layer
# how to verify,
# using output config.json to verify
#"num_hidden_layers": 12,
# Number of trainable parameters: 108907797 
#"num_hidden_layers": 3,
#Number of trainable parameters:    45116949 
#https://rdrr.io/github/jonathanbratt/RBERT/man/BertConfig.html
#https://www.deepspeed.ai/tutorials/bert-pretraining/




#joint intent and slot filling
# yues code as reference but following internet at first
# https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fbert_intent_slot.py&version=GBmaster&_a=contents
# option 1 and paper
# has good table for comparison
#https://arxiv.org/pdf/1902.10909.pdf
https://github.com/monologg/JointBERT
# intent label
#https://github.com/monologg/JointBERT/blob/master/data/snips/dev/label
# query 
# https://github.com/monologg/JointBERT/blob/master/data/snips/dev/seq.in
# slot output
# https://github.com/monologg/JointBERT/blob/master/data/snips/dev/seq.out
# for intent
# ? i do not need unk snice i have files_others or might be i need to have another default
# check MDM
# i can use current prod model to output intent for each slot data 
# (current slot data has intent  but might not e correct)
# if domain score >=0.35 , then intent should be valid, otherwise, setup it
# as default OTHER intent if needed
# convert_examples_to_features()
# this will generate features including intent. need to see how data being feed in
# class TensorDataset to store
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/main.py#L13
# here stores tensor dataset
# and feed to trainer class
# class trainer, train() function
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L15
# also using linear warmp and decay
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L87
# here output class and input has all arguments needed
# self.model_class.from_pretrained() pass classs
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L29
# here define possible class can be loaded
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/utils.py#L14
# here using **inputs to pass all necessary algorithm
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L87
# ? not sure about whether batch size will affect or not
#https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L87
# for self.bert, providng token_type_ids
# ? for my setting and yue's setting we do not do that might be uncessary
# it is necessary when calling bertModel
#  0 for the first part, 1 for the second part
# ? but question answer might need it ?
# https://huggingface.co/transformers/glossary.html
# https://huggingface.co/transformers/model_doc/bert.html
#https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/modeling_jointbert.py#L24
# [0] for sequence output (token classfication)
# [1] for intent classficaition output (sequence classfication)
# loss calculation
#total_loss = intent_loss + coef * slot_loss (Change coef with --slot_loss_coef option)
# intent loss: tensor(3.3958, grad_fn=<NllLossBackward>)
# slot loss: tensor(5.0549, grad_fn=<NllLossBackward>)
# toal_loss : tensor(8.4507, grad_fn=<AddBackward0>) 
# (in yue's code, it defines two losses, one for intent and othe other for slot)
# total_loss = intent_loss + slot_loss;
# total_loss.backward() <= and sum them up to do backward propagation
# in debugging mode, tensor(2.4028) + tensor(3.0916) = tensor(5.4944)
# ? not sure how this backprog works  but internet also use single lost to backward propagate
# https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fbert_intent_slot.py&version=GBmaster&_a=contents
# define intent_logits class
# drop out rate the same bert
# hidden size dimention the same bert
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/module.py#L4
# define slot_logits class
# hidden size dimention the same bert
#https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/module.py#L15
# ouput is a tuple 
# ? but it looks like this will fail onnx, can try to have two outputs or two class
#https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/modeling_jointbert.py#L59
# no care about inference for onnx
# need to follow what yue does
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/modeling_jointbert.py#L59
# ignore_index is ignored in function
# not sure wy need this
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/model/modeling_jointbert.py#L48 
# word_tokens = [unk_token]  # For handling the bad-encoded word
# ? not sure whether we need to hanlde this or not, or check yue's code
# ? can add test ihe futurue
# https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/data_loader.py#L148
# trainer.py
#epoch_iterator = tqdm(train_dataloader, desc="Iteration")
#            for step, batch in enumerate(epoch_iterator):
# batch : text_id, attention_mask,token_type_id (? not sure this is necessary), inten_label_ids, slot_label_ids
# each size of it is number of tranining examples based on  train_batch_size



# evaluation
# intent_logits.detach().cpu().numpy() generates array()
#array([[ 2.39753183e-02, -1.00037996e-02,  2.51673833e-02,
        -4.94064158e-03, -1.02473386e-02,  1.26954960e-02,
        -1.67937279e-02,  6.12567812e-02,  2.03567557e-04,
         2.56004222e-02,  6.15039468e-02],
       [ 2.23806202e-02, -1.10518197e-02,  2.86248215e-02,
        -3.70978750e-03, -1.03999898e-02,  1.11508286e-02,
        -1.12427333e-02,  6.14888370e-02, -9.67073254e-04,
         2.52279546e-02,  6.27490729e-02]])
# each is np.array
>>> np.append(a, b, axis=0)
array([[1, 2],
       [3, 4],
       [2, 1],
       [4, 3]])
#np.argmax(c, axis=1)
>>> np.argmax(c, axis=1)
array([1, 1, 0, 0], dtype=int64)
# intent has the first one
# 
#https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/trainer.py#L161


# also has function to calculate metrics (itent, slot)



# tensor.backward()
#
# will use the chain rule to compute the gradient for every parameter in the network. For a give param, w of size d, it will perform: gradient * dy/dw where dy/dw will will be computed by the chain rule.
# https://discuss.pytorch.org/t/what-does-tensor-backward-do-mathematically/27953


# QAS output 
# # for domain, it needs to have probabilty
# files_enus_mv7_domain_svm_score (tag: 1, string: 0)
# 0[-1,-1]=0.1968164
# for intent
# files_enus_mv7_intent_svm_score (tag: 11, string: 0)
 #                       0[-1,-1]=2.2269628
 #                       1[-1,-1]=-1.5521804
 #                       2[-1,-1]=-1.6113045
 #                       3[-1,-1]=-1.0233102
 #                       4[-1,-1]=-1.3408698
 #                       5[-1,-1]=-1.8470569
 #                       6[-1,-1]=-1.0570247
 #                       7[-1,-1]=-1.167596
 #                       8[-1,-1]=-1.8137677
 #                       9[-1,-1]=-0.99997
 #                       10[-1,-1]=-1.389961





# how to get tensor's value
 # Consider using variable.item() instead for 0-dim tensors. For higher-dimensional variables, use variable.tolist().
# https://discuss.pytorch.org/t/get-value-out-of-torch-cuda-float-tensor/2539/11




# nn.linear
#https://pytorch.org/docs/stable/generated/torch.nn.Linear.html



# good paper
#https://zhuanlan.zhihu.com/p/143123368



#yue's joint intent and slot
# not sure how this function works
# ? get_lexicon_vector might be can be checked
# https://msasg.visualstudio.com/LanguageUnderstanding/_git/Timex_Deep_Model?path=%2FTimexModelScripts%2FTimexModelScripts%2FBert_Email%2Fevaluate_bert.py&version=GBmaster&_a=contents
# in another file, it is truly being used here
# (by search function in repo)
# https://msasg.visualstudio.com/LanguageUnderstanding/_search?action=contents&text=get_lexicon_vector&type=code&lp=code-Project&filters=ProjectFilters%7BLanguageUnderstanding%7DRepositoryFilters%7BTimex_Deep_Model%7D&pageSize=25&result=DefaultCollection%2FLanguageUnderstanding%2FTimex_Deep_Model%2FGBmaster%2F%2FTimexModelScripts%2FTimexModelScripts%2FTensorflow%2Fevaluate_model_hypertuning.py


# evaluate_model()
# do not go with batch size
# [for slot]
# each query then evaluate model to get output
# predicted_labels_slot = list(map((lambda x: self.slot_dict_rev[x]), slot_result));
# this also 1-d list
# predicted_slot_arrays = self.create_slot_arrays_iob(predicted_labels_slot)
# predicted_slot_arrays will output
# key : contact_name , list : [[1,2], [3,4]]
# 1-2 is the span to have contact name
# golden_set=set(map(tuple, golden_slot_arrays[label]))
# [[1,2], [3,4]] =>((1,2) , (3,4))
# set is for type, map just tuple for each label's sublist
# since set , order does not mater
# python set operation
#https://www.geeksforgeeks.org/python-set-operations-union-intersection-difference-symmetric-difference/
# [for intent]



# pandas read empty string , not NAN, by setting an option
# https://stackoverflow.com/questions/10867028/get-pandas-read-csv-to-read-empty-values-as-empty-string-instead-of-nan/43832955




# bert output
# paper 
# https://arxiv.org/abs/1810.04805
# the authors describe two ways to work with BERT, 
# 1> one as with “feature extraction” mechanism. 
# That is, we use the final output of BERT as an input to another model. 
# This way we’re “extracting” features from text using BERT and then use it in a separate model for the actual task in hand. 
# 2> The other way is by “fine-tuning” BERT. 
# That is, we add additional layer/s on top of BERT and then train the whole thing together. 
# This way, we train our additional layer/s and also change (fine-tune) the BERTs weights.
# below link using the seocnd 
# [1] : pool output , for sequence claissfcaiotn
# [0]: sequence output (miusleading name) for token claasfication
# https://www.kaggle.com/questions-and-answers/86510
#Let's take an example of "You are a kaggle kernels master".
#Before passing it to bert model you need to add [CLS] token in the begining and [SEP] token at the end of the sentence.
#
#Now the sentence is "[CLS] You are a kaggle kernels master [SEP]".#
#
#Now if you give above sentence to BertModel you will get 768 dimension embedding for each token in the given sentence. So 'sequence output' will give output of dimension [1, 8, 768] since there are 8 tokens including [CLS] and [SEP] and 'pooled output' will give output of dimension [1, 1, 768] which is the embedding of [CLS] token.

# https://towardsdatascience.com/bert-to-the-rescue-17671379687f
# https://github.com/huggingface/transformers/issues/1827
# https://github.com/shudima/notebooks/blob/master/BERT_to_the_rescue.ipynb




#DistributedSampler
# in atis local run, i do not have sampler
# in atis remote run, using distributed sampler metric is pretty low so i switch to RandomSampler 
# default DistributedSampler's shuffle is true so before feeding to maching total data has been shuffle
# so data loader does not need to shuffle again
# usng self.epoch = 0 as default
# default shuffle= true
# epoch = 2
# dataset size = 16
# cuda : 0 and 1
# batch size = 2
# epoch 1
# cuda 0 will only see 8 datas (uncontrollable), c00
# cuda 1 will only see 8 datas (uncontrollable), c10
# epoch 2
# cuda 0 will only see 8 datas (uncontrollable), c01
# cuda 1 will only see 8 datas (uncontrollable), c11
# c00 = c01, c10 = c11
# by seeting up sampler.set_epoch(e), e = epoch number
# epoch 1
# cuda 0 will only see 8 datas (uncontrollable), c00
# cuda 1 will only see 8 datas (uncontrollable), c10
# epoch 2
# cuda 0 will only see 8 datas (uncontrollable), c01
# cuda 1 will only see 8 datas (uncontrollable), c11
# c00 != c01, c10 != c11
# then each epoch a cpu can see whole dataset
# does above create overfitting problem?
# No,每一个step不同进程之间都会去同步自己的参数、gradient、甚至buffer.
# so no need to setup seed
# https://zhuanlan.zhihu.com/p/97115875
# this link also explains and say no seed change is needed
# https://blog.csdn.net/weixin_45738220/article/details/112151455

# for data loader
# epoch = 2
# dataset size = 8
# nproc_per_node = 2
# cuda : 0 and 1
# batch size = 4
# rand_loader =DataLoader(dataset=dataset,batch_size=batch_size,sampler=None,shuffle=True)
# (without using distributedsampler and only shuffle)
# each epoch, a GPU sees 2 batch and each batch size = 4 and each batch is shuffle
# snice total epoch = 2 , a GPU see the whole dataset twice 
# rand_loader = DataLoader(dataset=dataset,batch_size=batch_size,sampler=sampler)
# using distributedsampler
# nproc_per_node = 2
# sample divides the whole dataset into nproc_per_node( = 2)
# nproc_per_node cpu shares the whole dataset
# 

# https://www.squncle.com/article/2020/5/10/29278.html
# https://murphypei.github.io/blog/2020/09/pytorch-distributed
# https://discuss.pytorch.org/t/distributedsampler/90205
# https://pytorch.org/cppdocs/api/classtorch_1_1data_1_1samplers_1_1_distributed_sampler.html?highlight=distributedsampler#_CPPv4I0EN5torch4data8samplers18DistributedSamplerE
# good repor
# has warmp up , early-stopping
# Warmup 是一種訓練技巧，透過由小到大預熱學習率可以避免一開始學習率過大所造成的不穩定
# https://github.com/Lance0218/Pytorch-DistributedDataParallel-Training-Tricks
# https://lance0218.medium.com/training-tricks-for-pytorch-distributed-data-parallel-1cd48cc7d97a

#init_weights
Have a look at the code for .from_pretrained(). What actually happens is something like this:

find the correct base model class to initialise
initialise that class with pseudo-random initialisation (by using the _init_weights function that you mention)
find the file with the pretrained weights
overwrite the weights of the model that we just created with the pretrained weightswhere applicable
This ensure that layers were not pretrained (e.g. in some cases the final classification layer) do get initialised in _init_weights but don't get overridden.

# in jointbert experiment it does not have it
#https://github.com/huggingface/transformers/issues/4701





# random sees setup to reproduce model
# https://github.com/Lance0218/Pytorch-DistributedDataParallel-Training-Tricks/blob/master/customized_function.py

# hvd horovod
#Accomplish this by guarding model checkpointing code with hvd.rank() != 0.
# ? in noline tutorial, it uses hvd_rank() = 0 to check
# Pin each GPU to a single process.
# # Save checkpoints only on worker 0 to prevent other workers from corrupting them.
#checkpoint_dir = '/tmp/train_logs' if hvd.rank() == 0 else None
# ? not sure how to load checkpoint_dir
# https://github.com/horovod/horovod
# https://github.com/horovod/horovod/issues/58
#https://horovod.readthedocs.io/en/stable/pytorch.html
# this link explians why using hvd.rank() to guard
#https://pyro.ai/examples/svi_horovod.html

# detail talks about ranker
# https://spell.ml/blog/distributed-model-training-using-horovod-XvqEGRUAACgAa5th
# https://github.com/horovod/horovod/issues/1774

# output epoch traning branch
#  cann add in the future
#        if batch_idx % args.log_interval == 0:
#            # Horovod: use train_sampler to determine the number of examples in
#            # this worker's partition.
#            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
#                epoch, batch_idx * len(data), len(train_sampler),
#                100. * batch_idx / len(train_loader), loss.item()))
# https://github.com/horovod/horovod/blob/master/examples/pytorch/pytorch_mnist.py

# flow to save checkpoint in pytorch in offical github
# can refer to pytorch offical website
     if hvd.rank() == 0:
        filepath = args.checkpoint_format.format(epoch=epoch + 1)
        state = {
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
        }
        torch.save(state, filepath)
# https://github.com/horovod/horovod/blob/f3af98649a26f4f3725fcab6a9bd3e8d29d7ffd2/examples/pytorch/pytorch_imagenet_resnet50.py
# can refer to pytorch offical website
# sotre checkpoint 
EPOCH = 5
PATH = "model.pt"
LOSS = 0.4

torch.save({
            'epoch': EPOCH,
            'model_state_dict': net.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': LOSS,
            }, PATH)
# load checkpount
model = Net()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

model.eval()
# - or -
model.train()
# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html



# BertPooler
# max pooling 的idea
#在Bert中，pool的作用是，输出的时候，用一个全连接层将整个句子的信息用第一个token来表示
#  BertForSequenceClassification 用到的是 pooled_output，即用1个位置上的输出表示整个句子的含义
class BertPooler(nn.Module):
    def __init__(self, config):
        super(BertPooler, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
# https://www.cnblogs.com/dogecheng/p/11907036.html

#IntentClassfier using in joint bert 
# similar to BertForSequenceClassification in hugging face
# Bert用于提取文本特征进行Embedding，Dropout防止过拟合，Linear是一个弱分类器
#  BertForSequenceClassification 用到的是 pooled_output，即用1个位置上的输出表示整个句子的含义
# 有多用到一個Bert Pooler
# https://www.cnblogs.com/dogecheng/p/11907036.html

#SlotClassfier using in joint bert 
# similar to BertForTokenClassification in hugging face
# Bert用于提取文本特征进行Embedding，Dropout防止过拟合，Linear是一个弱分类器
# BertForTokenClassification 的中 forward() 函数的部分代码，它用到的是全部 token 上的输出
# 沒有用到Bert Pooler
## https://www.cnblogs.com/dogecheng/p/11907036.html

# how to output hidden state from distill bert pretrained model
# https://stackoverflow.com/questions/60780181/access-the-output-of-several-layers-of-pretrained-distilbert-model

# multiple loss function
# ? can check in the future
# https://bbs.cvmart.net/topics/1449
#https://github.com/horovod/horovod/issues/58




# aml computer node cnt
# in my default with distributedSampler , my node_count = 8
# ? if want to use randomSampler, might be node_count  = 1 to try whether errors or ont
#https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.pytorch?view=azure-ml-py


# azure machine learning open a terminal
# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-terminal

# comment until here
 

#https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1

#https://github.com/DavidNemeskey/emBERT/blob/62825a1ef6b7d1e1eee8b8bf4644281c17860670/embert/model.py




# torch.max
#https://www.journaldev.com/39463/pytorch-torch-max

# torch sensor iterative
# https://stackoverflow.com/questions/62791942/how-do-i-iterate-over-pytorch-2d-tensors


# add_special_tokens=True
# <done>
# can add special toekn
# default is true
# text = "[CLS] a visually stunning rumination on love [SEP]"
# version 1
#fast_tokenized_text_old = fast_tokenizer.tokenize(text)
# version 2
#fast_tokenized_batch : BatchEncoding = fast_tokenizer(text)
#fast_tokenized_text :Encoding  =fast_tokenized_batch[0]
# version1 will not double
# version 2 will add duplicated

fast token ouput version 1: ['[CLS]', 'a', 'visually', 'stunning', 'rum', '##ination', 'on', 'love', '[SEP]']
fast token ouput version 2: ['[CLS]', '[CLS]', 'a', 'visually', 'stunning', 'rum', '##ination', 'on', 'love', '[SEP]', '[SEP]']

# with only a single
# text = "[CLS] a visually stunning rumination on love"
# version 1
#fast_tokenized_text_old = fast_tokenizer.tokenize(text)
# version 2
#fast_tokenized_batch : BatchEncoding = fast_tokenizer(text)
#fast_tokenized_text :Encoding  =fast_tokenized_batch[0]
# version1 will compelte it but not double
# version2 will compelte it and double
#fast token ouput version 1: ['[CLS]', 'a', 'visually', 'stunning', 'rum', '##ination', 'on', 'love', '[SEP]']
#fast token ouput version 2: ['[CLS]', '[CLS]', 'a', 'visually', 'stunning', 'rum', '##ination', 'on', 'love', '[SEP]', '[SEP]']


# ? can verify in the future to see 
# ? for domain traing(version) and slot traninig(version 2) , what should be the correct way
# ? if add to onnx or traning data does it affect or not

#https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/
#https://www.cnblogs.com/dogecheng/p/11907036.html



# torch.tensor(box)[None, :]
#RuntimeError: Could not infer dtype of NoneType
#abnormal value in column
#https://juejin.cn/post/6844904158039015431



# check isna in pandas
#https://datatofish.com/rows-with-nan-pandas-dataframe/



# save to onnx more examples
# https://www.programcreek.com/python/example/126284/transformers.BertTokenizer.from_pretrained









# inherited DistilBertPreTrainedModel
https://github.com/acsyl/transquest_vis/blob/42ece1a6f760681cf8498bb11d0a1627412aa244/algo/transformers/models/distilbert_model.py
https://github.com/SeniorDev009/simpletransformers/blob/b094a5b14adbbd5bf5f88c018bb9e09f9161a9c3/simpletransformers/classification/transformer_models/distilbert_model.py
https://github.com/TharinduDR/STS-Transformers/blob/f6e505d4b780486117bed9108d534b13010c9c8d/ststransformers/models/distilbert_model.py
https://github.com/lucmichalski/dmoz-utils/blob/4d15397ca7205d6b5ea243ae2f19d75db03e04c1/classifier/simpletransformers/simpletransformers/experimental/classification/transformer_models/distilbert_model.py
https://github.com/mfomicheva/TransQuest/blob/47445cf90690f57dcab1c4b416f83aa903b7f2e2/algo/transformers/models/distilbert_model.py
https://github.com/ym001/DAIA/blob/f58fed316e1599926f5457312ea0806e6a5da4e3/models/distilbert_model.py
#https://github.com/TharinduDR/STS-Transformers/blob/f6e505d4b780486117bed9108d534b13010c9c8d/ststransformers/models/distilbert_model.py
https://github.com/eduros93/my_simpletransformers/blob/ccc5c715bd8d682d5a8f68fbddaa8a25f4da6351/simpletransformers/experimental/classification/transformer_models/distilbert_model.py#L29



#inherited DistilBertForSequenceClassification
# this is the example i am looking for to define forward function
# oonly return logits
https://github.com/johannesmelsbach/fast-transformers/blob/63fb37515385b6dc6737c657ebd6eb48026c1d7b/src/fastformer/fastdistilbert.py
# reutrn loss + logits












#DistilBertForTokenClassification
# official tutorial
#https://huggingface.co/transformers/model_doc/distilbert.html
# joint slot and intent
https://github.com/monologg/JointBERT
# token level to sub token level token_labels_to_subtoken_labels()
# but using tensor flow...
# format : rumination -> rum ##ination 
# ? might be ## is a key 
# https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/

# http://docs.deeppavlov.ai/en/master/features/models/bert.html
#http://docs.deeppavlov.ai/en/master/_modules/deeppavlov/models/torch_bert/torch_bert_sequence_tagger.html
#https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/models/bert/bert_sequence_tagger.py#L2
#https://github.com/deepmipt/DeepPavlov/blob/b66179e584d3eb6da73c5731ba7b732dab7e94bd/examples/Pseudo-labeling%20for%20classification.ipynb



# using bert
# https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification/blob/master/run_sequence_labeling.py
# good repo to do that
# BertFastTokenizer
# format 1 : does not tell spanns start and end
# format 2: do tell span start and end, BIO schema (https://www.lighttag.io/blog/sequence-labeling-with-transformers/)
# https://github.com/LightTag/sequence-labeling-with-transformers

# another DistilBertTokenizerFast
# https://huggingface.co/transformers/custom_datasets.html


# CRFon top of BERT
#  does it wokr ot not
# https://www.reddit.com/r/LanguageTechnology/comments/g45nyv/is_putting_a_crf_on_top_of_bert_for_sequence/
# https://blog.csdn.net/qq_16949707/article/details/105742383


# pytorch CRF
#https://www.shuzhiduo.com/A/o75N0m8XzW/


###########################
# yue email slot model below
# add my model setup to test domain
# 
###########################
# file : cortana_email_enus_mv2_dev.bert-uncased-vocab.txt
# =>should be vocab.txt generated by training

# file L cortana_email_enus_mv2_dev.model.onnx.bin
# => should be traced_distill_bert.onnx.bin

# file : cortana_email_enus_mv2_dev.model.onnx.config.txt 
# i rename it to files_enus_mv5.domain.model.onnx.config.txt

#[model]
## onnx bin
#name = cortana_email_enus_mv2_dev.model.onnx.bin
# => name = traced_distill_bert.onnx.bin
#[inputs]
# featureset = input_ids : TAG      name = input_ids      type = INT64      shape = 1,-1      padding = NONE
# input_ids, should be the same as  DNNProcessor's input_ids
# ? what does TAG mean here
# ? how to decide type
# ? what does shape mean here
# ? what odes padding= NONE here mean
# => in my case i am using the same thing

# attention_maskfeatureset = attention_mask : TAG      name = attention_mask      type = INT64      shape = 1,-1      padding = NONE
# ? why needs attention_mask,i do not provide it at first
# same questions as input_ids
# refer to this 
# https://github.com/vilcek/fine-tuning-BERT-for-text-classification/blob/master/02-data-classification.ipynb
# it has the same length of input_ids
# ? Studying padding in the future since it is related
# => in my case i do not provide it

#[outputs]
# featureset = slot_output : WEIGHT      name = slot_output 
# ? not sure if INt 64 type is needed since it is configured durying training but i follow up at first
# it needs to be aigned with output port torch.onnx.export
# ? but name seems unreated to output format of class DistilBertPreTrainedModel
# in class DistilBertForSequenceClassification
# when return_dict is none in argument (config set it as true)
# will return
#         return SequenceClassifierOutput(
#            loss=loss,
#            logits=logits,
#            hidden_states=distilbert_output.hidden_states,
#            attentions=distilbert_output.attentions,
#        )
# => in my code i renmae it to 
# featureset = domain_output : WEIGHT      name = domain_output


# file : cortana_email_cortana_email_enus_mv2_dev.slot.deep.model.pipeline.txt
# => files_enus_mv5.domain.deep.model.pipeline.txt
# ? content not yet verified but need to follow slot



###########################
# yue email slot model above
# add my model setup to test domain
# 
###########################


###########################
# yue email slot model below
# add my model setup to test domain
# <this is obsolete, please refer to a new section>
###########################


# compare to tutorial
#https://msasg.visualstudio.com/Cortana/_git/CoreScienceDataStaging/pullrequest/1997902?_a=files&path=%2Fmodels%2Femail_pme%2Fdeveloper%2Fcortana_email_enus_mv2_dev.slot.deep.model.pipeline.txt
# file : cortana_email_enus_mv2_dev.bert-uncased-vocab.txt
# =>should be vocab.txt generated by training
# cortana_email_enus_mv2_dev.model.onnx.bin
# should be traced_distill_bert.onnx.bin
# file : cortana_email_enus_mv2_dev.model.onnx.config.txt (files_enus_mv5.slot.model.onnx.config.txt)
# https://msasg.visualstudio.com/DefaultCollection/QAS/_wiki/wikis/QAS.wiki/48023/Deploying-ONNX-Models-in-QAS
# follow this link to write your own
#[model]
## onnx bin
#name = cortana_email_enus_mv2_dev.model.onnx.bin
#[inputs]
# input_ids, should be the same as  DNNProcessor's input_ids
# ? what does TAG mean here
# ? how to decide type
# ? what does shape mean here
# ? what odes padding= NONE here mean
# attentopn_mask
# ? why needs attention_mask,i do not provide it at first
# same questions as input_ids
# refer to this 
# https://github.com/vilcek/fine-tuning-BERT-for-text-classification/blob/master/02-data-classification.ipynb
# it has the same length of input_ids
# ? Studying padding in the future since it is related

#[outputs]
# slot output, should be the same as  DNNProcessor's input_ids
# ? not sure if INt 64 type is needed since it is configured durying training but i follow up at first

featureset = input_ids : TAG      name = input_ids      type = INT64      shape = 1,-1      padding = NONE
featureset = attention_mask : TAG      name = attention_mask      type = INT64      shape = 1,-1      padding = NONE
[outputs]
featureset = slot_output : WEIGHT      name = slot_output



# cortana_email_enus_mv2_dev.model.onnx.warmup.txt
# ? not sure why needs this file
# by looking at the content of PR
#  it specific input_ids and attention_mask
#  in my first case , i only specific input_ids
# input_ids	6	0	101	1	0	0	3191	1	1	1	2026	1	2	2	22028	1	3	3	2055	1	4	4	102	1	5	5
# format likes this 
# 6 0 specicy 6 features 
# kth feature
# (feature id from bert, 1, k,k)
# ? not sure the correct format but can go with this setting at first


# for DNNPrcessor
# https://msasg.visualstudio.com/DefaultCollection/QAS/_wiki/wikis/MLG%20Processors/48552/DNNProcessor
# different -runtime will be provided regarding CPU or GPU models and it uses CPU
# in email, no externa lsegmenIDs to provide
# ? not sure if gpu-trained model can be used cpu to run or not
# ? how to filter based on offset
# since tags are the same 
    domain_output (tag: 2, string: 0)
                        0[0,0]=6.409485
                        0[1,1]=-7.704887
# method1 : but this does not apply to intent
#1>
#usinf feature shift to shift one and only one(label  =1) left
#2>
#usinf value aggregator to change it from x[0,0] to  x[-1,-1] 
#3>
#then normalzied (might be normalized does not need )

# result
# 1> not promising result
    run 109 (111 as child ) setup temporary
        # EG: show my file# wrong 
        # normal domain output
            Output:
                files_enus_mv5_domain_svm_score (tag: 1, string: 0)
                    0[-1,-1]=0.8508758
        #DNN output 
        # in my traninig ,positive example label is 1 , negative example label is 0
            Output:
                domain_output (tag: 2, string: 0)
                        0[0,0]=0.115999304
                        0[1,1]=-0.025065321
        # eg: send message to tom
        # wrong 
            Output:
                domain_output (tag: 2, string: 0)
                        0[0,0]=-0.044760797
                        0[1,1]=0.03344066


    run 115 (114 as child ) setup temporary
        # EG: show my file
            # normal domain output
            Output:
                domain_default_linear (tag: 1, string:  0)
                        0[-1,-1]=1.4813591
9: FeatureNormalizer --in=domain_default_linear --out=f0)         v5_domain_svm_score --files_enus_mv5_domain_svm_score --norm=sigmoid        
            Inputs:                                        iles_enus_m
                domain_default_linear (tag: 1, string:  0)
                        0[-1,-1]=1.4813591             0)
            Output:                                                   )
                files_enus_mv5_domain_svm_score (tag: 1
1, string: 0)                                          , string: 0
                        0[-1,-1]=0.8147778
            #DNN output 
            # in my traninig ,positive example label is 1 , negative example label is 0
            Output:
                domain_output (tag: 2, string: 0)
                        0[0,0]=-3.5034482
                        0[1,1]=4.7909913

        # eg: send message to tom
            # normal domain output
                    domain_default_linear (tag: 1, string: 0)
                        0[-1,-1]=-2.8886678
9: FeatureNormalizer --in=domain_default_linear --out=files_enus_mv5_domain_svm_score --norm=sigmoid
        Inputs:
                domain_default_linear (tag: 1, string: 0)
                        0[-1,-1]=-2.8886678
        Output:
                files_enus_mv5_domain_svm_score (tag: 
1, string: 0)
                        0[-1,-1]=0.0527166

            #DNN output 
            # in my traninig ,positive example label is 1 , negative example label is 0
            Output:
                domain_output (tag: 2, string: 0)
                        0[0,0]=6.409485
                        0[1,1]=-7.704887
# 2>
# using pytorch_model.bin will show up error. it cannot be loaded by QAS
# verifyed by 'run 109 (111 as child ) setup temporary'




###########################
# yue email slot model above
# <this is obsolete, please refer to a new section>
###########################


###########################
# yue email slot model below
# add my model setup to test slot
# 
###########################
# file : cortana_email_enus_mv2_dev.bert-uncased-vocab.txt
# =>should be vocab.txt generated by training

# file L cortana_email_enus_mv2_dev.model.onnx.bin
# => should be traced_distill_bert.onnx.bin

# file : cortana_email_enus_mv2_dev.model.onnx.config.txt 
# i rename it to files_enus_mv5.slot.model.onnx.config.txt

#[model]
## onnx bin
#name = cortana_email_enus_mv2_dev.model.onnx.bin
# => name = traced_distill_bert.onnx.bin
#[inputs]
# featureset = input_ids : TAG      name = input_ids      type = INT64      shape = 1,-1      padding = NONE
# input_ids, should be the same as  DNNProcessor's input_ids
# ? what does TAG mean here
# ? how to decide type
# ? what does shape mean here
# ? what odes padding= NONE here mean
# => in my case i am using the same thing

# attention_maskfeatureset = attention_mask : TAG      name = attention_mask      type = INT64      shape = 1,-1      padding = NONE
# ? why needs attention_mask,i do not provide it at first
# same questions as input_ids
# refer to this 
# https://github.com/vilcek/fine-tuning-BERT-for-text-classification/blob/master/02-data-classification.ipynb
# it has the same length of input_ids
# ? Studying padding in the future since it is related
# => in my case i do not provide it

#[outputs]
# featureset = slot_output : WEIGHT      name = slot_output 
# ? not sure if INt 64 type is needed since it is configured durying training but i follow up at first
# it needs to be aigned with output port torch.onnx.export
# ? but name seems unreated to output format of class DistilBertPreTrainedModel
# in class DistilBertForSequenceClassification
# when return_dict is none in argument (config set it as true)
# will return
#        return TokenClassifierOutput(
#            loss=loss,
#            my new change
#            logits=torch.argmax(logits.view(-1, self.num_labels), dim=1),
#            hidden_states=outputs.hidden_states,
#            attentions=outputs.attentions,
#        ) 
# => in my code i renmae it to 
# featureset = slot_output : WEIGHT      name = slot_output


# file : cortana_email_cortana_email_enus_mv2_dev.slot.deep.model.pipeline.txt
# => files_enus_mv5.slot.deep.model.pipeline.txt
# max token cnt change sfrom 1024 to 512
#WordPieceTokenizer --in=ExternalInput1 --out=tokens,input_ids,attention_mask,segment_ids --maxTokenCount=512 --unknownToken=[UNK] --vocabFile=vocab.txt --startToken=[CLS] --endToken=[SEP]
#StringFeatureSetInfo --in=tokens --out=SequenceLength --maxTokenLength=999

# dnn output changes to slot_output
# DNNProcessor --in=input_ids --out=slot_output --config=files_enus_mv5.slot.model.onnx.config.txt --runtime=Onnx-Latest --warmup=files_enus_mv5.slot.model.onnx.warmup.txt

# same as email domain
#SubwordToWord --in=tokens,slot_output --out=words,words_output --ignoreTokens=[CLS],[SEP]

# update related file name
#TagTranslator --in=words_output --out=tag_output --function=WeightToTag
#FeatureNormalizer --in=tag_output --out=files_enus_mv5_slot_dnn_tag --norm=sign --cutoff=-0.5


###########################
# yue email slot model below
# add my model setup to test slot
# 
###########################




###########################
# yue email slot model below real query outproof - in search gold folder
###########################

query : send email to tom

0: WordPieceTokenizer --in=ExternalInput1 --out=tokens,input_ids,attention_mask,segment_ids --maxTokenCount=1024 --unknownToken=[UNK] --vocabFile=cortana_email_enus_mv2_dev.bert-uncased-vocab.txt --startToken=[CLS] --endToken=[SEP]
        Inputs:
                ExternalInput1 (tag: 0, string: 4)
                        send[0,0]=1.0
                        email[1,1]=1.0
                        to[2,2]=1.0
                        tom[3,3]=1.0
        Output:
                tokens (tag: 0, string: 6)
                        [CLS][0,0]=1.0
                        send[0,0]=1.0
                        email[0,0]=1.0
                        to[0,0]=1.0
                        tom[0,0]=1.0
                        [SEP][0,0]=1.0
                input_ids (tag: 6, string: 0)
                        101[0,0]=1.0
                        4604[1,1]=1.0
                        10373[2,2]=1.0
                        2000[3,3]=1.0
                        3419[4,4]=1.0
                        102[5,5]=1.0
                attention_mask (tag: 6, string: 0)
                        1[0,0]=1.0
                        1[1,1]=1.0
                        1[2,2]=1.0
                        1[3,3]=1.0
                        1[4,4]=1.0
                        1[5,5]=1.0
                segment_ids (tag: 6, string: 0)
                        0[0,0]=1.0
                        0[1,1]=1.0
                        0[2,2]=1.0
                        0[3,3]=1.0
                        0[4,4]=1.0
                        0[5,5]=1.0
1: StringFeatureSetInfo --in=tokens --out=SequenceLength --maxTokenLength=999
        Inputs:
                tokens (tag: 0, string: 6)
                        [CLS][0,0]=1.0
                        send[0,0]=1.0
                        email[0,0]=1.0
                        to[0,0]=1.0
                        tom[0,0]=1.0
                        [SEP][0,0]=1.0
        Output:
                SequenceLength (tag: 1, string: 0)
                        0[-1,-1]=6.0
2: DNNProcessor --in=input_ids,attention_mask --out=slot_output --config=cortana_email_enus_mv2_dev.model.onnx.config.txt --runtime=Onnx-Latest --warmup=cortana_email_enus_mv2_dev.model.onnx.warmup.txt
        Inputs:
                input_ids (tag: 6, string: 0)
                        101[0,0]=1.0
                        4604[1,1]=1.0
                        10373[2,2]=1.0
                        2000[3,3]=1.0
                        3419[4,4]=1.0
                        102[5,5]=1.0
                attention_mask (tag: 6, string: 0)
                        1[0,0]=1.0
                        1[1,1]=1.0
                        1[2,2]=1.0
                        1[3,3]=1.0
                        1[4,4]=1.0
                        1[5,5]=1.0
        Output:
                # look like preprocessing output is done already
                # 21 ? might be the slot id ?
                slot_output (tag: 6, string: 0)
                        0[0,0]=0.0
                        0[1,1]=0.0
                        0[2,2]=21.0
                        0[3,3]=0.0
                        0[4,4]=1.0
                        0[5,5]=0.0
3: SubwordToWord --in=tokens,slot_output --out=words,words_output --ignoreTokens=[CLS],[SEP]
        Inputs:
                tokens (tag: 0, string: 6)
                        [CLS][0,0]=1.0
                        send[0,0]=1.0
                        email[0,0]=1.0
                        to[0,0]=1.0
                        [SEP][0,0]=1.0
                        [SEP][0,0]=1.0
                slot_output (tag: 6, string: 0)
                        0[0,0]=0.0
                        0[1,1]=0.0
                        0[2,2]=21.0
                        0[3,3]=0.0
                        0[4,4]=1.0
                        0[5,5]=0.0
        Output:
                words (tag: 0, string: 4)
                        send[0,0]=1.0
                        email[1,1]=1.0
                        to[2,2]=1.0
                        tom[3,3]=1.0
                words_output (tag: 4, string: 0)
                        0[0,0]=0.0
                        0[1,1]=21.0
                        0[2,2]=0.0
                        0[3,3]=1.0
4: TagTranslator --in=words_output --out=tag_output --function=WeightToTag       
        Inputs:
                words_output (tag: 4, string: 0)
                        0[0,0]=0.0
                        0[1,1]=21.0
                        0[2,2]=0.0
                        0[3,3]=1.0
        Output:
                tag_output (tag: 4, string: 0)
                        0[0,0]=0.0
                        21[1,1]=21.0
                        0[2,2]=0.0
                        1[3,3]=1.0
5: FeatureNormalizer --in=tag_output --out=cortana_email_enus_slot_dnn_tag --norm=sign --cutoff=-0.5      =sign --cutoff=-0.5
        Inputs:
                tag_output (tag: 4, string: 0)
                        0[0,0]=0.0
                        21[1,1]=21.0
                        0[2,2]=0.0
                        1[3,3]=1.0
        Output:
                # so this is just to remove weight
                cortana_email_enus_slot_dnn_tag (tag: 4, string: 0)
                        0[0,0]=1.0
                        21[1,1]=1.0
                        0[2,2]=1.0
                        1[3,3]=1.0







###########################
# yue email slot model below real query outproof - in search gold  folder
###########################



###########################
# git branch to test below
# branch for testing
###########################

# DNN output domain_output
# add softmax to test
#users/chiecha/files_bert_domain_qas_setup_12212020




# DNN output slot_output
# add softmax to test
#users/chiecha/files_bert_slot_qas_setup_12282020


###########################
# git branch to test above
# branch for testing
###########################

