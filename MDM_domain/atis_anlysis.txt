


==================
atis test dataset 
==================
problematic queries 
with number as token
query_with_unkonwn slot_issue   what meals are served on american flight 665 673 from milwaukee to seattle      O B-meal O O O B-airline_name O B-flight_number I-flight_number O B-fromloc.city_name O B-toloc.city_name
query_with_unkonwn slot_issue   show me the delta flights which serve a snack to coach passengers       O O O B-airline_name O O O O B-meal_description O B-compartment O


query_with_unkonwn slot_issue   list airports in arizona nevada and california please   O O O B-state_name I-state_name O B-state_name O
query_with_unkonwn slot_issue   what class is fare code q       O O O O O B-booking_class
query_with_unkonwn slot_issue   a flight from baltimore to san francisco arriving between 5 and 8 pm    O B-flight O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O O B-arrive_time.start_time O B-arrive_time.end_time I-arrive


has i-flight_number unknow slot

solution:
ignore if slot is not inside the list



=================
atis how to load file 
=================
archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)
'atis_model'
'pytorch_model.bin'
? not sure where the location is

cached_path
resolved_archive_file = cached_path(

TRANSFORMERS_CACHE
cache_dir : 'C:\\Users\\chiecha.REDMOND/.cache\\huggingface\\transformers'
in only stores weights, not other thing


=================
bert config  && huper paarmeter
atis experiment
=================

//joint bert 
        # Prepare optimizer and schedule (linear warmup and decay)
        no_decay = ['bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
             'weight_decay': self.args.weight_decay},
            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
        ]
		//fixed in code
		i update to this in atis


optimizer
	no hvd compressor

schedular traing steps 
t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs
		(140 // 1) * 5
		// floor operation 
		// fixed in code 
		// my case no floor operation but should be the same 
		
		
if (step + 1) % self.args.gradient_accumulation_steps == 0:
	whatever step it is ,  it will alawys be zero


batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU
	? it seems only one example will be evaluate, batch_size is not being used
		
epoch_iterator = tqdm(train_dataloader, desc="Iteration")
	4487 / 32 = 140
		會補足
	step will be 32


if 0 < self.args.max_steps < global_step:
	max_steps = -1 and global_step keeps +1
	so it will nevert happen


epsilon
1e-08
	pass as parameter, the same

drop rate = 0.1
evavulate batchsize = 64
trainf batch size = 32
use crf = false
warmup_steps = 0
	//fixed in code, the same 
weight_decay = 0.0
	//fixed in code
	i update to this in atis
gradient_accumulation_step = 1

igonre_index = 0 

learning rate 5e-05
	pass as parameter
	i updare to this in atis 
logging steps 200

max_grad_borm 1.0

max_seq_len = 50
max_steps = -1

num_train_epochs = 5.0
	pass as parameter, the same
save_steps = 200
	it will trigger to save model
seed = 1234
	? not sure if the same as seed_val fixed code , random seed for torch, cuda
	https://github.com/monologg/JointBERT/blob/7497631c2065f3f7be853b893e0730676745e0fe/utils.py
	refer to here 
	fixed in code 
	i update to this in atis

slot_loss_coef = 1.0
	fixed in code
	
slot_pad_label : PAD

pad_token_label_id
	assocaited with slot_pad_label
	

epoch = 5

bert config
	attention_probs_dropout_prob = 0.1
		fixed in code, the same 
	chunk_size_feed_forward : 0 
		fixed in code, the same 
	diviertiy penalty -0.0
		fixed in code, the same 
	do _ sample : False 
		fixed in code, the same 
	early_stopping = false 
		fixed in code, the same 
	gradient_checkpoint : false
		fixed in code, the same 
	hodden_act : gelu
		fixed in code, the same 
	hidden_dropout_prob = 0.1
		fixed in code, the same 
	hidden_size - 768
	idslabel 
		0 : label_0
		1: labe l1
		
	initialzie_range = 0.02
		fixed in code, the same 
	intermediat_size = 3072
		fixed in code, the same 
	
	layer_nor,_eps = 1e-12
		fixed in code, the same 
	length_penalty : 1.0
		fixed in code, the same 
	
	max_length 20
		fixed in code, the same 
	max_grad_norm 1.0
		fixed int code, the same 
		

	max_position_embedding:512
		fixed in code, the same 
	model_type : bert 
		fixed in code, the same 
	name_or_path : bert-case-uncased
		? not sure if it affects or not 
		need to provide by myself
	
	ni repated_ngram_size : 0
		fixed in code, the same 
	num attention ahead = 12
		fixed in code, the same 
	num beam grups 1
		fixed in code, the same 
	num beams 1
		fixed in code, the same 
	num_hidden layers -12
		my case i go witj 3 
	num_labels = 2
		fixed in code, the same 
	num_return_sequences 1
		fixed in code, the same 
	output attention false 
		fixed in code, the same 
	pad token _id = 0
		fixed in code, the same 
	psoitoon_emveddding absolue
		fixed in code, the same 
	repetititon penalty  1.0
		fixed in code, the same 
	return ditc : true 
		fixed in code, the same 
	temperatire 1.0
		fixed in code, the same 
	top k 50
		fixed in code, the same 
	top p 1.0
		fixed in code, the same 
	torchscript fasle
		fixed in code, the same 
	type vocan size = 2
		fixed in code, the same 
	vocan size = 30522
		fixed in code, the same 
	



// my config

run 7
	改了parameter
	跟run 4 比起來 slot 確實有比較好  雖然還是低
	atis iteration 中最後snapshot 沒有return failure 的case (還不知道原因是啥)
	intent 每個turn 都依樣 ? 這個真的不make sense
	from 
	Validation metric after iteration 5 : {'total_intent_precision': 0.7321428571428571, 'total_intent_recall': 0.7321428571428571, 'total_slot_precision': 0, 'total_slot_recall': 0}
	to
	Validation metric after iteration 5 : {'total_intent_precision': 0.7321428571428571, 'total_intent_recall': 0.7321428571428571, 'total_slot_precision': 0.5263466042154566, 'total_slot_recall': 0.2630962832894352}

run 10
	to 12 layers 
	all zero 
	
	? not sure why 

run 13 
	same as rum 7, max token = 50
	no big difference
	
run 37
	ouput_randomsampler_v1_02152021v1
	add random sampler
	status : fail but can output model
	Validation metric after iteration 5 : {'total_intent_precision': 0.9631449631449631, 'total_intent_recall': 0.9631449631449631, 'total_slot_precision': 0.6362861138588533, 'total_slot_recall': 0.6925818667854756}
	using cpu locally to load gpu trained model(*.pt) and do metric verification
			yes it can be verified, the same

run 42
	ouput_randomsampler_layer12_v1_02162021v1
	add random sampler
	status : fail but can output model
	try 12 layers to see whether performance is better than run 37
	Validation metric after iteration 5 : {'total_intent_precision': 0.9325441143622962, 'total_intent_recall': 0.9325441143622962, 'total_slot_precision': 0.570620239390642, 'total_slot_recall': 0.5840944531075963}
	Validation metric Iob after iteration 5 : {'intent_acc': 0.9325441143622962, 'slot_precision': 0.6037959381044488, 'slot_recall': 0.6852431127208869, 'slot_f1': 0.6419464294894864}			
	// no big difference
	using cpu locally to load gpu trained model(*.pt) and do metric verification
		yes it can be verified, the same
		
run 43/45
	add random sampler
	extend epoch from  5 to 15 to see performance
	layer = 3
	Validation metric after iteration 14 : {'total_intent_precision': 0.9955327228054501, 'total_intent_recall': 0.9955327228054501, 'total_slot_precision': 0.2737879045217596, 'total_slot_recall': 0.778050048266132}
	Validation metric Iob after iteration 14 : {'intent_acc': 0.9955327228054501, 'slot_precision': 0.27106448912714387, 'slot_recall': 0.8352540884644934, 'slot_f1': 0.4092993236611045}
	intent improves but slots does not update much

run 49/51
	add random sampler
	status : fail but can output model
	alawyas this error it can be different log files  eg : 70_drive_log_0.txt,  70_drive_log_1.txt
	Traceback (most recent call last):
	File "atis_train_horovod_joint_intent_slot_TNLR.py", line 2031, in <module>
		model_to_save.save_pretrained(out_dir)
	File "/azureml-envs/azureml_1807a796a9d0358a0054a3b50c9aa95a/lib/python3.6/site-packages/transformers/modeling_utils.py", line 812, in save_pretrained
		torch.save(state_dict, output_model_file)
	File "/azureml-envs/azureml_1807a796a9d0358a0054a3b50c9aa95a/lib/python3.6/site-packages/torch/serialization.py", line 328, in save
		_legacy_save(obj, opened_file, pickle_module, pickle_protocol)
	File "/azureml-envs/azureml_1807a796a9d0358a0054a3b50c9aa95a/lib/python3.6/site-packages/torch/serialization.py", line 196, in __exit__
		self.file_like.close()
	OSError: [Errno 5] Input/output error
		1>sometimes error happens to model_to_save.save_pretrained(out_dir) 
		2> sometimes error happens to torch.save(model, os.path.join(out_dir, 'model.pt'))
		看起來像是race condition
		trying in run 58/60 by comment the second one but it still error
		not sure why....

	extend epoch from  5 to 15 to see performance
	layer = 3
	igonre PAD token for evaluation to check performance - folliwng atis logic
	
	最好加個tab 分割
	wo pad metric 真的可以提升
	雖然iob 跟my logic 仍然有落差
	my logic 
		w pad
			slot_precision 非常低 
			Validation metric after iteration 15 : {'total_intent_precision': 0.9959794505249051, 'total_intent_recall': 0.9959794505249051, 'total_slot_precision': 0.3740392826643894, 'total_slot_recall': 0.7805747382490532}
			pad	: total_tp: 4912, total_fp: 27029, total_fn: 4042
			
			很多slot 的false positive 都增加了
			可能就來自於pad 的location 我想
		wo pad
			iteration 5 可以到80
			Validation metric wo pad after iteration 15 : {'total_intent_precision': 0.9959794505249051, 'total_intent_recall': 0.9959794505249051, 'total_slot_precision': 0.9048287478944413, 'total_slot_recall': 0.8960742882562278}
			pad	: total_tp: 0, total_fp: 111, total_fn: 0
	
	打算用這個version 在local repo 一下 
		but pytorch_model.bin size = 0 , 輸出有問題 我想... 只有model.pt
		mode.pt 沒辦法load 不知道為啥.... 無法verify
	training set 看看是不是都依樣
			
			
	
run 67/69
	remove ranmdom sampler 
	extend to 25 epochs to check metrics
	to see if 'status : fail but can output model' can be removed
	the status is correct by metric is low....
	Validation metric wo pad after iteration 25 : {'total_intent_precision': 0.9714285714285714, 'total_intent_recall': 0.9714285714285714, 'total_slot_precision': 0.7425512104283054, 'total_slot_recall': 0.6940818102697999}


run 70/74
	add random sampler
	status : fail but can output model
	alawyas this error it can be different log files  eg : 70_drive_log_5.txt,  70_drive_log_7.txt
	overall intent precision: 0.9975429975429976, overall intent recall: 0.9975429975429976
	Validation metric wo pad after iteration 15 : {'total_intent_precision': 0.9975429975429976, 'total_intent_recall': 0.9975429975429976, 'total_slot_precision': 0.8887891873703101, 'total_slot_recall': 0.8812277580071174}
	
	ouput_randomsampler_layer12_v1_02172021v1
	this onw pytorch_model.bin is not empty
	trying to replicate metrics and validating test dataset (run 49/51) but might be litle bit different
	
	atis_train
		local 
			Validation metric wo pad after iteration 15: {'total_intent_precision': 0.9975429975429976, 'total_intent_recall': 0.9975429975429976, 'total_slot_precision': 0.8887891873703101, 'total_slot_recall': 0.8812277580071174
	
	atis_test
		some problematic queris
		slot is lower compared to train set
			 Validation metric wo pad after iteration 15: {'total_intent_precision': 0.9761904761904762, 'total_intent_recall': 0.9761904761904762, 'total_slot_precision': 0.8285714285714286, 'total_slot_recall': 0.8255052661542841}
	
	
run 80/?
	in jupiter book 
	#disable distirbuted computing
    #node_count=8,
    node_count=1,
	nd wo hvd rank ,add random sampler
	
	then it seems one cpu can train 
	status will not fail
	
	
	//compared to run 70/74 the same epoch runs 
	// the metric is lower, but not sure why
	trainnig set 
		Validation metric Iob wo pad after iteration 5 : {'intent_acc': 0.9542104087558633, 'slot_precision': 0.7881576018401721, 'slot_recall': 0.7849831873776004, 'slot_f1': 0.7865671918099858}


=================
evaluation logic
i shou;d ignore o and pad for calculation 
i thikn 
================
	
=================
code flow 
=================
joint bert 
	in the first place 
	model zero_grad but i do not have , i have optimizer.zero_gra[
	
	in some case model will .zerograd  but i do have 
	
	

=================
original aml behavaior
=================
run 1
 Validation metric after iteration 5 : {'total_intent_precision': 0.7321428571428571, 'total_intent_recall': 0.7321428571428571, 'total_slot_precision': 0, 'total_slot_recall': 0}



=================
original atis behavior
=================

max_len =50

i want to fly from baltimore to dallas round trip

attention (same as my att_masks)
len(12)
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...]

input_id (same as my text_id)
[101, 1045, 2215, 2000, 4875, 2013, 6222, 2000, 5759, 2461, 4440, 102, 0, 0, ...]


len = 12


token_type1_id (i do not provide this )
all zero to max_len = 50
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]

slot label id (same as my labels_for_text_id)
len = 12
[0, 2, 2, 2, 2, 2, 73, 2, 114, 98, 99, 0, 0, 0, ...]
[0, 2, 2, 2, 2, 2, 73, 2, 114, 98, 99, 0, 0, 0, ...]


0 2 2 2 100, 111   
0 for pad token
2 for untag toekn
CLS SEP for zero
otherthan that using 2



i'm looking for a flight from charlotte to las vegas that stops in st. louis hopefully a dinner flight how can i find that out
golden query to check preprocessing

before adding CLS and sep
[2, 2, 2, 2, 2, 2, 2, 2, 73, 2, 114, 115, 2, 2, ...]
len() = 29=8
2
2
2
2
2
2
2
2
73
2
114
115
2
2
2
// here st.louis extend
103
103
104
2
2
81
2
2
2
2
2
2






slot precision 0.94



=================
TNLR  using atis repo
train = dev = eva
epoch = 5
layer = 3
=================

C:\Users\chiecha.REDMOND\.cache\huggingface_TNLR_02162021v1
// eboch_atis_five_extend_b_slot_and_with_CLS_SEP_PAD_with_TNLR_02162021
//including PAD for evaluation
evaluate TNLR {'intent_acc': 0.9669495310406432, 'slot_precision': 0.5178640144451351, 'slot_recall': 0.7128305423134906, 'slot_f1': 0.5999039879299113, 'sementic_frame_acc': 0.13867798124162573} :


// remove pad for evaluation
// slot is better
 evaluate TNLR {'intent_acc': 0.9669495310406432, 'slot_precision': 0.7281873056520943, 'slot_recall': 0.7662154359402066, 'slot_f1': 0.7467175190696511, 'sementic_frame_acc': 0.4582402858418937} :



=================
TNLR  using atis repo
train = dev = eva
epoch = 5
layer = 12
=================
layer12 is not better than layer3

// eboch_atis_five_extend_b_slot_and_with_CLS_SEP_PAD_with_TNLR_layer12_02162021
//including PAD for evaluation
 evaluate TNLR {'intent_acc': 0.93144260830728, 'slot_precision': 0.5129070455891059, 'slot_recall': 0.3529723342704641, 'slot_f1': 0.41816909226944704, 'sementic_frame_acc': 0.0031263957123715946} :


// remove pad for evaluation
// not being better.... not sure why
 evaluate TNLR {'intent_acc': 0.93144260830728, 'slot_precision': 0.49209596429235636, 'slot_recall': 0.5092705459677936, 'slot_f1': 0.5005359732643924, 'sementic_frame_acc': 0.08240285841893702} :




=================
v1
CLS , SEP label = 0, as pad
B-label extend 
train = dev = eva
this case 
https://github.com/chakki-works/seqeval/blob/master/seqeval/metrics/sequence_labeling.py
 warnings.warn('{} seems not to be NE tag.'.format(chunk))
         >>> from seqeval.metrics.sequence_labeling import get_entities
        >>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']
        >>> get_entities(seq)
        [('PER', 0, 1), ('LOC', 3, 3)]
		
since v1 will break this chunk


this case evaluation will be different
my logic: B-code I-code are different span 
IOB logic: B-code I-code are the same slot 

=================

// applied in atis repo
// cpu local
// slot 9,1,2
{'intent_acc': 0.9973202322465387}
{'slot_f1': 0.982715575187248, 'slot_precision': 0.9805812839348451, 'slot_recall': 0.984859177519728}
{'sementic_frame_acc': 0.9522108083966057}


//applied in my local
//3 layers
//cpu
// same setting but different runs might be differentve81
//first outputs_temp_load_v1_02142021v1
//outputs_local_load_v1_02142021v1
  Validation metric after iteration 5 : {'total_intent_precision': 0.9220460129551039, 'total_intent_recall': 0.9220460129551039, 'total_slot_precision': 0.6902472527472527, 'total_slot_recall': 0.7089552238805971}
//second time 
outputs_temp_load_v1_02152021v1 >outputs_local_load_v1_02152021v1
   Validation metric after iteration 5 : {'total_intent_precision': 0.8981460799642618, 'total_intent_recall': 0.8981460799642618, 'total_slot_precision': 0.5670757511637748, 'total_slot_recall': 0.5970149253731343}
  
Time: 32m 1s
// test local to load model and do metric calculation again, to further verify
// load onnx bin still cannot work -error
Unable to load from type '<class 'onnx.onnx_ml_pb2.ModelProto'>'
// store
E:\azure_ml_notebook\outputs_temp_load_v1_02142021v1
// using pytorch_model.bin to local test
// third time, my evauation replicate success in atis_model_saved
// thrid time  IOB logic is higher
Validation metric Iob: {'intent_acc': 0.8981460799642618, 'slot_precision': 0.6036986236579482, 'slot_recall': 0.700471956975085, 'slot_f1': 0.6484948558364029}  
// trying to reproduce for all merics - wo pad and wo pad
 Validation metric : {'total_intent_precision': 0.8956890775072593, 'total_intent_recall': 0.8956890775072593, 'total_slot_precision': 0.4078921277017648, 'total_slot_recall': 0.5346031038835672}
 Validation metric wo pad: {'total_intent_precision': 0.8956890775072593, 'total_intent_recall': 0.8956890775072593, 'total_slot_precision': 0.5564067712838255, 'total_slot_recall': 0.5007784697508897}
 Validation metric Iob wo pad: {'intent_acc': 0.8956890775072593, 'slot_precision': 0.4143335281304787, 'slot_recall': 0.6419986829107672, 'slot_f1': 0.5036324303380583}
 Validation metric Iob: {'intent_acc': 0.8956890775072593, 'slot_precision': 0.6458380757265346, 'slot_recall': 0.6290137826552858, 'slot_f1': 0.6373149136107523}




//applied in my local
//12 layers
//cpu
// not big improvement
outputs_local_load_layer12_v1_02162021v1
 Validation metric : {'total_intent_precision': 0.9309805673442038, 'total_intent_recall': 0.9309805673442038, 'total_slot_precision': 0.537526347485697, 'total_slot_recall': 0.5302220242073216}


=================
MDM experiment 
compared to hvd initilization
CLS , SEP label = 0, as pad
=================

run 14/16
	using distributed sampler
	original hyper parameters
	3 layer TNLR
	epoch = 25 to check
	so far 
	即使用distributedSampler  結果也還可以  看起來atis 就是data 量不夠多
	等跑完25 可能要加入code
	
	training da
	MDM_01202021v1
			check semantic frame (不知道yue 有沒有這個code)
		Validation metric wo pad after iteration 17 : {'total_intent_precision': 0.974581939799331, 'total_intent_recall': 0.974581939799331, 'total_slot_precision': 0.8828073246900728, 'total_slot_recall': 0.9165029228275287}
			seems after iteration 18 it will saturate
		Validation metric wo pad after iteration 25 : {'total_intent_precision': 0.9770122630992196, 'total_intent_recall': 0.9770122630992196, 'total_slot_precision': 0.8929008059442842, 'total_slot_recall': 0.9238345532249349}

	// original format
	//TVS_october_validation data(loccaly)
	//	Validation metric wo pad: {'total_intent_precision': 0.9302210081946859, 'total_intent_recall': 0.9302210081946859, 'total_slot_precision': 0.8338330834582709, 'total_slot_recall': 0.8737889499869076}


	change to IOB2 format as haoda suggeest
		training data
			whole night net yet finished
			might be not a good idea to evaluate
		TVS_october_validation data(loccaly)
		{
  'intent_acc': 0.9302210081946859,
  'ABSOLUTE_LOCATION': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'AD': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 0
  },
  'ATTACHMENT': {
    'precision': 0.6,
    'recall': 1.0,
    'f1-score': 0.7499999999999999,
    'support': 3
  },
  'AVAILABILITY': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 44
  },
  'CONTACT_NAME': {
    'precision': 0.9028213166144201,
    'recall': 0.9422028353326063,
    'f1-score': 0.9220917822838847,
    'support': 917
  },
  'DATE': {
    'precision': 0.3333333333333333,
    'recall': 0.5,
    'f1-score': 0.4,
    'support': 2
  },
  'DECK_LOCATION': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 1
  },
  'DECK_NAME': {
    'precision': 0.9565217391304348,
    'recall': 0.7857142857142857,
    'f1-score': 0.8627450980392156,
    'support': 28
  },
  'DESTINATION_CALENDAR': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'DESTINATION_PLATFORM': {
    'precision': 0.675,
    'recall': 0.9,
    'f1-score': 0.7714285714285714,
    'support': 30
  },
  'DURATION': {
    'precision': 1.0,
    'recall': 0.3333333333333333,
    'f1-score': 0.5,
    'support': 3
  },
  'END_TIME': {
    'precision': 1.0,
    'recall': 0.8333333333333334,
    'f1-score': 0.9090909090909091,
    'support': 6
  },
  'FEEDBACK_SUBJECT': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 0
  },
  'FEEDBACK_TYPE': {
    'precision': 0.5555555555555556,
    'recall': 0.8333333333333334,
    'f1-score': 0.6666666666666667,
    'support': 6
  },
  'FILE_ACTION': {
    'precision': 0.9459459459459459,
    'recall': 0.9722222222222222,
    'f1-score': 0.9589041095890412,
    'support': 36
  },
  'FILE_FILERECENCY': {
    'precision': 1.0,
    'recall': 0.875,
    'f1-score': 0.9333333333333333,
    'support': 8
  },
  'FILE_KEYWORD': {
    'precision': 0.8466666666666667,
    'recall': 0.8819444444444444,
    'f1-score': 0.8639455782312925,
    'support': 288
  },
  'FILE_RECENCY': {
    'precision': 0.9487179487179487,
    'recall': 0.9487179487179487,
    'f1-score': 0.9487179487179487,
    'support': 39
  },
  'FILE_TYPE': {
    'precision': 0.9144736842105263,
    'recall': 0.9455782312925171,
    'f1-score': 0.9297658862876254,
    'support': 294
  },
  'FROM_CONTACT_NAME': {
    'precision': 0.5,
    'recall': 1.0,
    'f1-score': 0.6666666666666666,
    'support': 2
  },
  'FROM_RELATIONSHIP_NAME': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'IMPLICIT_LOCATION': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'MEETING_STARTTIME': {
    'precision': 0.3333333333333333,
    'recall': 1.0,
    'f1-score': 0.5,
    'support': 1
  },
  'MEETING_TITLE': {
    'precision': 0.5,
    'recall': 0.75,
    'f1-score': 0.6,
    'support': 4
  },
  'MEETING_TYPE': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 5
  },
  'MESSAGE': {
    'precision': 0.7976653696498055,
    'recall': 0.8541666666666666,
    'f1-score': 0.8249496981891348,
    'support': 240
  },
  'MESSAGE_CATEGORY': {
    'precision': 0.8,
    'recall': 1.0,
    'f1-score': 0.888888888888889,
    'support': 4
  },
  'NUMERICAL_INCREMENT': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 1
  },
  'OFFICE_LOCATION': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'ORDER_REF': {
    'precision': 0.9479166666666666,
    'recall': 0.9578947368421052,
    'f1-score': 0.9528795811518324,
    'support': 95
  },
  'ORIGINAL_START_TIME': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 1
  },
  'ORIGINAL_TITLE': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'PEOPLE_ATTRIBUTE': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 0
  },
  'PHONE_NUMBER': {
    'precision': 0.8375,
    'recall': 0.9178082191780822,
    'f1-score': 0.8758169934640524,
    'support': 73
  },
  'POSITION_REF': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 3
  },
  'RELATIONSHIP_NAME': {
    'precision': 0.8333333333333334,
    'recall': 0.1724137931034483,
    'f1-score': 0.28571428571428575,
    'support': 29
  },
  'SEARCH_QUERY': {
    'precision': 0.8688524590163934,
    'recall': 0.8983050847457628,
    'f1-score': 0.8833333333333333,
    'support': 118
  },
  'SHARETARGET_NAME': {
    'precision': 0.8205128205128205,
    'recall': 0.9696969696969697,
    'f1-score': 0.8888888888888888,
    'support': 33
  },
  'SHARETARGET_TYPE': {
    'precision': 0.9166666666666666,
    'recall': 0.9166666666666666,
    'f1-score': 0.9166666666666666,
    'support': 12
  },
  'SLIDE_CONTENT_TYPE': {
    'precision': 0.75,
    'recall': 1.0,
    'f1-score': 0.8571428571428571,
    'support': 15
  },
  'SLIDE_NAME': {
    'precision': 0.8846153846153846,
    'recall': 0.9324324324324325,
    'f1-score': 0.9078947368421053,
    'support': 74
  },
  'SLIDE_NUMBER': {
    'precision': 0.9782608695652174,
    'recall': 0.9782608695652174,
    'f1-score': 0.9782608695652174,
    'support': 46
  },
  'SOURCE_PLATFORM': {
    'precision': 1.0,
    'recall': 0.5,
    'f1-score': 0.6666666666666666,
    'support': 2
  },
  'START_DATE': {
    'precision': 0.9481481481481482,
    'recall': 0.9588014981273408,
    'f1-score': 0.9534450651769087,
    'support': 267
  },
  'START_TIME': {
    'precision': 0.9390243902439024,
    'recall': 0.9871794871794872,
    'f1-score': 0.9625,
    'support': 312
  },
  'TEAMSPACE_CHANNEL': {
    'precision': 0.6424581005586593,
    'recall': 0.8582089552238806,
    'f1-score': 0.7348242811501599,
    'support': 134
  },
  'TEAMSPACE_KEYWORD': {
    'precision': 0.6420454545454546,
    'recall': 0.7337662337662337,
    'f1-score': 0.6848484848484849,
    'support': 154
  },
  'TEAMSPACE_MENU': {
    'precision': 0.7777777777777778,
    'recall': 0.875,
    'f1-score': 0.823529411764706,
    'support': 16
  },
  'TEAMSPACE_TEAM': {
    'precision': 0.6956521739130435,
    'recall': 0.5454545454545454,
    'f1-score': 0.6114649681528662,
    'support': 88
  },
  'TEAMSUSER_ACTIVITYTYPE': {
    'precision': 0.7878787878787878,
    'recall': 0.8666666666666667,
    'f1-score': 0.8253968253968254,
    'support': 30
  },
  'TEAMSUSER_STATUS': {
    'precision': 0.8333333333333334,
    'recall': 0.9615384615384616,
    'f1-score': 0.8928571428571429,
    'support': 26
  },
  'TIME': {
    'precision': 0.9090909090909091,
    'recall': 0.975609756097561,
    'f1-score': 0.9411764705882352,
    'support': 82
  },
  'TITLE': {
    'precision': 0.5151515151515151,
    'recall': 0.5743243243243243,
    'f1-score': 0.5431309904153354,
    'support': 148
  },
  'TO_CONTACT_NAME': {
    'precision': 0.9120879120879121,
    'recall': 0.9764705882352941,
    'f1-score': 0.9431818181818181,
    'support': 85
  },
  'VOLUME_LEVEL': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 8
  },
  'slot_precision': array([
    0.,
    0.,
    0.6,
    0.,
    0.90282132,
    0.33333333,
    1.,
    0.95652174,
    0.,
    0.675,
    1.,
    1.,
    0.,
    0.55555556,
    0.94594595,
    1.,
    0.84666667,
    0.94871795,
    0.91447368,
    0.5,
    0.,
    0.,
    0.33333333,
    0.5,
    0.,
    0.79766537,
    0.8,
    1.,
    0.,
    0.94791667,
    1.,
    0.,
    0.,
    0.8375,
    0.,
    0.83333333,
    0.86885246,
    0.82051282,
    0.91666667,
    0.75,
    0.88461538,
    0.97826087,
    1.,
    0.94814815,
    0.93902439,
    0.6424581,
    0.64204545,
    0.77777778,
    0.69565217,
    0.78787879,
    0.83333333,
    0.90909091,
    0.51515152,
    0.91208791,
    1.
  ]),
  'slot_recall': array([
    0.,
    0.,
    1.,
    0.,
    0.94220284,
    0.5,
    1.,
    0.78571429,
    0.,
    0.9,
    0.33333333,
    0.83333333,
    0.,
    0.83333333,
    0.97222222,
    0.875,
    0.88194444,
    0.94871795,
    0.94557823,
    1.,
    0.,
    0.,
    1.,
    0.75,
    0.,
    0.85416667,
    1.,
    1.,
    0.,
    0.95789474,
    1.,
    0.,
    0.,
    0.91780822,
    0.,
    0.17241379,
    0.89830508,
    0.96969697,
    0.91666667,
    1.,
    0.93243243,
    0.97826087,
    0.5,
    0.9588015,
    0.98717949,
    0.85820896,
    0.73376623,
    0.875,
    0.54545455,
    0.86666667,
    0.96153846,
    0.97560976,
    0.57432432,
    0.97647059,
    1.
  ]),
  'slot_f1': array([
    0.,
    0.,
    0.75,
    0.,
    0.92209178,
    0.4,
    1.,
    0.8627451,
    0.,
    0.77142857,
    0.5,
    0.90909091,
    0.,
    0.66666667,
    0.95890411,
    0.93333333,
    0.86394558,
    0.94871795,
    0.92976589,
    0.66666667,
    0.,
    0.,
    0.5,
    0.6,
    0.,
    0.8249497,
    0.88888889,
    1.,
    0.,
    0.95287958,
    1.,
    0.,
    0.,
    0.87581699,
    0.,
    0.28571429,
    0.88333333,
    0.88888889,
    0.91666667,
    0.85714286,
    0.90789474,
    0.97826087,
    0.66666667,
    0.95344507,
    0.9625,
    0.73482428,
    0.68484848,
    0.82352941,
    0.61146497,
    0.82539683,
    0.89285714,
    0.94117647,
    0.54313099,
    0.94318182,
    1.
  ])
}

run 38/40
	using distributed sampler
	original hyper parameters
	3 layer hugging face bert
	after 25 epochs
	CSDS 
	https://msasg.visualstudio.com/Cortana/_build/results?buildId=19042889&view=ms.vss-test-web.build-test-results-tab&runId=197153574&paneView=debug
		it looks like intent is still an issue
		but some queries pipeline will fail
			share this file with jay ongg
			ad jay
			
	training data
	{
  'intent_acc': 0.9867448523427832,
  'ABSOLUTE_LOCATION': {
    'precision': 1.0,
    'recall': 0.25,
    'f1-score': 0.4,
    'support': 8
  },
  'AD': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 0
  },
  'ATTACHMENT': {
    'precision': 0.8490566037735849,
    'recall': 0.8226691042047533,
    'f1-score': 0.8356545961002786,
    'support': 547
  },
  'ATTRIBUTE_TYPE': {
    'precision': 0.5,
    'recall': 0.5,
    'f1-score': 0.5,
    'support': 2
  },
  'AUDIO_DEVICE_TYPE': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 7
  },
  'CONTACT_ATTRIBUTE': {
    'precision': 0.3333333333333333,
    'recall': 0.125,
    'f1-score': 0.18181818181818182,
    'support': 8
  },
  'CONTACT_NAME': {
    'precision': 0.9645897750857724,
    'recall': 0.9732051282051282,
    'f1-score': 0.9688782999000192,
    'support': 23400
  },
  'CONTACT_NAME_TYPE': {
    'precision': 0.3333333333333333,
    'recall': 0.2222222222222222,
    'f1-score': 0.26666666666666666,
    'support': 9
  },
  'DATA_SOURCE': {
    'precision': 0.7307692307692307,
    'recall': 0.6785714285714286,
    'f1-score': 0.7037037037037038,
    'support': 28
  },
  'DATA_SOURCE_NAME': {
    'precision': 0.9876847290640394,
    'recall': 0.9876847290640394,
    'f1-score': 0.9876847290640394,
    'support': 406
  },
  'DATA_SOURCE_TYPE': {
    'precision': 0.9791921664626683,
    'recall': 0.9864364981504316,
    'f1-score': 0.9828009828009827,
    'support': 811
  },
  'DATE': {
    'precision': 0.9284116331096197,
    'recall': 0.9263392857142857,
    'f1-score': 0.9273743016759776,
    'support': 448
  },
  'DECK_LOCATION': {
    'precision': 0.9545454545454546,
    'recall': 0.8936170212765957,
    'f1-score': 0.9230769230769231,
    'support': 47
  },
  'DECK_NAME': {
    'precision': 0.9158200290275762,
    'recall': 0.9348148148148148,
    'f1-score': 0.9252199413489736,
    'support': 675
  },
  'DESTINATION_CALENDAR': {
    'precision': 0.9736842105263158,
    'recall': 0.8409090909090909,
    'f1-score': 0.9024390243902439,
    'support': 44
  },
  'DESTINATION_PLATFORM': {
    'precision': 0.9054263565891473,
    'recall': 0.9404186795491143,
    'f1-score': 0.9225908372827805,
    'support': 621
  },
  'DURATION': {
    'precision': 0.6818181818181818,
    'recall': 0.6521739130434783,
    'f1-score': 0.6666666666666666,
    'support': 23
  },
  'END_DATE': {
    'precision': 1.0,
    'recall': 0.75,
    'f1-score': 0.8571428571428571,
    'support': 16
  },
  'END_TIME': {
    'precision': 0.8113207547169812,
    'recall': 0.86,
    'f1-score': 0.8349514563106797,
    'support': 50
  },
  'FEEDBACK_SUBJECT': {
    'precision': 0.9821428571428571,
    'recall': 0.9649122807017544,
    'f1-score': 0.9734513274336283,
    'support': 114
  },
  'FEEDBACK_TYPE': {
    'precision': 0.8497109826589595,
    'recall': 0.9607843137254902,
    'f1-score': 0.9018404907975459,
    'support': 306
  },
  'FILE_ACTION': {
    'precision': 0.9700680272108844,
    'recall': 0.9596231493943472,
    'f1-score': 0.9648173207036536,
    'support': 2229
  },
  'FILE_FILERECENCY': {
    'precision': 0.9095022624434389,
    'recall': 0.9617224880382775,
    'f1-score': 0.9348837209302325,
    'support': 209
  },
  'FILE_FOLDER': {
    'precision': 0.9230769230769231,
    'recall': 0.9230769230769231,
    'f1-score': 0.9230769230769231,
    'support': 13
  },
  'FILE_KEYWORD': {
    'precision': 0.9280849904658132,
    'recall': 0.9469149527515286,
    'f1-score': 0.9374054202778924,
    'support': 7196
  },
  'FILE_NAME': {
    'precision': 0.5694444444444444,
    'recall': 0.40594059405940597,
    'f1-score': 0.4739884393063584,
    'support': 101
  },
  'FILE_RECENCY': {
    'precision': 0.9701492537313433,
    'recall': 0.972568578553616,
    'f1-score': 0.9713574097135742,
    'support': 401
  },
  'FILE_TYPE': {
    'precision': 0.9664750957854407,
    'recall': 0.9800194255584848,
    'f1-score': 0.9732001377884948,
    'support': 7207
  },
  'FROM_CONTACT_NAME': {
    'precision': 0.8859060402684564,
    'recall': 0.9041095890410958,
    'f1-score': 0.894915254237288,
    'support': 146
  },
  'IMPLICIT_LOCATION': {
    'precision': 0.5,
    'recall': 0.08333333333333333,
    'f1-score': 0.14285714285714285,
    'support': 12
  },
  'JOB_TITLE': {
    'precision': 0.5,
    'recall': 0.25,
    'f1-score': 0.3333333333333333,
    'support': 24
  },
  'MEETING_ROOM': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'MEETING_STARTTIME': {
    'precision': 0.9,
    'recall': 0.9113924050632911,
    'f1-score': 0.9056603773584907,
    'support': 158
  },
  'MEETING_TITLE': {
    'precision': 0.7004608294930875,
    'recall': 0.7916666666666666,
    'f1-score': 0.7432762836185819,
    'support': 192
  },
  'MEETING_TYPE': {
    'precision': 1.0,
    'recall': 0.6585365853658537,
    'f1-score': 0.7941176470588235,
    'support': 41
  },
  'MESSAGE': {
    'precision': 0.9241216950380297,
    'recall': 0.9411656215418664,
    'f1-score': 0.9325657894736843,
    'support': 5422
  },
  'MESSAGE_CATEGORY': {
    'precision': 0.8918918918918919,
    'recall': 0.9166666666666666,
    'f1-score': 0.9041095890410958,
    'support': 36
  },
  'MOVE_EARLIER_TIME': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'MOVE_LATER_TIME': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 3
  },
  'NUMERICAL_INCREMENT': {
    'precision': 0.8913043478260869,
    'recall': 0.9111111111111111,
    'f1-score': 0.9010989010989011,
    'support': 45
  },
  'OFFICE_LOCATION': {
    'precision': 1.0,
    'recall': 0.6666666666666666,
    'f1-score': 0.8,
    'support': 3
  },
  'ORDER_REF': {
    'precision': 0.9683301343570058,
    'recall': 0.9824732229795521,
    'f1-score': 0.9753504108264863,
    'support': 2054
  },
  'ORG_NAME': {
    'precision': 0.3333333333333333,
    'recall': 0.1111111111111111,
    'f1-score': 0.16666666666666666,
    'support': 9
  },
  'ORIGINAL_CONTACT_NAME': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 10
  },
  'ORIGINAL_START_DATE': {
    'precision': 0.1111111111111111,
    'recall': 0.06666666666666667,
    'f1-score': 0.08333333333333334,
    'support': 15
  },
  'ORIGINAL_START_TIME': {
    'precision': 0.7045454545454546,
    'recall': 0.6739130434782609,
    'f1-score': 0.688888888888889,
    'support': 46
  },
  'PEOPLE_ATTRIBUTE': {
    'precision': 0.9401709401709402,
    'recall': 0.9401709401709402,
    'f1-score': 0.9401709401709402,
    'support': 351
  },
  'PHONE_NUMBER': {
    'precision': 0.9230080572963295,
    'recall': 0.9617537313432836,
    'f1-score': 0.9419826404751028,
    'support': 1072
  },
  'POSITION_REF': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 2
  },
  'PROJECT_NAME': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 2
  },
  'RELATIONSHIP_NAME': {
    'precision': 0.8672566371681416,
    'recall': 0.9074074074074074,
    'f1-score': 0.8868778280542987,
    'support': 540
  },
  'SEARCH_QUERY': {
    'precision': 0.8668280871670703,
    'recall': 0.9701897018970189,
    'f1-score': 0.9156010230179029,
    'support': 369
  },
  'SETTING_TYPE': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 1
  },
  'SHARETARGET_NAME': {
    'precision': 0.8889925373134329,
    'recall': 0.9297560975609757,
    'f1-score': 0.9089175011921794,
    'support': 1025
  },
  'SHARETARGET_TYPE': {
    'precision': 0.9428044280442804,
    'recall': 0.9489322191272052,
    'f1-score': 0.945858398889403,
    'support': 1077
  },
  'SHARE_TARGET': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 7
  },
  'SLIDE_CONTENT_TYPE': {
    'precision': 0.9290780141843972,
    'recall': 0.9703703703703703,
    'f1-score': 0.9492753623188406,
    'support': 135
  },
  'SLIDE_NAME': {
    'precision': 0.9340974212034384,
    'recall': 0.9476744186046512,
    'f1-score': 0.9408369408369409,
    'support': 1032
  },
  'SLIDE_NUMBER': {
    'precision': 0.9696682464454977,
    'recall': 0.9893617021276596,
    'f1-score': 0.9794159885112494,
    'support': 1034
  },
  'SLOT_ATTRIBUTE': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 10
  },
  'SOURCE_PLATFORM': {
    'precision': 0.47058823529411764,
    'recall': 0.32432432432432434,
    'f1-score': 0.38400000000000006,
    'support': 74
  },
  'SPEED_DIAL': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 5
  },
  'START_DATE': {
    'precision': 0.9649793996468511,
    'recall': 0.9770560190703218,
    'f1-score': 0.9709801599052413,
    'support': 3356
  },
  'START_TIME': {
    'precision': 0.9523281596452328,
    'recall': 0.9752128666035951,
    'f1-score': 0.9636346639244648,
    'support': 5285
  },
  'TEAMSPACE_CHANNEL': {
    'precision': 0.9085487077534792,
    'recall': 0.9364754098360656,
    'f1-score': 0.922300706357215,
    'support': 488
  },
  'TEAMSPACE_KEYWORD': {
    'precision': 0.8479349186483104,
    'recall': 0.8943894389438944,
    'f1-score': 0.8705428846771603,
    'support': 1515
  },
  'TEAMSPACE_MENU': {
    'precision': 0.849802371541502,
    'recall': 0.9598214285714286,
    'f1-score': 0.9014675052410902,
    'support': 448
  },
  'TEAMSPACE_TAB': {
    'precision': 0.5,
    'recall': 0.23076923076923078,
    'f1-score': 0.3157894736842105,
    'support': 13
  },
  'TEAMSPACE_TEAM': {
    'precision': 0.7585139318885449,
    'recall': 0.8221476510067114,
    'f1-score': 0.7890499194847022,
    'support': 298
  },
  'TEAMSUSER_ACTIVITYTYPE': {
    'precision': 0.9113924050632911,
    'recall': 0.9391304347826087,
    'f1-score': 0.9250535331905783,
    'support': 690
  },
  'TEAMSUSER_STATUS': {
    'precision': 0.9509981851179673,
    'recall': 0.9868173258003766,
    'f1-score': 0.9685767097966729,
    'support': 531
  },
  'TEAMSUSER_TOPIC': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'TIME': {
    'precision': 0.95276008492569,
    'recall': 0.9604066345639379,
    'f1-score': 0.9565680788702371,
    'support': 1869
  },
  'TITLE': {
    'precision': 0.7220408163265306,
    'recall': 0.7514868309260833,
    'f1-score': 0.7364696086594504,
    'support': 2354
  },
  'TO_CONTACT_NAME': {
    'precision': 0.9358423844405153,
    'recall': 0.9640905542544886,
    'f1-score': 0.9497564726993079,
    'support': 3843
  },
  'VOLUME_LEVEL': {
    'precision': 0.9219512195121952,
    'recall': 0.9742268041237113,
    'f1-score': 0.9473684210526315,
    'support': 194
  },
  'slot_precision': array([
    1.,
    0.,
    0.8490566,
    0.5,
    0.,
    0.33333333,
    0.96458978,
    0.33333333,
    0.73076923,
    0.98768473,
    0.97919217,
    0.92841163,
    0.95454545,
    0.91582003,
    0.97368421,
    0.90542636,
    0.68181818,
    1.,
    0.81132075,
    0.98214286,
    0.84971098,
    0.97006803,
    0.90950226,
    0.92307692,
    0.92808499,
    0.56944444,
    0.97014925,
    0.9664751,
    0.88590604,
    0.5,
    0.5,
    0.,
    0.9,
    0.70046083,
    1.,
    0.9241217,
    0.89189189,
    0.,
    0.,
    0.89130435,
    1.,
    0.96833013,
    0.33333333,
    0.,
    0.11111111,
    0.70454545,
    0.94017094,
    0.92300806,
    0.,
    0.,
    0.86725664,
    0.86682809,
    1.,
    0.88899254,
    0.94280443,
    0.,
    0.92907801,
    0.93409742,
    0.96966825,
    0.,
    0.47058824,
    1.,
    0.9649794,
    0.95232816,
    0.90854871,
    0.84793492,
    0.84980237,
    0.5,
    0.75851393,
    0.91139241,
    0.95099819,
    0.,
    0.95276008,
    0.72204082,
    0.93584238,
    0.92195122
  ]),
  'slot_recall': array([
    0.25,
    0.,
    0.8226691,
    0.5,
    0.,
    0.125,
    0.97320513,
    0.22222222,
    0.67857143,
    0.98768473,
    0.9864365,
    0.92633929,
    0.89361702,
    0.93481481,
    0.84090909,
    0.94041868,
    0.65217391,
    0.75,
    0.86,
    0.96491228,
    0.96078431,
    0.95962315,
    0.96172249,
    0.92307692,
    0.94691495,
    0.40594059,
    0.97256858,
    0.98001943,
    0.90410959,
    0.08333333,
    0.25,
    0.,
    0.91139241,
    0.79166667,
    0.65853659,
    0.94116562,
    0.91666667,
    0.,
    0.,
    0.91111111,
    0.66666667,
    0.98247322,
    0.11111111,
    0.,
    0.06666667,
    0.67391304,
    0.94017094,
    0.96175373,
    0.,
    0.,
    0.90740741,
    0.9701897,
    1.,
    0.9297561,
    0.94893222,
    0.,
    0.97037037,
    0.94767442,
    0.9893617,
    0.,
    0.32432432,
    1.,
    0.97705602,
    0.97521287,
    0.93647541,
    0.89438944,
    0.95982143,
    0.23076923,
    0.82214765,
    0.93913043,
    0.98681733,
    0.,
    0.96040663,
    0.75148683,
    0.96409055,
    0.9742268
  ]),
  'slot_f1': array([
    0.4,
    0.,
    0.8356546,
    0.5,
    0.,
    0.18181818,
    0.9688783,
    0.26666667,
    0.7037037,
    0.98768473,
    0.98280098,
    0.9273743,
    0.92307692,
    0.92521994,
    0.90243902,
    0.92259084,
    0.66666667,
    0.85714286,
    0.83495146,
    0.97345133,
    0.90184049,
    0.96481732,
    0.93488372,
    0.92307692,
    0.93740542,
    0.47398844,
    0.97135741,
    0.97320014,
    0.89491525,
    0.14285714,
    0.33333333,
    0.,
    0.90566038,
    0.74327628,
    0.79411765,
    0.93256579,
    0.90410959,
    0.,
    0.,
    0.9010989,
    0.8,
    0.97535041,
    0.16666667,
    0.,
    0.08333333,
    0.68888889,
    0.94017094,
    0.94198264,
    0.,
    0.,
    0.88687783,
    0.91560102,
    1.,
    0.9089175,
    0.9458584,
    0.,
    0.94927536,
    0.94083694,
    0.97941599,
    0.,
    0.384,
    1.,
    0.97098016,
    0.96363466,
    0.92230071,
    0.87054288,
    0.90146751,
    0.31578947,
    0.78904992,
    0.92505353,
    0.96857671,
    0.,
    0.95656808,
    0.73646961,
    0.94975647,
    0.94736842
  ])
}

	TVS_october_validation data(loccaly)
{
  'intent_acc': 0.9327042463372237,
  'ABSOLUTE_LOCATION': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'AD': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 0
  },
  'ATTACHMENT': {
    'precision': 0.75,
    'recall': 1.0,
    'f1-score': 0.8571428571428571,
    'support': 3
  },
  'AVAILABILITY': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 44
  },
  'CONTACT_NAME': {
    'precision': 0.8952772073921971,
    'recall': 0.95092693565976,
    'f1-score': 0.9222633527234267,
    'support': 917
  },
  'DATE': {
    'precision': 0.25,
    'recall': 0.5,
    'f1-score': 0.3333333333333333,
    'support': 2
  },
  'DECK_LOCATION': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 1
  },
  'DECK_NAME': {
    'precision': 0.92,
    'recall': 0.8214285714285714,
    'f1-score': 0.8679245283018867,
    'support': 28
  },
  'DESTINATION_CALENDAR': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'DESTINATION_PLATFORM': {
    'precision': 0.6944444444444444,
    'recall': 0.8333333333333334,
    'f1-score': 0.7575757575757577,
    'support': 30
  },
  'DURATION': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 3
  },
  'END_TIME': {
    'precision': 1.0,
    'recall': 0.8333333333333334,
    'f1-score': 0.9090909090909091,
    'support': 6
  },
  'FEEDBACK_TYPE': {
    'precision': 0.6,
    'recall': 1.0,
    'f1-score': 0.7499999999999999,
    'support': 6
  },
  'FILE_ACTION': {
    'precision': 0.9459459459459459,
    'recall': 0.9722222222222222,
    'f1-score': 0.9589041095890412,
    'support': 36
  },
  'FILE_FILERECENCY': {
    'precision': 1.0,
    'recall': 0.875,
    'f1-score': 0.9333333333333333,
    'support': 8
  },
  'FILE_KEYWORD': {
    'precision': 0.8547854785478548,
    'recall': 0.8993055555555556,
    'f1-score': 0.8764805414551607,
    'support': 288
  },
  'FILE_RECENCY': {
    'precision': 0.9487179487179487,
    'recall': 0.9487179487179487,
    'f1-score': 0.9487179487179487,
    'support': 39
  },
  'FILE_TYPE': {
    'precision': 0.9269102990033222,
    'recall': 0.9489795918367347,
    'f1-score': 0.9378151260504202,
    'support': 294
  },
  'FROM_CONTACT_NAME': {
    'precision': 0.4,
    'recall': 1.0,
    'f1-score': 0.5714285714285715,
    'support': 2
  },
  'FROM_RELATIONSHIP_NAME': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'IMPLICIT_LOCATION': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'MEETING_STARTTIME': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 1
  },
  'MEETING_TITLE': {
    'precision': 0.25,
    'recall': 0.5,
    'f1-score': 0.3333333333333333,
    'support': 4
  },
  'MEETING_TYPE': {
    'precision': 1.0,
    'recall': 0.4,
    'f1-score': 0.5714285714285715,
    'support': 5
  },
  'MESSAGE': {
    'precision': 0.7421875,
    'recall': 0.7916666666666666,
    'f1-score': 0.7661290322580645,
    'support': 240
  },
  'MESSAGE_CATEGORY': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 4
  },
  'NUMERICAL_INCREMENT': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 1
  },
  'OFFICE_LOCATION': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'ORDER_REF': {
    'precision': 0.968421052631579,
    'recall': 0.968421052631579,
    'f1-score': 0.968421052631579,
    'support': 95
  },
  'ORIGINAL_START_TIME': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 1
  },
  'ORIGINAL_TITLE': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 1
  },
  'PHONE_NUMBER': {
    'precision': 0.8375,
    'recall': 0.9178082191780822,
    'f1-score': 0.8758169934640524,
    'support': 73
  },
  'POSITION_REF': {
    'precision': 0.0,
    'recall': 0.0,
    'f1-score': 0.0,
    'support': 3
  },
  'RELATIONSHIP_NAME': {
    'precision': 0.8571428571428571,
    'recall': 0.20689655172413793,
    'f1-score': 0.33333333333333326,
    'support': 29
  },
  'SEARCH_QUERY': {
    'precision': 0.8974358974358975,
    'recall': 0.8898305084745762,
    'f1-score': 0.8936170212765957,
    'support': 118
  },
  'SHARETARGET_NAME': {
    'precision': 0.8611111111111112,
    'recall': 0.9393939393939394,
    'f1-score': 0.8985507246376813,
    'support': 33
  },
  'SHARETARGET_TYPE': {
    'precision': 0.8461538461538461,
    'recall': 0.9166666666666666,
    'f1-score': 0.8799999999999999,
    'support': 12
  },
  'SLIDE_CONTENT_TYPE': {
    'precision': 0.7894736842105263,
    'recall': 1.0,
    'f1-score': 0.8823529411764706,
    'support': 15
  },
  'SLIDE_NAME': {
    'precision': 0.8961038961038961,
    'recall': 0.9324324324324325,
    'f1-score': 0.9139072847682119,
    'support': 74
  },
  'SLIDE_NUMBER': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 46
  },
  'SOURCE_PLATFORM': {
    'precision': 1.0,
    'recall': 0.5,
    'f1-score': 0.6666666666666666,
    'support': 2
  },
  'START_DATE': {
    'precision': 0.9558823529411765,
    'recall': 0.9737827715355806,
    'f1-score': 0.9647495361781077,
    'support': 267
  },
  'START_TIME': {
    'precision': 0.9298780487804879,
    'recall': 0.9775641025641025,
    'f1-score': 0.9531250000000001,
    'support': 312
  },
  'TEAMSPACE_CHANNEL': {
    'precision': 0.7212121212121212,
    'recall': 0.8880597014925373,
    'f1-score': 0.7959866220735785,
    'support': 134
  },
  'TEAMSPACE_KEYWORD': {
    'precision': 0.6390532544378699,
    'recall': 0.7012987012987013,
    'f1-score': 0.6687306501547987,
    'support': 154
  },
  'TEAMSPACE_MENU': {
    'precision': 0.7647058823529411,
    'recall': 0.8125,
    'f1-score': 0.787878787878788,
    'support': 16
  },
  'TEAMSPACE_TEAM': {
    'precision': 0.6463414634146342,
    'recall': 0.6022727272727273,
    'f1-score': 0.6235294117647059,
    'support': 88
  },
  'TEAMSUSER_ACTIVITYTYPE': {
    'precision': 0.8181818181818182,
    'recall': 0.9,
    'f1-score': 0.8571428571428572,
    'support': 30
  },
  'TEAMSUSER_STATUS': {
    'precision': 0.8571428571428571,
    'recall': 0.9230769230769231,
    'f1-score': 0.888888888888889,
    'support': 26
  },
  'TIME': {
    'precision': 0.9302325581395349,
    'recall': 0.975609756097561,
    'f1-score': 0.9523809523809524,
    'support': 82
  },
  'TITLE': {
    'precision': 0.5448717948717948,
    'recall': 0.5743243243243243,
    'f1-score': 0.5592105263157894,
    'support': 148
  },
  'TO_CONTACT_NAME': {
    'precision': 0.898876404494382,
    'recall': 0.9411764705882353,
    'f1-score': 0.9195402298850575,
    'support': 85
  },
  'VOLUME_LEVEL': {
    'precision': 1.0,
    'recall': 1.0,
    'f1-score': 1.0,
    'support': 8
  },
  'slot_precision': array([
    0.,
    0.,
    0.75,
    0.,
    0.89527721,
    0.25,
    1.,
    0.92,
    0.,
    0.69444444,
    0.,
    1.,
    0.6,
    0.94594595,
    1.,
    0.85478548,
    0.94871795,
    0.9269103,
    0.4,
    0.,
    0.,
    1.,
    0.25,
    1.,
    0.7421875,
    1.,
    1.,
    0.,
    0.96842105,
    1.,
    0.,
    0.8375,
    0.,
    0.85714286,
    0.8974359,
    0.86111111,
    0.84615385,
    0.78947368,
    0.8961039,
    1.,
    1.,
    0.95588235,
    0.92987805,
    0.72121212,
    0.63905325,
    0.76470588,
    0.64634146,
    0.81818182,
    0.85714286,
    0.93023256,
    0.54487179,
    0.8988764,
    1.
  ]),
  'slot_recall': array([
    0.,
    0.,
    1.,
    0.,
    0.95092694,
    0.5,
    1.,
    0.82142857,
    0.,
    0.83333333,
    0.,
    0.83333333,
    1.,
    0.97222222,
    0.875,
    0.89930556,
    0.94871795,
    0.94897959,
    1.,
    0.,
    0.,
    1.,
    0.5,
    0.4,
    0.79166667,
    1.,
    1.,
    0.,
    0.96842105,
    1.,
    0.,
    0.91780822,
    0.,
    0.20689655,
    0.88983051,
    0.93939394,
    0.91666667,
    1.,
    0.93243243,
    1.,
    0.5,
    0.97378277,
    0.9775641,
    0.8880597,
    0.7012987,
    0.8125,
    0.60227273,
    0.9,
    0.92307692,
    0.97560976,
    0.57432432,
    0.94117647,
    1.
  ]),
  'slot_f1': array([
    0.,
    0.,
    0.85714286,
    0.,
    0.92226335,
    0.33333333,
    1.,
    0.86792453,
    0.,
    0.75757576,
    0.,
    0.90909091,
    0.75,
    0.95890411,
    0.93333333,
    0.87648054,
    0.94871795,
    0.93781513,
    0.57142857,
    0.,
    0.,
    1.,
    0.33333333,
    0.57142857,
    0.76612903,
    1.,
    1.,
    0.,
    0.96842105,
    1.,
    0.,
    0.87581699,
    0.,
    0.33333333,
    0.89361702,
    0.89855072,
    0.88,
    0.88235294,
    0.91390728,
    1.,
    0.66666667,
    0.96474954,
    0.953125,
    0.79598662,
    0.66873065,
    0.78787879,
    0.62352941,
    0.85714286,
    0.88888889,
    0.95238095,
    0.55921053,
    0.91954023,
    1.
  ])
}

	//test 比較
	// bert 少了feedback_subject, people_attribute
	// ? 可能用單獨設query 看一下
	// 很難說誰比較好
	// 還是要aether evaluate 一下比較好


run 41/43
	using distributed sampler
	original hyper parameters
	3 layer huggingface distilBert
	epoch = 25 to check
	
	training da
	MDM_01202021v1

run 44/46
	using distributed sampler
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
	epoch = 5
	
	training da
	MDM_01202021v1
	
	這個case 的ppt 的相對好一些 
	
	還是要把validation set 都修好  就不用看eran 的PR


run 47/49	
	using distributed sampler
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
	epoch = 10
	
	training da
	MDM_01202021v1
	
	

run 53/55
	test 
	using distributed sampler
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
	epoch = 10
	test data agumentation DSAT(static)
		target open (.*) ppt
		repeat 10 times
		751705



run 56/??
	this one get cancel 
	test 
	using distributed sampler
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
	epoch = 10
	test data agumentation DSAT(static)
		target open (.*) ppt
		add more files DSAT based on 38/40
		repeat 10 times		
		
run 61/63
	test 
	using distributed sampler
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
	epoch = 5
		5 似乎沒有什麼太大的幫助  光training loss 就比不上了
	test data agumentation DSAT(static)
		target open (.*) ppt
		add more DSAT based on 38/40
			相較53/55 根多 files DSAT 
			發現很多augmented data 竟然是multi turn 的  要investigate 一下
			原來是很多的multi turn 的query 找不到
			原因是因為user 這個case 可能太strict....
			
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			
		repeat 10 times
		
		這個因該不需要evaluate 
			
	
run 65/67
	using distributed sampler
	original hyper parameters
	3 layer TNLR
	epoch = 25 to check
	after 25 epochs
	模擬run 14/16
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			沒有intent = X 的logic 
			讓他跑  可能之後evaluate 看看行不行
	job 一瞬間會沒有output 要等一段時間
			
run 85/87
	using distributed sampler
	original hyper parameters
	3 layer TNLR
	10 epoch
	original hyper parameters + learning rate from 1e-5 to 5e-5 
		看看10個epoch + multi turn change + higher learning rate 的搶先看結
	模擬run 38/40
	comared to 65/67, intent might be better
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)

run 85/88
	using distributed sampler
	original hyper parameters
	3 layer TNLR
	10 epoch
	模擬run 38/40
	comared to 65/67, intent might be better
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)




run 89/91
	using distributed sampler
	original hyper parameters
	3 layer Bert
		10 epoch
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
			
run 92/94
	using distributed sampler
	original hyper parameters
	3 layer TNLR
		5 epoch
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	IOB logic
		this is to verify new IOB training code then less node 
		local can verify and new CSDS setup branch is needed
		2 nodes , 一個batch 時間變成5倍 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)



run 96/98
	using distributed sampler
	original hyper parameters
	3 layer TNLR
	original hyper parameters + learning rate from 1e-5 to 5e-5 
		5 epoch
		6 nodes => 只剩6 個nodes
		不小心跟92/94 用到一樣的condition
	IOB logic
		this is to verify new IOB training code then less node 
		2 nodes , 一個batch 時間變成5倍 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)

run 108/110
	using distributed sampler
	original hyper parameters
	3 layer TNLR
	original hyper parameters + learning rate from 1e-5 to 5e-5 
		10 epoch
		6 nodes => 只剩6 個nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue sugge
	sted
	? 可能有不小心新增額外的一些training data, we can verify next week
		766034
	


run 123/125
	using distributed sampler
	original hyper parameters
	3 layer bert
	original hyper parameters + learning rate from 1e-5 to 5e-5 
		10 e
		4 nodes => 只剩4 個nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	data
		765794


run 126/?
	using distributed sampler
	original hyper parameters
	3 layer TNLR
		10 epoch
		4 nodes => 只剩4 個nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	拿掉extra training data 


run 129/?
	using distributed sampler
	original hyper parameters
	3 layer bert
		10 epoch
		4 nodes => 只剩4 個nodes
	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested


run 132/134
	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5
	3 layer TNLR
		25 epoch
		4 nodes => 只剩4 個nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	拿掉extra training dat


run 135/137
	using distributed sampler
	original hyper parameters
	3 layer bert
	original hyper parameters + learning rate from 1e-5 to 5e-5 
		25 epoch
		4 nodes => 只剩4 個nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested

run 138/?
	using distributed sampler
	original hyper parameters
	3 layer TNLR
		25 epoch
		4 nodes => 只剩4 個nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	拿掉extra training dat


run 141/?
	using distributed sampler
	original hyper parameters
	3 layer bert
		25 epoch
		4 nodes => 只剩4 個nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	
	
run 151/153
	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
		2 epoch
		2 nodes => only

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	拿掉extra training dat
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
	765794 -> 765798
	in tokenizer_enforcement_query.tsv slot 有的解了  有的沒解

run 154/?

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer bert
		10 epoch
		2 nodes => 看有沒有node 可以用 理論上因該有

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	拿掉extra training dat
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
	增加epoch 看看會不會都解掉
	765794 -> 765798
	
	
run 157/?

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
		10 epoch
		2 nodes => only

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	拿掉extra training dat
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
	增加epoch 看看會不會都解掉
	765794 -> 765798
	

run 163/?
	not sure the condition
	跟154 基本上一樣的condition  我覺得
	所以先忽略

run 166/168

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
		10 epoch
		2 nodes => only

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110
		32
	765794 -> 767266
	1472 - 32 - 1440
	
	
run 172/174

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer bert
		10 epoch
		4 nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110
		32
	765794 -> 767266
	1472 - 32 - 1440
	
	
	
	
run 175/?

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer bert
		25 epoch
		6 nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110
		32
	765794 -> 767266
	1472 - 32 - 1440
	

done
run 178/?

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
		25 epoch
		6 nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110
		32
	765794 -> 767266
	1472 - 32 - 1440
	
	
run 187/?

	using distributed sampler
	original hyper parameters
	3 layer TNLR
		25 epoch
		6 nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110
		32
	765794 -> 767266
	1472 - 32 - 1440
	
run 190/?

	using distributed sampler
	original hyper parameters
	3 layer bert
		10 epoch
		6 nodes

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110
		32
	765794 -> 767266
	1472 - 32 - 1440	
	

run 193/195

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
		10 epoch
		6 nodes => only
	MDM_TrainSet_01202021v1
		wrong annotation upd

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168
		62
	765794 -> 767296
	1502 - 62 - 1440	
	
run 196/198

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 	3 layer bert
		10 epoch
		6 nodes
	MDM_TrainSet_01202021v1
		wrong annotation update

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 720
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168
		62
	765794 -> 767296
	1502 - 62 - 1440
	
run 199/?

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
		25 epoch
		6 nodes => only
	MDM_TrainSet_01202021v1
		wrong annotation upd

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168
		62
	765794 -> 767296
	1502 - 62 - 1440	
	
run 202/?

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer bert
		25 epoch
		6 nodes
	MDM_TrainSet_01202021v1
		wrong annotation update

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 720
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168
		62
	765794 -> 767296
	1502 - 62 - 1440	



run 205/207
	? not sure why this reporting set will fail
	but bert one 208 will not have problem so using bert data to fine tune again

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
		10 epoch
		6 nodes => only
	MDM_TrainSet_01202021v1
		wrong annotation upd

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168 / 193_195
		77
	765794 -> 767311
	1502 - 77 - 1440	
	
run 208/210

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer bert
		10 epoch
		6 nodes
	MDM_TrainSet_01202021v1
		wrong annotation update

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 720
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168 / 193_195
		62
	765794 -> 767311
	1502 - 77 - 1440

run 214/216

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
		10 epoch
		6 nodes => only
	MDM_TrainSet_01202021v1
		wrong annotation upd

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168 / 193_195 / 208_210
		80
	765794 -> 767314
	1502 - 80 - 1440	
	
run 217/219

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer bert
		10 epoch
		6 nodes
	MDM_TrainSet_01202021v1
		wrong annotation update

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 720
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168 / 193_195 / 208_210
		62
	765794 -> 767314
	1502 - 80 - 1440



run 220/222

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
		10 epoch
		6 nodes => only
	MDM_TrainSet_01202021v1
		wrong annotation upd

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168 / 193_195 / 208_210 / 217_219
		80
	765794 -> 767317
	1502 - 83 - 1440

run 223/225

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer bert
		10 epoch
		6 nodes
	MDM_TrainSet_01202021v1
		wrong annotation update

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 720
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168 / 193_195 / 208_210 / 217_219
		62
	765794 -> 767317
	1502 - 83 - 1440


run 226/228

	using distributed sampler
	original hyper parameters
	3 layer TNLR
		10 epoch
		6 nodes => only
	MDM_TrainSet_01202021v1
		wrong annotation upd

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168 / 193_195 / 208_210 / 217_219
		80
	765794 -> 767317
	1502 - 83 - 1440
	
	感覺會學得不好 因為learning rate 太低

run 229/231

	using distributed sampler
	original hyper parameters
	3 layer bert
		10 epoch
		6 nodes
	MDM_TrainSet_01202021v1
		wrong annotation update

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 720
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168 / 193_195 / 208_210 / 217_219 
		62
	765794 -> 767317
	1502 - 83 - 1440
	
	感覺會學得不好 因為learning rate 太低

run 232/234

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer TNLR
		10 epoch
		6 nodes => only
	MDM_TrainSet_01202021v1
		wrong annotation upd

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 1440
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168 / 193_195 / 208_210 / 217_219 /  223_225
		80
	765794 -> 767318
	1502 - 84 - 1440

run 235/237

	using distributed sampler
	original hyper parameters
	original hyper parameters + learning rate from 1e-5 to 5e-5 
	3 layer bert
		10 epoch
		6 nodes
	MDM_TrainSet_01202021v1
		wrong annotation update

	IOB logic
		this is to verify new IOB training code then less node 
	但是修正multi turn 的data 
			現在把
			personal grammar 當成multi turn (? 以後可能要revisit)
			normal conversational context 的multi 濾掉
			(? 可能以後要加上logic 如果 intent is X 就留著  反正是negative example 但是是不是絕對不好說)
	修正fast_tokenizer 的input 要用untokenizer 的input 
	修正dynamic output as yue suggested
	增加 extra training data set in (open_ppt_augmentation.tsv)
		根據108_110 
		72 * 20 = 720
	test tokenizer_enforcement_query.tsv for wrong tokenizered queries
		根據108_110 / 166_168 / 193_195 / 208_210 / 217_219 /  223_225
		62
	765794 -> 767318
	1502 - 84 - 1440
	
	
==================
eran discussion
==================
query.strip()

token vocab.txt

character => go that
	? but xinjie mentions UNK token as well
	it will have problems



[CLS] call back [SEP]
101   3     4    102 PAD PAD 
PAD   o     o PAD PAD PAD 


for cls and SEP should be zero in mask or not 
[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...]


intent mapping 


create_slot_arrays


miniLM

tinyBERT


word piece
	exception.
	chinese
	UNK
	will check
	
	
post quantification
	in xinjie
	it will also compact mobel size


send an email to notification
	we need to notify again
	
	
======================
seqeval	
https://datascience.stackexchange.com/questions/37824/difference-between-iob-and-iob2-format
======================
https://datascience.stackexchange.com/questions/37824/difference-between-iob-and-iob2-format
2

IOB: Here, I is used for a token inside a chunk, O is used for a token outside a chunk and B is only used for the beginning token of a Named Entity (chunk) spanning more than one token.

Alex I-PER
is O
going O
to O
Los B-LOC
Angeles I-LOC


IOB2: It is same as IOB, except that the B- tag is used in the beginning of every chunk (i.e. all chunks start with the B- tag).
// for jointBERt it uses IOB2 according to traning dataset
Alex B-PER
is O
going O
to O
Los B-LOC
Angeles I-LOC

	差在span 只有single toekn 會是  B  or I
	
	
http://cs229.stanford.edu/proj2005/KrishnanGanapathy-NamedEntityRecognition.pdf


default case 
// should be IOB 2
>>> from seqeval.metrics import accuracy_score
>>> from seqeval.metrics import classification_report
>>> from seqeval.metrics import f1_score
>>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> f1_score(y_true, y_pred)
0.50
>>> classification_report(y_true, y_pred)
              precision    recall  f1-score   support

        MISC       0.00      0.00      0.00         1
			// MISC is not entire span correct so its result is false
         PER       1.00      1.00      1.00         1

   micro avg       0.50      0.50      0.50         2
   macro avg       0.50      0.50      0.50         2
weighted avg       0.50      0.50      0.50         2



//default case will be micro
// average: none to cover the span
======================
haoda sync 02192021
======================
transferdistill 是MSR 在working 
可以有空看看怎麼用




======================
qas setup
======================

//method 1
// this case will miss probability from other intents , other than top 1 intent



//original

Prediction
ModelPrediction + RulePrediction
including al intent and slot
	index starts with 0
	
	each intent from ModelPrediciton
	intent 137
		extends length to the same as queries
			137[1096,1096]=4.63459e-28

			0[1097,1097]=4.63459e-28

			0[1098,1098]=4.63459e-28

			0[1099,1099]=4.63459e-28

			0[1100,1100]=4.63459e-28

			0[1101,1101]=4.63459e-28

			0[1102,1102]=4.63459e-28

			0[1103,1103]=4.63459e-28
	
	each domain will filter based on assistant_enus_tvs_xxx_mv1.intent.idmap.txt
	
	need to update dispatch for each domain
		create IntentProbFieldToTag from IntentProbs, replace intents in each dispatch, no need CRF select
		//reuse IntentProbs repalce intents and no need CRF select
		reuse SlotTags and no need CRF

IntentProbs
139 tags 
包含hotfix (138 +1)
	tag = intent number
to minic originla IntentProbs
	extra rule intent 
	normalize intent_prob from DNN to 0.99
	merge them together

SlotTags
	indexing starting with zero
include cls token, not include intent
SlotTags (tag: 7, string: 0)



output3

// rule 
		ExternalInput4 (tag: 8, string: 0)

			39[0,0]=1.0

			0[1,1]=1.0

			0[2,2]=1.0

			0[3,3]=1.0

			78[4,4]=1.0

			91[5,5]=1.0

			0[6,6]=1.0

			0[7,7]=1.0
			
// dnn 

// intent_prob
			0[0,0]=0.00012395185
			0[1,1]=1.595618e-09
			0[136,136]=2.0400597e-09

			0[137,137]=1.1641267e-08
/  intent tag
			0[0,0]=39.0
			need to move weight to tag
			and set weight according to intent prob

// slot_tag
// need to scale to 0.99 to let hotfix wins
// and shift right by one
// also, tagid needs to map
// 2 => 0 becasue extra two slots unless retrain

			2[0,0]=1.0

			2[1,1]=1.0

			2[2,2]=1.0

			80[3,3]=1.0

			93[4,4]=1.0

			2[5,5]=1.0

			2[6,6]=1.0



=====================
eran branch 
=====================

https://msasg.visualstudio.com/Cortana/_build/results?buildId=19092871&view=ms.vss-test-web.build-test-results-tab&runId=197935550&resultId=100005&paneView=debug

run 38/40
	ValidatingKingstonOctober2020
	ValidatingTeamsOctober2020
	

old MDM check metrics
https://msasg.visualstudio.com/Cortana/_git/CoreScienceDataStaging/pullrequest/1971873?_a=files

======================
YC suggested 02252021
======================

mask LM
language
unlabel data

learing rate
might be 1e-5  too small ?

5e-5



======================
yong suggested 02262021
======================
ppt 
in domain try
out of domain try 




======================
haoda suggested 03022021
======================

02262021
share code for review


03022021
// old 做法是
// trian 的時候是只有給type
// seqeval 轉成b-type i-type
// evalvuate 用原本的type 

open the lu migration pp ##t	2 2 29 29 32 32
									       b-type i-type
										   type   type
										   
										   
										   type type
										   32   32
										   
										   b-type i-type



// haoda 的建議
// 最好在train 的時候  給他boundry 
// type becomes b-type /i type 
//然後  seqeval 可以用
然後再 qas 30 跟31 map 成一樣的
slot = 30
slot = 60  B ,I

										 b-type i-type  i-type
										 30     31		31
										 30     31      31
										 evaluate 
											seqeval  loss 
										 evaluate_cortana
												31 to 30 
										30       30      30
										open question ?




======================
eran sync 03022021 03022021
======================
<message> slot is broken
missed tokens
	has clarified

contact name might be wrong



======================
data problem
======================
training da
MDM_01202021v1
	sent_text => should be send_text
		this is from MDM CRF data arguement modules. originla MDM CRF will ignore it so leave it 
	search_message => should be search_messages

	send_tex => should be send_text

	volum_up => should be volume_up
	
	
======================
gpu subscription list
======================
https://ml.azure.com/compute/list/training?wsid=/subscriptions/ddb33dc4-889c-4fa1-90ce-482d793d6480/resourcegroups/DevExp/workspaces/DevExperimentation&tid=72f988bf-86f1-41af-91ab-2d7cd011db47




	
======================
gpu subscription list
======================

assistant_enus_default_encoder_mv1.normalizer.fst
	 要問minwoo 這個source 在哪
	 我寫的因該是tokenizer
	 
	 
	 看起來要用raw query 然後自己再lower case 
	 而不是 encoding, enocding 會有問題
	 
	 

3:00 pm 
0: SingleTokenView --in=BodySurfaceStream --out=singletoken --inputIsContig
        Inputs:
                BodySurfaceStream (tag: 0, string: 3)
                        Token 0: 3
                        Token 1: 00
                        Token 2: am
        Output:
                singletoken (tag: 0, string: 1)
                        3:00 am[0,2]=1.0
1: NLPFSTApplier --in=singletoken --out=preproc --fst=assistant_enus_default_encoder_mv1.tokenizer.fst --mode=transducer
        Inputs:
                singletoken (tag: 0, string: 1)
                        3:00 am[0,2]=1.0
        Output:
                preproc (tag: 1, string: 1)
                        0[-1,-1]=1.0
                        3:00 am[-1,-1]=1.0
2: ExtractNthFeature --in=preproc --out=lastout --n=-1 --stringfs
        Inputs:
                preproc (tag: 1, string: 1)
                        0[-1,-1]=1.0
                        3:00 am[-1,-1]=1.0
        Output:
                lastout (tag: 0, string: 1)
                        3:00 am[-1,-1]=1.0
3: CharTokenizer --in=lastout --out=RawQuery,InterTokens --sep=\s\t\r\n --del=\t
        Inputs:
                lastout (tag: 0, string: 1)
                        3:00 am[-1,-1]=1.0
        Output:
                RawQuery (tag: 0, string: 2)
                        3:00[0,0]=1.0
                        am[1,1]=1.0
                InterTokens (tag: 0, string: 3)
                        [0,0]=1.0
                         [1,1]=1.0
                        [2,2]=1.0
4: SingleTokenView --in=RawQuery --out=norm_singletoken
        Inputs:
                RawQuery (tag: 0, string: 2)
                        3:00[0,0]=1.0
                        am[1,1]=1.0
        Output:
                norm_singletoken (tag: 0, string: 1)
                        3:00 am[0,0]=1.0
5: NLPFSTApplier --in=norm_singletoken --out=normalized --fst=assistant_enus_default_encoder_mv1.normalizer.fst --mode=transducer
        Inputs:
                norm_singletoken (tag: 0, string: 1)
                        3:00 am[0,0]=1.0
        Output:
                normalized (tag: 1, string: 1)
                        0[-1,-1]=1.0
                        0:00 am[-1,-1]=1.0
6: ExtractNthFeature --in=normalized --out=norm_lastout --n=-1 --stringfs
        Inputs:
                normalized (tag: 1, string: 1)
                        0[-1,-1]=1.0
                        0:00 am[-1,-1]=1.0
        Output:
                norm_lastout (tag: 0, string: 1)
                        0:00 am[-1,-1]=1.0
7: CharTokenizer --in=norm_lastout --out=NormalizedRawQuery,NormalizedInterTokens --sep=\s\t\r\n --del=\t
        Inputs:
                norm_lastout (tag: 0, string: 1)
                        0:00 am[-1,-1]=1.0
        Output:
                NormalizedRawQuery (tag: 0, string: 2)
                        0:00[0,0]=1.0
                        am[1,1]=1.0
                NormalizedInterTokens (tag: 0, string: 3)
                        [0,0]=1.0
                         [1,1]=1.0
                        [2,2]=1.0
8: TextEditor --in=NormalizedRawQuery --out=Encoding --config=assistant_enus_default_encoder_mv1.lowercase.texteditor.txt
        Inputs:
                NormalizedRawQuery (tag: 0, string: 2)
                        0:00[0,0]=1.0
                        am[1,1]=1.0
        Output:
                Encoding (tag: 0, string: 2)
                        0:00[0,0]=1.0
                        am[1,1]=1.0
						
			


==========================
my python && QAS compared
==========================		

//QAS
	Inputs:

		ExternalInput1 (tag: 0, string: 5)

			open[0,0]=1.0

			the[1,1]=1.0

			lu[2,2]=1.0

			migration[3,3]=1.0

			ppt[4,4]=1.0

	Output:

		tokens (tag: 0, string: 8)

			[CLS][0,0]=1.0

			open[0,0]=1.0

			the[0,0]=1.0

			lu[0,0]=1.0

			migration[0,0]=1.0

			pp[0,0]=1.0

			##t[0,0]=1.0

			[SEP][0,0]=1.0

		input_ids (tag: 8, string: 0)

			101[0,0]=1.0

			2330[1,1]=1.0

			1996[2,2]=1.0

			11320[3,3]=1.0

			9230[4,4]=1.0

			4903[5,5]=1.0

			2102[6,6]=1.0

			102[7,7]=1.0


// python output 
// different tokenizer
'open the lu migration pp ##t'
[101, 2330, 1996, 11320, 9230, 4903, 1001, 1001, 1056, 102, 0, 0, 0, 0, ...]	

fast_tokenizer 
	因為 因該要用raw string  extracted from annotation 才對
	如果用 tokenized 的string 會被tokenizerd 兩次 
					

=============================
MSIT, problematic word piece tokenizer
=============================
//MSIT
Open abc
open abc


			
				
==========================
problematic word piece tokenizer
==========================

恩  因該是你說的word piece 的algorithm 可能真的不一樣了....

query : 3:00 am 

//qas 
[6:18 PM] Chieh-Chun Chang
            input_ids (tag: 6, string: 0)
            101[0,0]=1.0
            1017[1,1]=1.0
            29627[2,2]=1.0
            8889[3,3]=1.0
            2572[4,4]=1.0
            102[5,5]=1.0


			3[0,0]=1.0

			##:[0,0]=1.0

			##00[0,0]=1.0

//python 
101, 1017, 1024, 4002, 2572,  102,
            :    00
			
			


Un-favorite this team
//qas
		tokens (tag: 0, string: 9)

			[CLS][0,0]=1.0

			un[0,0]=1.0

			##-[0,0]=1.0

			##fa[0,0]=1.0

			##vor[0,0]=1.0

			##ite[0,0]=1.0

			this[0,0]=1.0

			team[0,0]=1.0

			[SEP][0,0]=1.0

		input_ids (tag: 9, string: 0)

			101[0,0]=1.0

			4895[1,1]=1.0

			29624[2,2]=1.0

			7011[3,3]=1.0

			14550[4,4]=1.0

			4221[5,5]=1.0

			2023[6,6]=1.0

			2136[7,7]=1.0

			102[8,8]=1.0
//python 
tensor([ 101, 4895, 1011, 5440, 2023, 2136,  102,   



Un-favorite the voice skills team

//qas 
		tokens (tag: 0, string: 11)

			[CLS][0,0]=1.0

			un[0,0]=1.0

			##-[0,0]=1.0

			##fa[0,0]=1.0

			##vor[0,0]=1.0

			##ite[0,0]=1.0

			the[0,0]=1.0

			voice[0,0]=1.0

			skills[0,0]=1.0

			team[0,0]=1.0

			[SEP][0,0]=1.0

		input_ids (tag: 11, string: 0)

			101[0,0]=1.0

			4895[1,1]=1.0

			29624[2,2]=1.0

			7011[3,3]=1.0

			14550[4,4]=1.0

			4221[5,5]=1.0

			1996[6,6]=1.0

			2376[7,7]=1.0

			4813[8,8]=1.0

			2136[9,9]=1.0

			102[10,10]=1.0
			
//python
101 4895 1011 5440 1996 2376 4813 2136 102



==========================
adapy yue's link result 
==========================
query:
	Un-favorite the voice skills team
BertTokenizer
	also has BertTokenizer2
https://msasg.visualstudio.com/QAS/_git/qas?path=%2Fprivate%2Fanswers%2FSDS%2FQCS%2Flib%2Fsrc%2Fmlg3.4%2Fdocs%2Fprocessors%2FBertBasicTokenizer.md&_a=contents&version=GBmaster

MLM files pipeline.txt
cmfv2_cpu_mlm.domain.tokenizer.pipeline.txt

 -c mlm_cmfv2_test_qas_model\ --variant qd_mlm_qas_merge --dumpFormat json --avoidProcessingTermination -o qas_eval\ --debugContext --verbose --threadsNum 4 --loadingThreadsNum 1 --legacyallowunusedparameters --queryInColumn 1 -i queries.txt



WordPiecetokenizer1,2,5
	?還不知道差別


original raw query
        Inputs:
                RawQuery (tag: 0, string: 5)
                        Un-favorite[0,0]=1.0
                        the[1,1]=1.0
                        voice[2,2]=1.0
                        skills[3,3]=1.0
                        team[4,4]=1.0


SubwordToWord --in=tokens,slot_output --out=words,words_output --ignoreTokens=[CLS],[SEP]

original
		## will ber merged into sinlge token
			Output:

		words (tag: 0, string: 5)

			un-favorite[0,0]=1.0

			the[1,1]=1.0

			voice[2,2]=1.0

			skills[3,3]=1.0

			team[4,4]=1.0

		words_output (tag: 5, string: 0)

			0[0,0]=173.0

			0[1,1]=2.0

			0[2,2]=173.0

			0[3,3]=174.0

			0[4,4]=2.0
			
new // unmatch original raw query 
			Output:

		words (tag: 0, string: 7)

			un[0,0]=1.0

			-[1,1]=1.0

			favorite[2,2]=1.0

			the[3,3]=1.0

			voice[4,4]=1.0

			skills[5,5]=1.0

			team[6,6]=1.0

		words_output (tag: 7, string: 0)

			0[0,0]=2.0

			0[1,1]=2.0

			0[2,2]=2.0

			0[3,3]=2.0

			0[4,4]=173.0

			0[5,5]=174.0

			0[6,6]=2.0
			
==========================
Wordtokenizer 5

==========================			
WordPieceTokenizer5 --in=ExternalInput1 --out=original_tokens,

files answer internatial 
用unk 取代original query

測試chiness 任一個query 就會btea


跟他的chat 來看看
Guoxuan Zhu
04012021
國學說 intenational 的會把它變成最小的力度 
測試中會變成3個token 所以length 會match




originla token 就不會有UNK 可能是不是要把這個最後用來replace 
			[CLS][0,0]=1.0

			測[1,1]=1.0

			試[2,2]=1.0

			[SEP][3,3]=1.0

		tokens_temp (tag: 0, string: 4)

			[CLS][0,0]=1.0

			[UNK][1,1]=1.0

			[UNK][2,2]=1.0

			[SEP][3,3]=1.0


non cpu version 
	still using wordpiecetokenizer
qd_mlm_qas_merge.QueryProcessingConfiguration.ini

cpu version 
	using wordpiecetokenizer5
	this is for extra
		cmfv2_cpu_mlm_domain_tokenizer_sequence_length
	
qd_mlm_qas_merge_cpu.QueryProcessingConfiguration.ini


qd_mlm_domain_model_featurizer
cmfv2_cpu_mlm.domain.model.pipeline.txt
	input1
		cmfv2_cpu_mlm_domain_tokenizer_input_ids
	input2
		cmfv2_cpu_mlm_domain_tokenizer_input_mask
		
in post_proc
12: FeatureSetConcat --in=ExternalInput1 --out=CRFInputQuery_temp

	// normal 
	// 'two token'
	// both of them are two tokens


	//chiness
	// 測試
	// normal
	//  external input only one 
	// bert 
	//  two unk tokens
	
	
	
	Inputs:

		ExternalInput1 (tag: 0, string: 2)

			[CLS][0,0]=1.0

			測試[1,1]=1.0

	Output:

		CRFInputQuery_temp (tag: 0, string: 2)

			[CLS][0,0]=1.0

			測試[1,1]=1.0
	// but bert output two tokens